{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-21T23:36:24.541846Z",
     "iopub.status.busy": "2025-01-21T23:36:24.541540Z",
     "iopub.status.idle": "2025-01-21T23:38:28.106736Z",
     "shell.execute_reply": "2025-01-21T23:38:28.105839Z",
     "shell.execute_reply.started": "2025-01-21T23:36:24.541824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Define Constants and Parameters\n",
    "# -------------------------------------------------\n",
    "IMG_SIZE = 128\n",
    "VOLUME_SLICES = 100\n",
    "VOLUME_START_AT = 22\n",
    "SEGMENT_CLASSES = {\n",
    "    0: 'NOT tumor',\n",
    "    1: 'NECROTIC/CORE',\n",
    "    2: 'EDEMA',\n",
    "    3: 'ENHANCING'\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Function Definitions\n",
    "# -------------------------------------------------\n",
    "def extract_tar_files(tar_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts tar.gz files with progress tracking.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting extraction of {os.path.basename(tar_path)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        members = tar.getmembers()\n",
    "        print(f\"Total files in archive: {len(members)}\")\n",
    "        \n",
    "        for member in tqdm(members, desc=\"Extracting files\"):\n",
    "            tar.extract(member, extract_to)\n",
    "            \n",
    "    print(f\"✓ Extraction complete: {tar_path}\")\n",
    "    print(f\"✓ Files extracted to: {extract_to}\\n\")\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Lists all files with detailed information.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Scanning directory: {directory}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    print(f\"Found {len(files)} files/directories:\")\n",
    "    for i, file in enumerate(files, 1):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"{i}. {file} ({size:.2f} MB)\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "def load_nifti_image(file_path):\n",
    "    \"\"\"\n",
    "    Loads a NIfTI file with timing and memory information.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nLoading: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    img = nib.load(file_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    memory_usage = data.nbytes / (1024 * 1024)  # Convert to MB\n",
    "    \n",
    "    print(f\"✓ Load time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"✓ Data shape: {data.shape}\")\n",
    "    print(f\"✓ Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"✓ Data type: {data.dtype}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_training_data(dataset_path, sample_id):\n",
    "    \"\"\"\n",
    "    Loads all imaging modalities with progress tracking.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Loading complete dataset for sample: {sample_id}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    sample_path = os.path.join(dataset_path, sample_id)\n",
    "    modalities = ['flair', 't1', 't1ce', 't2', 'seg']\n",
    "    data = {}\n",
    "    \n",
    "    for modality in tqdm(modalities, desc=\"Loading modalities\"):\n",
    "        file_path = os.path.join(sample_path, f\"{sample_id}_{modality}.nii.gz\")\n",
    "        data[modality] = load_nifti_image(file_path)\n",
    "        \n",
    "    print(\"\\n✓ All modalities loaded successfully\")\n",
    "    return data\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Usage\n",
    "# -------------------------------------------------\n",
    "def main():\n",
    "    print(\"\\n🧠 Starting Brain Tumor Image Processing Pipeline 🧠\\n\")\n",
    "    \n",
    "    # Define paths\n",
    "    train_tar_path = '../input/brats-2021-task1/BraTS2021_Training_Data.tar'\n",
    "    sample_tar_path = '../input/brats-2021-task1/BraTS2021_00621.tar'\n",
    "    training_data_path = './BraTS2021_Training_Data'\n",
    "    sample_data_path = './sample_img'\n",
    "    \n",
    "    # Extract files with progress tracking\n",
    "    extract_tar_files(train_tar_path, training_data_path)\n",
    "    extract_tar_files(sample_tar_path, sample_data_path)\n",
    "    \n",
    "    # List extracted files\n",
    "    files = list_files_in_directory(sample_data_path)\n",
    "    \n",
    "    # Load sample data\n",
    "    sample_id = 'BraTS2021_01261'\n",
    "    print(f\"\\nProcessing sample: {sample_id}\")\n",
    "    \n",
    "    sample_data = load_training_data(training_data_path, sample_id)\n",
    "    \n",
    "    # Access modalities\n",
    "    modalities = {\n",
    "        'FLAIR': sample_data['flair'],\n",
    "        'T1': sample_data['t1'],\n",
    "        'T1CE': sample_data['t1ce'],\n",
    "        'T2': sample_data['t2'],\n",
    "        'Segmentation': sample_data['seg']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 Final Data Summary:\")\n",
    "    for name, data in modalities.items():\n",
    "        print(f\"✓ {name:<12} Shape: {data.shape}, Range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n✨ Processing pipeline step 1 completed successfully! ✨\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:38:28.108345Z",
     "iopub.status.busy": "2025-01-21T23:38:28.107917Z",
     "iopub.status.idle": "2025-01-21T23:40:06.993595Z",
     "shell.execute_reply": "2025-01-21T23:40:06.992726Z",
     "shell.execute_reply.started": "2025-01-21T23:38:28.108322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import rotate, gaussian_filter, map_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import nilearn as nl\n",
    "import nilearn.plotting as nlplt\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Constants and Parameters\n",
    "# -------------------------------------------------\n",
    "IMG_SIZE = 128\n",
    "VOLUME_SLICES = 100\n",
    "VOLUME_START_AT = 22\n",
    "SEGMENT_CLASSES = {\n",
    "    0: 'NOT tumor',\n",
    "    1: 'NECROTIC/CORE',\n",
    "    2: 'EDEMA',\n",
    "    3: 'ENHANCING'\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Utility Functions\n",
    "# -------------------------------------------------\n",
    "def extract_tar_files(tar_path, extract_to):\n",
    "    \"\"\"Extracts .tar or .tar.gz files with progress tracking.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📦 Extracting: {os.path.basename(tar_path)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with tarfile.open(tar_path, 'r') as file:\n",
    "        members = file.getmembers()\n",
    "        print(f\"Total files to extract: {len(members)}\")\n",
    "        \n",
    "        for member in tqdm(members, desc=\"Extracting files\", unit=\"file\"):\n",
    "            file.extract(member, extract_to)\n",
    "    \n",
    "    print(f\"✅ Extraction complete: {len(members)} files extracted to {extract_to}\\n\")\n",
    "\n",
    "def check_nifti(file_path):\n",
    "    \"\"\"Checks if a NIfTI file can be loaded successfully.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        nib.load(file_path)\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"✓ Validated {os.path.basename(file_path)} ({load_time:.2f}s)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"Loads a NIfTI file with timing information.\"\"\"\n",
    "    start_time = time.time()\n",
    "    img = nib.load(file_path)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"📂 Loaded {os.path.basename(file_path)} ({load_time:.2f}s)\")\n",
    "    return img\n",
    "\n",
    "def save_nifti(img, save_path):\n",
    "    \"\"\"Saves a NIfTI image with timing information.\"\"\"\n",
    "    start_time = time.time()\n",
    "    nib.save(img, save_path)\n",
    "    save_time = time.time() - start_time\n",
    "    print(f\"💾 Saved {os.path.basename(save_path)} ({save_time:.2f}s)\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Preprocessing Functions\n",
    "# -------------------------------------------------\n",
    "def normalize_nifti(img, desc=\"\"):\n",
    "    \"\"\"Normalizes intensity values in the NIfTI image.\"\"\"\n",
    "    start_time = time.time()\n",
    "    data = img.get_fdata()\n",
    "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    normalized = nib.Nifti1Image(data, img.affine)\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"⚖️  Normalized{' '+desc if desc else ''} ({process_time:.2f}s)\")\n",
    "    return normalized\n",
    "\n",
    "def resize_nifti(img, target_shape=(256, 256, 128), desc=\"\"):\n",
    "    \"\"\"Resizes a NIfTI image to the specified shape.\"\"\"\n",
    "    start_time = time.time()\n",
    "    data = img.get_fdata()\n",
    "    data_resized = resize(data, target_shape, anti_aliasing=True)\n",
    "    resized = nib.Nifti1Image(data_resized, img.affine)\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"📐 Resized{' '+desc if desc else ''} to {target_shape} ({process_time:.2f}s)\")\n",
    "    return resized\n",
    "\n",
    "def adjust_brightness(img, brightness_factor=2, desc=\"\"):\n",
    "    \"\"\"Adjusts the brightness of a NIfTI image.\"\"\"\n",
    "    start_time = time.time()\n",
    "    data = img.get_fdata()\n",
    "    data_adjusted = exposure.adjust_gamma(data, gamma=brightness_factor)\n",
    "    adjusted = nib.Nifti1Image(data_adjusted, img.affine)\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"🔆 Adjusted brightness{' '+desc if desc else ''} ({process_time:.2f}s)\")\n",
    "    return adjusted\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization Functions\n",
    "# -------------------------------------------------\n",
    "def plot_modality_comparison(images, slice_w=25):\n",
    "    \"\"\"Plots comparison of different MRI modalities.\"\"\"\n",
    "    print(\"\\n📊 Generating modality comparison plot...\")\n",
    "    \n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20, 10))\n",
    "    \n",
    "    modalities = [\n",
    "        ('flair', ax1, 'Image FLAIR'),\n",
    "        ('t1', ax2, 'Image T1'),\n",
    "        ('t1ce', ax3, 'Image T1CE'),\n",
    "        ('t2', ax4, 'Image T2'),\n",
    "        ('seg', ax5, 'Mask')\n",
    "    ]\n",
    "    \n",
    "    for modality, ax, title in tqdm(modalities, desc=\"Plotting modalities\"):\n",
    "        img = images[modality]\n",
    "        slice_idx = img.shape[0]//2 - slice_w\n",
    "        \n",
    "        if modality != 'seg':\n",
    "            ax.imshow(img[:,:,slice_idx], cmap='gray')\n",
    "        else:\n",
    "            ax.imshow(img[:,:,slice_idx])\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    print(\"✅ Modality comparison plot generated\")\n",
    "    return fig\n",
    "\n",
    "def plot_detailed_visualization(flair_path, mask_path):\n",
    "    \"\"\"Creates detailed visualizations using nilearn.\"\"\"\n",
    "    print(\"\\n🎨 Generating detailed nilearn visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=2, desc=\"Loading images\") as pbar:\n",
    "            niimg = nl.image.load_img(flair_path)\n",
    "            pbar.update(1)\n",
    "            nimask = nl.image.load_img(mask_path)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n",
    "        \n",
    "        views = [\n",
    "            (nlplt.plot_anat, 'Anatomical View'),\n",
    "            (nlplt.plot_epi, 'EPI View'),\n",
    "            (nlplt.plot_img, 'Standard View'),\n",
    "            (None, 'ROI with Mask')  # Special case for ROI plot\n",
    "        ]\n",
    "        \n",
    "        for i, (plot_func, title) in enumerate(tqdm(views, desc=\"Generating views\")):\n",
    "            if i == 3:  # ROI plot\n",
    "                nlplt.plot_roi(\n",
    "                    nimask,\n",
    "                    title=f'FLAIR with mask {title}',\n",
    "                    bg_img=niimg,\n",
    "                    axes=axes[i],\n",
    "                    cmap='Paired'\n",
    "                )\n",
    "            else:\n",
    "                plot_func(\n",
    "                    niimg,\n",
    "                    title=f'FLAIR {title}',\n",
    "                    axes=axes[i]\n",
    "                )\n",
    "                \n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "        print(\"✅ Detailed visualization completed\")\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating detailed visualization: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Processing Pipeline\n",
    "# -------------------------------------------------\n",
    "def process_single_patient(sample_id, input_path, output_path):\n",
    "    \"\"\"Processes and visualizes data for a single patient.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🏥 Processing patient: {sample_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Load all modalities\n",
    "        modalities = ['flair', 't1', 't1ce', 't2', 'seg']\n",
    "        images = {}\n",
    "        \n",
    "        for modality in tqdm(modalities, desc=\"Loading modalities\"):\n",
    "            file_path = os.path.join(input_path, sample_id, f\"{sample_id}_{modality}.nii.gz\")\n",
    "            images[modality] = load_nifti(file_path).get_fdata()\n",
    "        \n",
    "        # Create visualizations\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # Modality comparison plot\n",
    "        comparison_fig = plot_modality_comparison(images)\n",
    "        comparison_fig.savefig(os.path.join(output_path, f\"{sample_id}_modality_comparison.png\"))\n",
    "        plt.close(comparison_fig)\n",
    "        \n",
    "        # Detailed visualization\n",
    "        flair_path = os.path.join(input_path, sample_id, f\"{sample_id}_flair.nii.gz\")\n",
    "        mask_path = os.path.join(input_path, sample_id, f\"{sample_id}_seg.nii.gz\")\n",
    "        detailed_fig = plot_detailed_visualization(flair_path, mask_path)\n",
    "        \n",
    "        if detailed_fig:\n",
    "            detailed_fig.savefig(os.path.join(output_path, f\"{sample_id}_detailed_visualization.png\"))\n",
    "            plt.close(detailed_fig)\n",
    "        \n",
    "        print(f\"✅ Processing complete for patient {sample_id}\")\n",
    "        return images\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing patient {sample_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n🧠 BraTS Image Processing Pipeline 🧠\")\n",
    "    \n",
    "    # Define paths\n",
    "    TRAIN_TAR_PATH = '../input/brats-2021-task1/BraTS2021_Training_Data.tar'\n",
    "    SAMPLE_TAR_PATH = '../input/brats-2021-task1/BraTS2021_00621.tar'\n",
    "    TRAINING_PATH = './BraTS2021_Training_Data'\n",
    "    OUTPUT_PATH = './processed_data'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Extract data\n",
    "    extract_tar_files(TRAIN_TAR_PATH, TRAINING_PATH)\n",
    "    \n",
    "    # Process sample patient\n",
    "    sample_id = 'BraTS2021_01261'\n",
    "    sample_data = process_single_patient(sample_id, TRAINING_PATH, OUTPUT_PATH)\n",
    "    \n",
    "    if sample_data:\n",
    "        print(\"\\n✨ Processing pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Processing pipeline encountered errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:06.995377Z",
     "iopub.status.busy": "2025-01-21T23:40:06.994973Z",
     "iopub.status.idle": "2025-01-21T23:40:08.876486Z",
     "shell.execute_reply": "2025-01-21T23:40:08.875588Z",
     "shell.execute_reply.started": "2025-01-21T23:40:06.995355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Using PIL (Python Imaging Library)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Method 1: Direct loading with PIL\n",
    "image = Image.open('/kaggle/working/processed_data/BraTS2021_01261_detailed_visualization.png')\n",
    "\n",
    "# To display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Using OpenCV (if you need image processing)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read image (OpenCV loads in BGR format)\n",
    "image = cv2.imread('/kaggle/working/processed_data/BraTS2021_01261_modality_comparison.png')\n",
    "# Convert BGR to RGB for display\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:08.878014Z",
     "iopub.status.busy": "2025-01-21T23:40:08.877738Z",
     "iopub.status.idle": "2025-01-21T23:40:20.953451Z",
     "shell.execute_reply": "2025-01-21T23:40:20.952698Z",
     "shell.execute_reply.started": "2025-01-21T23:40:08.877986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob \n",
    "# used to find all the pathnames matching a specified pattern\n",
    "import PIL\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage.util import montage \n",
    "import skimage.transform as skTrans\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "from PIL import Image, ImageOps  \n",
    "\n",
    "# neural imaging\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "import nilearn.plotting as nlplt\n",
    "\n",
    "\n",
    "# ml libs\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:20.954856Z",
     "iopub.status.busy": "2025-01-21T23:40:20.954331Z",
     "iopub.status.idle": "2025-01-21T23:40:20.966408Z",
     "shell.execute_reply": "2025-01-21T23:40:20.965352Z",
     "shell.execute_reply.started": "2025-01-21T23:40:20.954820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # dice loss as defined above for 4 classes\n",
    "# def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "#     class_num = 4\n",
    "#     for i in range(class_num):\n",
    "#         y_true_f = K.flatten(y_true[:,:,:,i])\n",
    "#         y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
    "#         intersection = K.sum(y_true_f * y_pred_f)\n",
    "#         loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "#    #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n",
    "#         if i == 0:\n",
    "#             total_loss = loss\n",
    "#         else:\n",
    "#             total_loss = total_loss + loss\n",
    "            \n",
    "#     total_loss = total_loss / class_num\n",
    "# #    K.print_tensor(total_loss, message=' total dice coef: ')\n",
    "#     return total_loss\n",
    "\n",
    "\n",
    " \n",
    "# # These functions are used for evaluating the performance of a segmentation model on three different classes\n",
    "# # in medical imaging (presumably related to brain tumor segmentation).\n",
    "# # Input Parameters:\n",
    "# # y_true: The ground truth segmentation mask for the edema class.\n",
    "# # y_pred: The predicted segmentation mask for the edema class.\n",
    "# # epsilon: A small constant to avoid division by zero.\n",
    "\n",
    "# def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "#     intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
    "#     return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
    "\n",
    "# def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "#     intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
    "#     return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
    "\n",
    "# def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "#     intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
    "#     return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n",
    "\n",
    "\n",
    "\n",
    "# # Computing Precision \n",
    "# def precision(y_true, y_pred):\n",
    "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#         return precision\n",
    "\n",
    "    \n",
    "# # Computing Sensitivity      \n",
    "# def sensitivity(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "\n",
    "# # Computing Specificity\n",
    "# def specificity(y_true, y_pred):\n",
    "#     true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "#     possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "#     return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    class_num = 4  # Number of classes\n",
    "    dice_list = []\n",
    "    for i in range(class_num):\n",
    "        # Reshape the tensors into 1D arrays for each class\n",
    "        y_true_f = tf.reshape(y_true[:, :, :, i], [-1])\n",
    "        y_pred_f = tf.reshape(y_pred[:, :, :, i], [-1])\n",
    "        \n",
    "        # Compute intersection and Dice coefficient\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        union = K.sum(y_true_f) + K.sum(y_pred_f)\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_list.append(dice)\n",
    "\n",
    "    # Convert the list of Dice scores into a tensor and average across all classes\n",
    "    return K.mean(tf.stack(dice_list))\n",
    "\n",
    "\n",
    "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
    "    union = K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
    "    union = K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
    "    union = K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "\n",
    "# Computing Precision \n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "# Computing Sensitivity      \n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "# Computing Specificity\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Input, Lambda, UpSampling2D, concatenate, Dropout, LayerNormalization, Dense, Layer\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, transformer_name=\"huawei-noah/TinyBERT_General_4L_312D\", embedding_dim=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transformer_name = transformer_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Load transformer configuration first\n",
    "        self.transformer_config = TFAutoModel.from_pretrained(transformer_name, from_pt=True).config\n",
    "        # Then create the model\n",
    "        self.transformer = TFAutoModel.from_pretrained(transformer_name, from_pt=True, config=self.transformer_config)\n",
    "        self.transformer.trainable = False\n",
    "        self.input_dense = None\n",
    "        self.output_dense = None\n",
    "        self.layer_norm = LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.h, self.w = input_shape[1], input_shape[2]\n",
    "        bert_hidden_size = self.transformer_config.hidden_size\n",
    "        \n",
    "        self.input_dense = Dense(bert_hidden_size)\n",
    "        self.output_dense = Dense(self.embedding_dim, activation='relu')\n",
    "        \n",
    "        self.pos_embedding = self.add_weight(\n",
    "            shape=(self.h * self.w, bert_hidden_size),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Convert inputs to tensor if needed\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Reshape to sequence: (batch, sequence_length, channels)\n",
    "        x = tf.reshape(x, (batch_size, self.h * self.w, x.shape[-1]))\n",
    "        \n",
    "        # Project to BERT's hidden size\n",
    "        x = self.input_dense(x)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # Layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = tf.ones((batch_size, self.h * self.w), dtype=tf.int32)\n",
    "        \n",
    "        # Call transformer using the functional API\n",
    "        transformer_outputs = self.transformer.call(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            inputs_embeds=x,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Get the sequence output\n",
    "        sequence_output = transformer_outputs[0]\n",
    "        \n",
    "        # Project to desired dimension\n",
    "        x = self.output_dense(sequence_output)\n",
    "        \n",
    "        # Reshape back to spatial dimensions\n",
    "        x = tf.reshape(x, (batch_size, self.h, self.w, self.embedding_dim))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], self.embedding_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"transformer_name\": self.transformer_name,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_enhanced_unetpp_with_transformer(input_shape, ker_init='he_normal', dropout_rate=0.2):\n",
    "    \n",
    "    # Fix for 2-channel input (convert to 3 channels)\n",
    "    inputs = Input(input_shape)  # Input with 2 channels\n",
    "    inputs_3_channel = Lambda(lambda x: tf.concat([x, x[:, :, :, :1]], axis=-1))(inputs)  # Convert to 3 channels\n",
    "\n",
    "    # Load ResNet50 as the encoder backbone with 3-channel input\n",
    "    resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs_3_channel)\n",
    "    resnet.trainable = False  # Freeze the ResNet backbone\n",
    "\n",
    "    # inputs = resnet.input\n",
    "    conv1 = resnet.get_layer(\"conv1_relu\").output  # 1/2 resolution\n",
    "    conv2 = resnet.get_layer(\"conv2_block3_out\").output  # 1/4 resolution\n",
    "    conv3 = resnet.get_layer(\"conv3_block4_out\").output  # 1/8 resolution\n",
    "    conv4 = resnet.get_layer(\"conv4_block6_out\").output  # 1/16 resolution\n",
    "    bottleneck = resnet.get_layer(\"conv5_block3_out\").output  # 1/32 resolution\n",
    "\n",
    "    # Transformer enhancement applied at bottleneck\n",
    "    transformer_output = TransformerBlock()(bottleneck)\n",
    "    enhanced_features = concatenate([bottleneck, transformer_output])\n",
    "    enhanced_features = Conv2D(256, 1, activation='relu', padding='same')(enhanced_features)\n",
    "\n",
    "    # Decoder Path with Nested Skip Connections\n",
    "    # Decoder Level 4 (Up-sampling bottleneck to conv4 resolution)\n",
    "    up4 = UpSampling2D(size=(2, 2))(enhanced_features)\n",
    "    merge4 = concatenate([conv4, up4])\n",
    "    merge4 = Dropout(dropout_rate)(merge4)\n",
    "    conv4_1 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge4)\n",
    "    conv4_1 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv4_1)\n",
    "\n",
    "    # Decoder Level 3 (Up-sampling conv4_1 to conv3 resolution)\n",
    "    up3 = UpSampling2D(size=(2, 2))(conv4_1)\n",
    "    merge3 = concatenate([conv3, up3])\n",
    "    merge3_nested = concatenate([merge3, UpSampling2D(size=(2, 2))(conv4)])  # Nested connection\n",
    "    merge3_nested = Dropout(dropout_rate)(merge3_nested)\n",
    "    conv3_1 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge3_nested)\n",
    "    conv3_1 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv3_1)\n",
    "\n",
    "    # Decoder Level 2 (Up-sampling conv3_1 to conv2 resolution)\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv3_1)\n",
    "    merge2 = concatenate([conv2, up2])\n",
    "    merge2_nested = concatenate([merge2, UpSampling2D(size=(2, 2))(conv3)])  # Nested connection\n",
    "    merge2_nested = Dropout(dropout_rate)(merge2_nested)\n",
    "    conv2_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge2_nested)\n",
    "    conv2_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv2_1)\n",
    "\n",
    "    # Decoder Level 1 (Up-sampling conv2_1 to conv1 resolution)\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv2_1)\n",
    "    merge1 = concatenate([conv1, up1])\n",
    "    merge1_nested = concatenate([merge1, UpSampling2D(size=(2, 2))(conv2)])  # Nested connection\n",
    "    merge1_nested = Dropout(dropout_rate)(merge1_nested)\n",
    "    conv1_1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge1_nested)\n",
    "    conv1_1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv1_1)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Conv2D(4, 1, activation='softmax')(conv1_1)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:47:39.900531Z",
     "iopub.status.busy": "2025-01-21T23:47:39.900153Z",
     "iopub.status.idle": "2025-01-21T23:47:39.916694Z",
     "shell.execute_reply": "2025-01-21T23:47:39.915853Z",
     "shell.execute_reply.started": "2025-01-21T23:47:39.900507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Input, MaxPooling2D, Dropout, UpSampling2D, \n",
    "    concatenate, Reshape, Dense, Flatten, LayerNormalization,\n",
    "    Layer\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# google-bert/bert-base-uncased\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, transformer_name=\"huawei-noah/TinyBERT_General_4L_312D\", embedding_dim=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transformer_name = transformer_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Load transformer configuration first\n",
    "        self.transformer_config = TFAutoModel.from_pretrained(transformer_name,from_pt=True).config\n",
    "        # Then create the model\n",
    "        self.transformer = TFAutoModel.from_pretrained(transformer_name,from_pt=True,config=self.transformer_config)\n",
    "        self.transformer.trainable = False\n",
    "        self.input_dense = None\n",
    "        self.output_dense = None\n",
    "        self.layer_norm = LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.h, self.w = input_shape[1], input_shape[2]\n",
    "        bert_hidden_size = self.transformer_config.hidden_size\n",
    "        \n",
    "        self.input_dense = Dense(bert_hidden_size)\n",
    "        self.output_dense = Dense(self.embedding_dim, activation='relu')\n",
    "        \n",
    "        self.pos_embedding = self.add_weight(\n",
    "            shape=(self.h * self.w, bert_hidden_size),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Convert inputs to tensor if needed\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Reshape to sequence: (batch, sequence_length, channels)\n",
    "        x = tf.reshape(x, (batch_size, self.h * self.w, x.shape[-1]))\n",
    "        \n",
    "        # Project to BERT's hidden size\n",
    "        x = self.input_dense(x)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # Layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = tf.ones((batch_size, self.h * self.w), dtype=tf.int32)\n",
    "        \n",
    "        # Call transformer using the functional API\n",
    "        transformer_outputs = self.transformer.call(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            inputs_embeds=x,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Get the sequence output\n",
    "        sequence_output = transformer_outputs[0]\n",
    "        \n",
    "        # Project to desired dimension\n",
    "        x = self.output_dense(sequence_output)\n",
    "        \n",
    "        # Reshape back to spatial dimensions\n",
    "        x = tf.reshape(x, (batch_size, self.h, self.w, self.embedding_dim))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], self.embedding_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"transformer_name\": self.transformer_name,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    \n",
    "def build_unetpp_with_transformer(\n",
    "    input_shape, \n",
    "    ker_init='he_normal', \n",
    "    dropout_rate=0.2\n",
    "):\n",
    "    # Input Layer\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder Path\n",
    "    conv1_1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(inputs)\n",
    "    conv1_1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv1_1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_1)\n",
    "    pool1 = Dropout(dropout_rate)(pool1)\n",
    "\n",
    "    conv2_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(pool1)\n",
    "    conv2_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv2_1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_1)\n",
    "    pool2 = Dropout(dropout_rate)(pool2)\n",
    "\n",
    "    # U-Net++ nested skip connection\n",
    "    conv2_2 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(\n",
    "        concatenate([\n",
    "            conv2_1,\n",
    "            UpSampling2D(size=(2, 2))(pool2)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    conv3_1 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(pool2)\n",
    "    conv3_1 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv3_1)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3_1)\n",
    "    pool3 = Dropout(dropout_rate)(pool3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=ker_init)(pool3)\n",
    "    bottleneck = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer=ker_init)(bottleneck)\n",
    "    \n",
    "    # Transformer Feature Enhancement\n",
    "    transformer_output = TransformerBlock()(bottleneck)\n",
    "    \n",
    "    # Combine CNN and transformer features\n",
    "    enhanced_features = concatenate([bottleneck, transformer_output])\n",
    "    enhanced_features = Conv2D(256, 1, activation='relu', padding='same')(enhanced_features)\n",
    "\n",
    "    # Decoder Path\n",
    "    up3 = UpSampling2D(size=(2, 2))(enhanced_features)\n",
    "    up3 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer=ker_init)(up3)\n",
    "    merge3 = concatenate([conv3_1, up3])\n",
    "    merge3 = Dropout(dropout_rate)(merge3)\n",
    "    conv7 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge3)\n",
    "    conv7 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv7)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up2 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer=ker_init)(up2)\n",
    "    merge2 = concatenate([conv2_1, conv2_2, up2])\n",
    "    merge2 = Dropout(dropout_rate)(merge2)\n",
    "    conv8 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge2)\n",
    "    conv8 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv8)\n",
    "\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up1 = Conv2D(32, 2, activation='relu', padding='same', kernel_initializer=ker_init)(up1)\n",
    "    merge1 = concatenate([conv1_1, up1])\n",
    "    merge1 = Dropout(dropout_rate)(merge1)\n",
    "    conv9 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(merge1)\n",
    "    conv9 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer=ker_init)(conv9)\n",
    "\n",
    "    outputs = Conv2D(4, 1, activation='softmax')(conv9)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:47:40.797363Z",
     "iopub.status.busy": "2025-01-21T23:47:40.796991Z",
     "iopub.status.idle": "2025-01-21T23:47:42.203640Z",
     "shell.execute_reply": "2025-01-21T23:47:42.202898Z",
     "shell.execute_reply.started": "2025-01-21T23:47:40.797336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "input_shape = (128, 128, 2)\n",
    "model = build_unetpp_with_transformer(input_shape)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.MeanIoU(num_classes=4),\n",
    "        dice_coef,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        dice_coef_necrotic,\n",
    "        dice_coef_edema,\n",
    "        dice_coef_enhancing\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a sample input\n",
    "test_input = np.random.random((100, 128, 128, 2))  # Add batch dimension\n",
    "print(\"Input shape:\", test_input.shape)\n",
    "test_output = model.predict(test_input)\n",
    "print(\"Output shape:\", test_output.shape)\n",
    "# Test model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:35.413394Z",
     "iopub.status.busy": "2025-01-21T23:40:35.413164Z",
     "iopub.status.idle": "2025-01-21T23:40:36.059153Z",
     "shell.execute_reply": "2025-01-21T23:40:36.058209Z",
     "shell.execute_reply.started": "2025-01-21T23:40:35.413374Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, \n",
    "           show_shapes = True,\n",
    "           show_dtype=False,\n",
    "           show_layer_names = True, \n",
    "           rankdir = 'TB', \n",
    "           expand_nested = False, \n",
    "           dpi = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.060480Z",
     "iopub.status.busy": "2025-01-21T23:40:36.060164Z",
     "iopub.status.idle": "2025-01-21T23:40:36.069904Z",
     "shell.execute_reply": "2025-01-21T23:40:36.069092Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.060443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAINING_PATH) if f.is_dir()]\n",
    "\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "#train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories); \n",
    "\n",
    "    \n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.071091Z",
     "iopub.status.busy": "2025-01-21T23:40:36.070777Z",
     "iopub.status.idle": "2025-01-21T23:40:36.083615Z",
     "shell.execute_reply": "2025-01-21T23:40:36.082809Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.071059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_and_test_ids[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.084789Z",
     "iopub.status.busy": "2025-01-21T23:40:36.084529Z",
     "iopub.status.idle": "2025-01-21T23:40:36.098635Z",
     "shell.execute_reply": "2025-01-21T23:40:36.097907Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.084768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.099462Z",
     "iopub.status.busy": "2025-01-21T23:40:36.099274Z",
     "iopub.status.idle": "2025-01-21T23:40:36.113263Z",
     "shell.execute_reply": "2025-01-21T23:40:36.112499Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.099446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.114441Z",
     "iopub.status.busy": "2025-01-21T23:40:36.114169Z",
     "iopub.status.idle": "2025-01-21T23:40:36.132388Z",
     "shell.execute_reply": "2025-01-21T23:40:36.131715Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.114421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
    "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
    "\n",
    "        \n",
    "        # Generate data\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(TRAINING_PATH, i)\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_flair.nii.gz');\n",
    "            flair = nib.load(data_path).get_fdata()    \n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_t1ce.nii.gz');\n",
    "            ce = nib.load(data_path).get_fdata()\n",
    "            \n",
    "            data_path = os.path.join(case_path, f'{i}_seg.nii.gz');\n",
    "            seg = nib.load(data_path).get_fdata()\n",
    "        \n",
    "            for j in range(VOLUME_SLICES):\n",
    "                X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "                X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "\n",
    "                y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
    "                    \n",
    "        # Generate masks\n",
    "        y[y==4] = 3;\n",
    "        mask = tf.one_hot(y, 4);\n",
    "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
    "        return X/np.max(X), Y\n",
    "        \n",
    "training_generator = DataGenerator(train_ids)\n",
    "valid_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.133273Z",
     "iopub.status.busy": "2025-01-21T23:40:36.133078Z",
     "iopub.status.idle": "2025-01-21T23:40:36.150385Z",
     "shell.execute_reply": "2025-01-21T23:40:36.149545Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.133256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(train_ids))\n",
    "print(len(val_ids))\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:40:36.151607Z",
     "iopub.status.busy": "2025-01-21T23:40:36.151260Z",
     "iopub.status.idle": "2025-01-21T23:40:36.374128Z",
     "shell.execute_reply": "2025-01-21T23:40:36.373266Z",
     "shell.execute_reply.started": "2025-01-21T23:40:36.151573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# show number of data for each dir \n",
    "def showDataLayout():\n",
    "    plt.bar([\"Train\",\"Valid\",\"Test\"],\n",
    "    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylabel('Number of images')\n",
    "    plt.title('Data distribution')\n",
    "    plt.savefig('data2018.png')\n",
    "    plt.show()\n",
    "    \n",
    "showDataLayout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T23:48:09.075831Z",
     "iopub.status.busy": "2025-01-21T23:48:09.075475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LambdaCallback, CSVLogger\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "\n",
    "filepath=\"3D-MedVision++-weights-improvement-{epoch:02d}-{val_loss:.3f}.keras\" \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=300, verbose=1, restore_best_weights=True)\n",
    "log_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: print(f\"Epoch {epoch} logs: {logs}\"))\n",
    "csv_logger = CSVLogger('training_UNet++.log')\n",
    "\n",
    "history =  model.fit(training_generator,\n",
    "                    epochs=100,\n",
    "                    batch_size =2,\n",
    "                    steps_per_epoch=len(train_ids) // 2,\n",
    "                    callbacks= [checkpoint, csv_logger, early_stop],\n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps = len(val_ids) // 2,\n",
    "                    )  \n",
    "\n",
    "# Metrics: Various metrics are reported during training and validation to assess the model's performance. These include:\n",
    "\n",
    "# Loss: A measure of how well the model is performing. It represents an error value that the model is trying to minimize during training.\n",
    "# Accuracy: The proportion of correctly classified samples.\n",
    "# Mean Intersection over Union (mean_io_u): A metric used for image segmentation tasks, measuring the overlap between predicted and true segmentation masks.\n",
    "# Dice Coefficient (dice_coef): Another metric for segmentation tasks, measuring the similarity between predicted and true masks.\n",
    "# Precision, Sensitivity, Specificity: These are commonly used metrics in binary classification tasks. Precision is the ratio of true positives to the sum of true positives and false positives. Sensitivity (recall) is the ratio of true positives to the sum of true positives and false negatives. Specificity is the ratio of true negatives to the sum of true negatives and false positives.\n",
    "# Dice Coefficients for Necrotic, Edema, Enhancing: Specific dice coefficients for different classes in the segmentation task.\n",
    "# This is common in medical image segmentation where different regions of interest are segmented separately.\n",
    "# Validation Metrics: These are metrics evaluated on a separate dataset not used for training. \n",
    "# They give an indication of how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Available GPUs: {physical_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Set the policy to mixed precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Define a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print('Number of devices: ', strategy.num_replicas_in_sync)\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    # Build and compile the model inside the strategy scope\n",
    "    model = build_unetpp_with_transformer(input_shape=(128, 128, 2))\n",
    "    model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.MeanIoU(num_classes=4),\n",
    "        dice_coef,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        dice_coef_necrotic,\n",
    "        dice_coef_edema,\n",
    "        dice_coef_enhancing\n",
    "    ]\n",
    ")\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(training_generator,\n",
    "                        epochs=100,\n",
    "                        batch_size=2,\n",
    "                        steps_per_epoch=len(train_ids) // 2,\n",
    "                        validation_data=valid_generator,\n",
    "                        validation_steps=len(val_ids) // 2,\n",
    "                        callbacks=[checkpoint, csv_logger, early_stop, log_callback])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1541666,
     "sourceId": 2542390,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
