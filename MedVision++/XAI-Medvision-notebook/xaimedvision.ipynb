{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-14T14:27:01.711415Z",
     "iopub.status.busy": "2025-03-14T14:27:01.711232Z",
     "iopub.status.idle": "2025-03-14T14:27:10.537445Z",
     "shell.execute_reply": "2025-03-14T14:27:10.536365Z",
     "shell.execute_reply.started": "2025-03-14T14:27:01.711397Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:27:31.507949Z",
     "iopub.status.busy": "2025-03-14T14:27:31.507647Z",
     "iopub.status.idle": "2025-03-14T14:27:46.376096Z",
     "shell.execute_reply": "2025-03-14T14:27:46.374998Z",
     "shell.execute_reply.started": "2025-03-14T14:27:31.507928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob \n",
    "# used to find all the pathnames matching a specified pattern\n",
    "import PIL\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage.util import montage \n",
    "import skimage.transform as skTrans\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "from PIL import Image, ImageOps  \n",
    "\n",
    "# neural imaging\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "import nilearn.plotting as nlplt\n",
    "\n",
    "\n",
    "# # ml libs\n",
    "# import keras\n",
    "# import keras.backend as K\n",
    "# from keras.callbacks import CSVLogger\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# from tensorflow.keras.models import *\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.optimizers import *\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:27:46.377838Z",
     "iopub.status.busy": "2025-03-14T14:27:46.377262Z",
     "iopub.status.idle": "2025-03-14T14:27:46.383635Z",
     "shell.execute_reply": "2025-03-14T14:27:46.380802Z",
     "shell.execute_reply.started": "2025-03-14T14:27:46.377808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DEFINE seg-areas  \n",
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',   \n",
    "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE - RED\n",
    "    2 : 'EDEMA',  # Green\n",
    "    3 : 'ENHANCING' # original 4 -> converted into 3 later, Yellow\n",
    "}\n",
    "\n",
    "# there are 155 slices per volume\n",
    "# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n",
    "VOLUME_SLICES = 100 \n",
    "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
    "\n",
    "IMG_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:27:46.386292Z",
     "iopub.status.busy": "2025-03-14T14:27:46.385813Z",
     "iopub.status.idle": "2025-03-14T14:29:26.767984Z",
     "shell.execute_reply": "2025-03-14T14:29:26.766978Z",
     "shell.execute_reply.started": "2025-03-14T14:27:46.386263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "file = tarfile.open('../input/brats-2021-task1/BraTS2021_Training_Data.tar')\n",
    "\n",
    "file.extractall('./BraTS2021_Training_Data')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:29:26.769274Z",
     "iopub.status.busy": "2025-03-14T14:29:26.768961Z",
     "iopub.status.idle": "2025-03-14T14:29:26.772667Z",
     "shell.execute_reply": "2025-03-14T14:29:26.771815Z",
     "shell.execute_reply.started": "2025-03-14T14:29:26.769249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = './BraTS2021_Training_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-12T07:46:40.145067Z",
     "iopub.status.idle": "2025-03-12T07:46:40.145788Z",
     "shell.execute_reply": "2025-03-12T07:46:40.145235Z",
     "shell.execute_reply.started": "2025-03-12T07:46:40.145198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "# from skimage.transform import resize\n",
    "# from skimage import exposure\n",
    "# from scipy.ndimage import rotate, gaussian_filter, map_coordinates\n",
    "# import matplotlib.pyplot as plt\n",
    "# import imageio\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def load_nifti(file_path):\n",
    "#     return nib.load(file_path)\n",
    "\n",
    "# def save_nifti(img, save_path):\n",
    "#     nib.save(img, save_path)\n",
    "\n",
    "# def resize_nifti(img, target_shape=(256, 256, 128)):\n",
    "#     data = img.get_fdata()\n",
    "#     data_resized = resize(data, target_shape, anti_aliasing=True)\n",
    "#     return nib.Nifti1Image(data_resized, img.affine)\n",
    "\n",
    "# def crop_brain_region(img, threshold=0.1):\n",
    "#     data = img.get_fdata()\n",
    "#     mask = data > (threshold * np.max(data))\n",
    "#     coords = np.array(np.nonzero(mask))\n",
    "#     min_coords = np.min(coords, axis=1)\n",
    "#     max_coords = np.max(coords, axis=1)\n",
    "#     cropped_data = data[min_coords[0]:max_coords[0], \n",
    "#                        min_coords[1]:max_coords[1], \n",
    "#                        min_coords[2]:max_coords[2]]\n",
    "#     return nib.Nifti1Image(cropped_data, img.affine)\n",
    "\n",
    "# def contrast_enhance_nifti(img):\n",
    "#     data = img.get_fdata()\n",
    "#     data_enhanced = exposure.equalize_hist(data)\n",
    "#     return nib.Nifti1Image(data_enhanced, img.affine)\n",
    "\n",
    "# def augment_nifti(img):\n",
    "#     data = img.get_fdata()\n",
    "#     data_rotated = rotate(data, angle=15, axes=(0, 1), reshape=False)\n",
    "#     alpha, sigma = 50, 5\n",
    "#     random_state = np.random.RandomState(None)\n",
    "#     shape = data.shape\n",
    "#     dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "#     dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "#     dz = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "#     x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
    "#     indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "#     data_deformed = map_coordinates(data, indices, order=1, mode='reflect').reshape(shape)\n",
    "#     return nib.Nifti1Image(data_deformed, img.affine)\n",
    "\n",
    "# def plot_mri_slices(img, title, save_path):\n",
    "#     data = img.get_fdata()\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "#     for i, axis in enumerate([0, 1, 2]):\n",
    "#         mid_slice = data.shape[axis] // 2\n",
    "#         if axis == 0:\n",
    "#             axes[i].imshow(data[mid_slice, :, :], cmap='gray')\n",
    "#         elif axis == 1:\n",
    "#             axes[i].imshow(data[:, mid_slice, :], cmap='gray')\n",
    "#         else:\n",
    "#             axes[i].imshow(data[:, :, mid_slice], cmap='gray')\n",
    "#         axes[i].set_title(f'Slice {mid_slice} (axis {axis})')\n",
    "#     plt.suptitle(title)\n",
    "#     plt.show()\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# def process_single_file(input_file, output_file, modality , count):\n",
    "#     \"\"\"Process a single NIFTI file based on its modality.\"\"\"\n",
    "#     try:\n",
    "        \n",
    "#         img = load_nifti(input_file)\n",
    "#         if count == 3:\n",
    "#             plot_mri_slices(img, 'Original MRI', output_file.replace('.nii.gz', '_original.png'))\n",
    "        \n",
    "#         if modality != 'seg':\n",
    "#             img = crop_brain_region(img)\n",
    "#             img = contrast_enhance_nifti(img)\n",
    "#             img = resize_nifti(img)\n",
    "#             img = augment_nifti(img)\n",
    "        \n",
    "#         save_nifti(img, output_file)\n",
    "#         if count == 3:\n",
    "#             plot_mri_slices(img, 'Processed MRI', output_file.replace('.nii.gz', '_processed.png'))\n",
    "            \n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {input_file}: {str(e)}\")\n",
    "#         return False\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# def process_brats_dataset(input_root, output_root , count):\n",
    "#     \"\"\"Process the entire BraTS dataset maintaining exact file structure.\"\"\"\n",
    "#     os.makedirs(output_root, exist_ok=True)\n",
    "    \n",
    "#     all_files = [os.path.join(root, f) for root, _, files in os.walk(input_root) for f in files if f.endswith('.nii.gz')]\n",
    "    \n",
    "#     for file_path in tqdm(all_files, desc=\"Processing files\"):\n",
    "#         rel_path = os.path.relpath(file_path, input_root)\n",
    "#         output_path = os.path.join(output_root, rel_path)\n",
    "#         os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "#         modality = None\n",
    "#         if '_t1.' in file_path.lower() or '_t1.nii.gz' in file_path.lower():\n",
    "#             modality = 't1'\n",
    "#         elif '_t1ce.' in file_path.lower() or '_t1ce.nii.gz' in file_path.lower():\n",
    "#             modality = 't1ce'\n",
    "#         elif '_t2.' in file_path.lower() or '_t2.nii.gz' in file_path.lower():\n",
    "#             modality = 't2'\n",
    "#         elif '_flair.' in file_path.lower() or '_flair.nii.gz' in file_path.lower():\n",
    "#             modality = 'flair'\n",
    "#         elif '_seg.' in file_path.lower() or '_seg.nii.gz' in file_path.lower():\n",
    "#             modality = 'seg'\n",
    "        \n",
    "#         if modality:\n",
    "#             success = process_single_file(file_path, output_path, modality , count)\n",
    "#             count=count+1\n",
    "#             if not success:\n",
    "#                 print(f\"Failed to process {file_path}\")\n",
    "\n",
    "# def verify_preprocessing(input_root, output_root):\n",
    "#     \"\"\"Verify that all files were processed and maintained structure.\"\"\"\n",
    "#     input_files = {os.path.relpath(os.path.join(root, file), input_root) for root, _, files in os.walk(input_root) for file in files if file.endswith('.nii.gz')}\n",
    "#     output_files = {os.path.relpath(os.path.join(root, file), output_root) for root, _, files in os.walk(output_root) for file in files if file.endswith('.nii.gz')}\n",
    "    \n",
    "#     missing_files = input_files - output_files\n",
    "#     if missing_files:\n",
    "#         print(\"\\nWarning: Some files were not processed:\")\n",
    "#         for file in missing_files:\n",
    "#             print(f\"Missing: {file}\")\n",
    "#     else:\n",
    "#         print(\"\\nAll files were processed successfully!\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     count = 0 \n",
    "#     BRATS_ROOT = '/kaggle/working/BraTS2021_Training_Data'  \n",
    "#     OUTPUT_ROOT = './preprocessed_brats_data'  \n",
    "    \n",
    "#     print(\"Starting preprocessing...\")\n",
    "#     process_brats_dataset(BRATS_ROOT, OUTPUT_ROOT , count)\n",
    "    \n",
    "#     print(\"\\nVerifying preprocessing...\")\n",
    "#     verify_preprocessing(BRATS_ROOT, OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import rotate, gaussian_filter, map_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Constants for preprocessing\n",
    "TARGET_SHAPE = (128, 128, 128)  # Target shape for resampling\n",
    "PATCH_SIZE = (128, 128)  # Patch size for extraction\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"Load a NIfTI file and return the image object.\"\"\"\n",
    "    return nib.load(file_path)\n",
    "\n",
    "def save_nifti(data, affine, save_path):\n",
    "    \"\"\"Save data as a NIfTI file.\"\"\"\n",
    "    nib.save(nib.Nifti1Image(data, affine), save_path)\n",
    "\n",
    "def resample_volume(img, target_shape=TARGET_SHAPE):\n",
    "    \"\"\"Resample volume to a uniform resolution of 1×1×1 mm.\"\"\"\n",
    "    data = img.get_fdata()\n",
    "    original_shape = data.shape\n",
    "    \n",
    "    # Calculate resampling factors\n",
    "    scale_factors = [t/o for t, o in zip(target_shape, original_shape)]\n",
    "    \n",
    "    # Resample data to target shape\n",
    "    data_resampled = resize(data, target_shape, order=3, anti_aliasing=True, mode='constant')\n",
    "    \n",
    "    # Create new affine for 1x1x1 mm spacing\n",
    "    new_affine = img.affine.copy()\n",
    "    for i in range(3):\n",
    "        new_affine[i, i] = img.affine[i, i] / scale_factors[i]\n",
    "    \n",
    "    return nib.Nifti1Image(data_resampled, new_affine)\n",
    "\n",
    "def skull_strip(img, threshold=0.05):\n",
    "    \"\"\"Remove non-brain regions using intensity-based thresholding.\"\"\"\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # Create mask based on threshold\n",
    "    mask = data > (threshold * np.max(data))\n",
    "    \n",
    "    # Apply mask\n",
    "    data_stripped = data * mask\n",
    "    \n",
    "    # Find bounding box coordinates\n",
    "    coords = np.array(np.nonzero(mask))\n",
    "    min_coords = np.min(coords, axis=1)\n",
    "    max_coords = np.max(coords, axis=1)\n",
    "    \n",
    "    # Crop to bounding box\n",
    "    cropped_data = data_stripped[\n",
    "        min_coords[0]:max_coords[0]+1, \n",
    "        min_coords[1]:max_coords[1]+1, \n",
    "        min_coords[2]:max_coords[2]+1\n",
    "    ]\n",
    "    \n",
    "    # Adjust affine for cropped region\n",
    "    new_affine = img.affine.copy()\n",
    "    new_affine[:3, 3] = img.affine[:3, 3] + img.affine[:3, :3] @ min_coords\n",
    "    \n",
    "    return nib.Nifti1Image(cropped_data, new_affine)\n",
    "\n",
    "def intensity_normalization(img):\n",
    "    \"\"\"Standardize intensity values to z-scores within brain region.\"\"\"\n",
    "    data = img.get_fdata()\n",
    "    mask = data > 0  # Brain mask\n",
    "    \n",
    "    if np.sum(mask) > 0:  # Ensure we have brain voxels\n",
    "        # Calculate mean and std of brain region\n",
    "        mean = np.mean(data[mask])\n",
    "        std = np.std(data[mask])\n",
    "        \n",
    "        if std > 0:  # Avoid division by zero\n",
    "            # Z-score normalization\n",
    "            data_norm = np.zeros_like(data)\n",
    "            data_norm[mask] = (data[mask] - mean) / std\n",
    "        else:\n",
    "            data_norm = data - mean  # If std=0, just subtract mean\n",
    "    else:\n",
    "        data_norm = data\n",
    "        \n",
    "    return nib.Nifti1Image(data_norm, img.affine)\n",
    "\n",
    "def histogram_equalization(img):\n",
    "    \"\"\"Apply histogram equalization to enhance contrast.\"\"\"\n",
    "    data = img.get_fdata()\n",
    "    mask = data > 0  # Only process brain voxels\n",
    "    \n",
    "    if np.sum(mask) > 0:\n",
    "        # Get brain voxels\n",
    "        brain_voxels = data[mask]\n",
    "        \n",
    "        # Apply histogram equalization\n",
    "        p2, p98 = np.percentile(brain_voxels, (2, 98))\n",
    "        brain_voxels_eq = exposure.rescale_intensity(brain_voxels, in_range=(p2, p98))\n",
    "        \n",
    "        # Put equalized values back\n",
    "        data_eq = np.zeros_like(data)\n",
    "        data_eq[mask] = brain_voxels_eq\n",
    "        \n",
    "        return nib.Nifti1Image(data_eq, img.affine)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def extract_patches(data, patch_size=PATCH_SIZE, overlap=0.25):\n",
    "    \"\"\"Extract 2D patches from 3D volume with overlap.\"\"\"\n",
    "    depth, height, width = data.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    \n",
    "    # Calculate stride with overlap\n",
    "    stride_h = int(patch_h * (1 - overlap))\n",
    "    stride_w = int(patch_w * (1 - overlap))\n",
    "    \n",
    "    patches = []\n",
    "    patch_coords = []\n",
    "    \n",
    "    # Extract patches from each slice\n",
    "    for z in range(depth):\n",
    "        for y in range(0, height - patch_h + 1, stride_h):\n",
    "            for x in range(0, width - patch_w + 1, stride_w):\n",
    "                patch = data[z, y:y+patch_h, x:x+patch_w]\n",
    "                patches.append(patch)\n",
    "                patch_coords.append((z, y, x))\n",
    "    \n",
    "    return patches, patch_coords\n",
    "\n",
    "def data_augmentation(img, random_state=None):\n",
    "    \"\"\"Apply multiple data augmentation techniques.\"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "        \n",
    "    data = img.get_fdata()\n",
    "    augmented_data = data.copy()\n",
    "    \n",
    "    # Random rotation\n",
    "    if random_state.rand() > 0.5:\n",
    "        angle = random_state.uniform(-15, 15)\n",
    "        axes = random_state.choice([(0, 1), (0, 2), (1, 2)])\n",
    "        augmented_data = rotate(augmented_data, angle=angle, axes=axes, reshape=False, mode='constant')\n",
    "    \n",
    "    # Random elastic deformation\n",
    "    if random_state.rand() > 0.5:\n",
    "        shape = augmented_data.shape\n",
    "        alpha = random_state.uniform(30, 70)  # Controls deformation strength\n",
    "        sigma = random_state.uniform(4, 8)    # Controls smoothness\n",
    "        \n",
    "        # Generate displacement fields\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "        dz = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "        \n",
    "        # Create mesh grid and apply deformation\n",
    "        x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n",
    "        indices = (x + dx).reshape(-1), (y + dy).reshape(-1), (z + dz).reshape(-1)\n",
    "        augmented_data = map_coordinates(augmented_data, indices, order=1, mode='reflect').reshape(shape)\n",
    "    \n",
    "    # Random intensity scaling\n",
    "    if random_state.rand() > 0.5:\n",
    "        scale_factor = random_state.uniform(0.9, 1.1)\n",
    "        augmented_data = augmented_data * scale_factor\n",
    "    \n",
    "    # Random gamma correction\n",
    "    if random_state.rand() > 0.5:\n",
    "        gamma = random_state.uniform(0.8, 1.2)\n",
    "        augmented_data = np.power(augmented_data, gamma)\n",
    "    \n",
    "    return nib.Nifti1Image(augmented_data, img.affine)\n",
    "\n",
    "def visualize_preprocessing(original_img, processed_img, title, save_path):\n",
    "    \"\"\"Create a visualization of original vs processed images across all axes.\"\"\"\n",
    "    original_data = original_img.get_fdata()\n",
    "    processed_data = processed_img.get_fdata()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    for i, axis in enumerate([0, 1, 2]):\n",
    "        # Middle slice for each axis\n",
    "        mid_slice = original_data.shape[axis] // 2\n",
    "        \n",
    "        # Original image\n",
    "        if axis == 0:\n",
    "            axes[0, i].imshow(original_data[mid_slice, :, :], cmap='gray')\n",
    "            axes[1, i].imshow(processed_data[mid_slice, :, :], cmap='gray')\n",
    "        elif axis == 1:\n",
    "            axes[0, i].imshow(original_data[:, mid_slice, :], cmap='gray')\n",
    "            axes[1, i].imshow(processed_data[:, mid_slice, :], cmap='gray')\n",
    "        else:\n",
    "            axes[0, i].imshow(original_data[:, :, mid_slice], cmap='gray')\n",
    "            axes[1, i].imshow(processed_data[:, :, mid_slice], cmap='gray')\n",
    "        \n",
    "        axes[0, i].set_title(f'Original - Axis {axis}')\n",
    "        axes[1, i].set_title(f'Processed - Axis {axis}')\n",
    "        \n",
    "        # Turn off axis ticks\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def process_single_file(input_file, output_file, modality=None, save_visualization=False, vis_sample_idx=None):\n",
    "    \"\"\"Process a single MRI file based on the paper's methodology.\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing {input_file}...\")\n",
    "        img = load_nifti(input_file)\n",
    "        original_img = img  # Save original for visualization\n",
    "        \n",
    "        # Only process imaging data (not segmentation maps)\n",
    "        if modality != 'seg':\n",
    "            # 1. Resampling\n",
    "            img = resample_volume(img)\n",
    "            \n",
    "            # 2. Skull stripping (not applied to segmentation masks)\n",
    "            img = skull_strip(img)\n",
    "            \n",
    "            # 3. Intensity normalization\n",
    "            img = intensity_normalization(img)\n",
    "            \n",
    "            # 4. Contrast enhancement\n",
    "            img = histogram_equalization(img)\n",
    "            \n",
    "            # 5. (Optional) Data augmentation - only applied if specified\n",
    "            if os.environ.get('AUGMENT_DATA', 'False').lower() == 'true':\n",
    "                img = data_augmentation(img)\n",
    "        else:\n",
    "            # For segmentation masks, just resample to match image dimensions\n",
    "            img = resample_volume(img)\n",
    "            \n",
    "        # Save the processed file\n",
    "        save_nifti(img.get_fdata(), img.affine, output_file)\n",
    "        \n",
    "        # Create visualization if requested and this is a sample case\n",
    "        if save_visualization and vis_sample_idx is not None:\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            vis_path = output_file.replace('.nii.gz', f'_visualization_{modality}.png')\n",
    "            visualize_preprocessing(original_img, img, f'BraTS - {modality.upper()} - Case {vis_sample_idx}', vis_path)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_brats_dataset(input_root, output_root, num_visualization_samples=5):\n",
    "    \"\"\"Process the entire BraTS dataset according to paper's methodology.\"\"\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    \n",
    "    # Collect all .nii.gz files\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for f in files:\n",
    "            if f.endswith('.nii.gz'):\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    \n",
    "    # Group files by patient\n",
    "    patient_files = {}\n",
    "    for file_path in all_files:\n",
    "        # Extract patient ID\n",
    "        rel_path = os.path.relpath(file_path, input_root)\n",
    "        parts = rel_path.split(os.sep)\n",
    "        patient_id = parts[0]  # Assuming first folder is patient ID\n",
    "        \n",
    "        if patient_id not in patient_files:\n",
    "            patient_files[patient_id] = []\n",
    "        \n",
    "        patient_files[patient_id].append(file_path)\n",
    "    \n",
    "    # Randomly select patients for visualization\n",
    "    all_patients = list(patient_files.keys())\n",
    "    vis_patients = random.sample(all_patients, min(num_visualization_samples, len(all_patients)))\n",
    "    \n",
    "    # Process all files\n",
    "    total_success = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    for patient_idx, patient_id in enumerate(tqdm(all_patients, desc=\"Processing patients\")):\n",
    "        for file_path in patient_files[patient_id]:\n",
    "            rel_path = os.path.relpath(file_path, input_root)\n",
    "            output_path = os.path.join(output_root, rel_path)\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Determine modality\n",
    "            modality = None\n",
    "            if '_t1.' in file_path.lower() or '_t1_' in file_path.lower():\n",
    "                modality = 't1'\n",
    "            elif '_t1ce.' in file_path.lower() or '_t1ce_' in file_path.lower():\n",
    "                modality = 't1ce'\n",
    "            elif '_t2.' in file_path.lower() or '_t2_' in file_path.lower():\n",
    "                modality = 't2'\n",
    "            elif '_flair.' in file_path.lower() or '_flair_' in file_path.lower():\n",
    "                modality = 'flair'\n",
    "            elif '_seg.' in file_path.lower() or '_seg_' in file_path.lower():\n",
    "                modality = 'seg'\n",
    "            \n",
    "            # Save visualization for selected patients\n",
    "            save_vis = patient_id in vis_patients\n",
    "            vis_idx = vis_patients.index(patient_id) if save_vis else None\n",
    "            \n",
    "            if modality:\n",
    "                success = process_single_file(file_path, output_path, modality, save_vis, vis_idx)\n",
    "                if success:\n",
    "                    total_success += 1\n",
    "                else:\n",
    "                    total_failed += 1\n",
    "                    print(f\"Failed to process {file_path}\")\n",
    "    \n",
    "    print(f\"\\nProcessing completed: {total_success} files processed successfully, {total_failed} files failed.\")\n",
    "\n",
    "def generate_metadata_csv(input_root, output_csv):\n",
    "    \"\"\"Generate a CSV with metadata about all processed files.\"\"\"\n",
    "    metadata = []\n",
    "    \n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for f in files:\n",
    "            if f.endswith('.nii.gz'):\n",
    "                file_path = os.path.join(root, f)\n",
    "                rel_path = os.path.relpath(file_path, input_root)\n",
    "                \n",
    "                # Determine modality\n",
    "                modality = None\n",
    "                if '_t1.' in f.lower() or '_t1_' in f.lower():\n",
    "                    modality = 't1'\n",
    "                elif '_t1ce.' in f.lower() or '_t1ce_' in f.lower():\n",
    "                    modality = 't1ce'\n",
    "                elif '_t2.' in f.lower() or '_t2_' in f.lower():\n",
    "                    modality = 't2'\n",
    "                elif '_flair.' in f.lower() or '_flair_' in f.lower():\n",
    "                    modality = 'flair'\n",
    "                elif '_seg.' in f.lower() or '_seg_' in f.lower():\n",
    "                    modality = 'seg'\n",
    "                \n",
    "                try:\n",
    "                    # Load image to get shape and other metadata\n",
    "                    img = load_nifti(file_path)\n",
    "                    data = img.get_fdata()\n",
    "                    \n",
    "                    # Calculate statistics for non-zero values (brain tissue)\n",
    "                    non_zero = data[data > 0]\n",
    "                    if len(non_zero) > 0:\n",
    "                        mean_intensity = np.mean(non_zero)\n",
    "                        std_intensity = np.std(non_zero)\n",
    "                        min_intensity = np.min(non_zero)\n",
    "                        max_intensity = np.max(non_zero)\n",
    "                    else:\n",
    "                        mean_intensity = std_intensity = min_intensity = max_intensity = 0\n",
    "                    \n",
    "                    # Get patient ID from directory structure\n",
    "                    parts = rel_path.split(os.sep)\n",
    "                    patient_id = parts[0]  # Assuming first folder is patient ID\n",
    "                    \n",
    "                    metadata.append({\n",
    "                        'file_path': rel_path,\n",
    "                        'patient_id': patient_id,\n",
    "                        'modality': modality,\n",
    "                        'shape_x': data.shape[0],\n",
    "                        'shape_y': data.shape[1],\n",
    "                        'shape_z': data.shape[2],\n",
    "                        'mean_intensity': mean_intensity,\n",
    "                        'std_intensity': std_intensity,\n",
    "                        'min_intensity': min_intensity,\n",
    "                        'max_intensity': max_intensity,\n",
    "                        'voxel_count': data.size,\n",
    "                        'non_zero_voxel_count': len(non_zero)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting metadata for {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Metadata saved to {output_csv}\")\n",
    "\n",
    "def visualize_dataset_statistics(metadata_csv, output_dir):\n",
    "    \"\"\"Generate visualizations of dataset statistics.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "    \n",
    "    # 1. Distribution of intensities by modality\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "    for modality in modalities:\n",
    "        data = df[df['modality'] == modality]['mean_intensity']\n",
    "        if not data.empty:\n",
    "            plt.hist(data, bins=50, alpha=0.5, label=modality)\n",
    "    plt.title('Distribution of Mean Intensities by Modality')\n",
    "    plt.xlabel('Mean Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'intensity_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Volume statistics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    df['volume'] = df['non_zero_voxel_count'] / (df['shape_x'] * df['shape_y'] * df['shape_z'])\n",
    "    \n",
    "    for modality in modalities:\n",
    "        data = df[df['modality'] == modality]['volume']\n",
    "        if not data.empty:\n",
    "            plt.hist(data, bins=50, alpha=0.5, label=modality)\n",
    "    plt.title('Distribution of Brain Volume Ratios by Modality')\n",
    "    plt.xlabel('Volume Ratio (non-zero voxels / total voxels)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'volume_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Box plots of intensity statistics\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    df_subset = df[df['modality'].isin(modalities)]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns_plot = sns.boxplot(x='modality', y='mean_intensity', data=df_subset)\n",
    "    plt.title('Mean Intensity by Modality')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns_plot = sns.boxplot(x='modality', y='std_intensity', data=df_subset)\n",
    "    plt.title('Intensity Standard Deviation by Modality')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'intensity_boxplots.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_patch_dataset(input_root, output_dir, patch_size=PATCH_SIZE, samples_per_patient=10):\n",
    "    \"\"\"Extract 2D patches from processed images and save as PNG files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Group files by patient\n",
    "    patient_files = {}\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for f in files:\n",
    "            if f.endswith('.nii.gz'):\n",
    "                file_path = os.path.join(root, f)\n",
    "                rel_path = os.path.relpath(file_path, input_root)\n",
    "                parts = rel_path.split(os.sep)\n",
    "                patient_id = parts[0]\n",
    "                \n",
    "                if patient_id not in patient_files:\n",
    "                    patient_files[patient_id] = []\n",
    "                \n",
    "                patient_files[patient_id].append((file_path, rel_path))\n",
    "    \n",
    "    # Process each patient\n",
    "    for patient_id, files in tqdm(patient_files.items(), desc=\"Extracting patches\"):\n",
    "        # Create directories for each modality\n",
    "        for modality in ['t1', 't1ce', 't2', 'flair', 'seg']:\n",
    "            os.makedirs(os.path.join(output_dir, modality), exist_ok=True)\n",
    "        \n",
    "        # Process each file for the patient\n",
    "        patient_data = {}\n",
    "        for file_path, rel_path in files:\n",
    "            # Determine modality\n",
    "            modality = None\n",
    "            if '_t1.' in file_path.lower() or '_t1_' in file_path.lower():\n",
    "                modality = 't1'\n",
    "            elif '_t1ce.' in file_path.lower() or '_t1ce_' in file_path.lower():\n",
    "                modality = 't1ce'\n",
    "            elif '_t2.' in file_path.lower() or '_t2_' in file_path.lower():\n",
    "                modality = 't2'\n",
    "            elif '_flair.' in file_path.lower() or '_flair_' in file_path.lower():\n",
    "                modality = 'flair'\n",
    "            elif '_seg.' in file_path.lower() or '_seg_' in file_path.lower():\n",
    "                modality = 'seg'\n",
    "            \n",
    "            if modality:\n",
    "                try:\n",
    "                    img = load_nifti(file_path)\n",
    "                    patient_data[modality] = img.get_fdata()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        \n",
    "        # Only proceed if we have all modalities\n",
    "        required_modalities = ['t1', 't1ce', 't2', 'flair', 'seg']\n",
    "        if all(m in patient_data for m in required_modalities):\n",
    "            # Find slices with tumor (non-zero segmentation)\n",
    "            tumor_slices = []\n",
    "            for z in range(patient_data['seg'].shape[0]):\n",
    "                if np.any(patient_data['seg'][z, :, :] > 0):\n",
    "                    tumor_slices.append(z)\n",
    "            \n",
    "            # Select random slices (prioritizing tumor slices)\n",
    "            if tumor_slices:\n",
    "                selected_slices = random.sample(tumor_slices, min(samples_per_patient, len(tumor_slices)))\n",
    "                # If we need more slices, add random non-tumor slices\n",
    "                if len(selected_slices) < samples_per_patient:\n",
    "                    non_tumor_slices = [z for z in range(patient_data['seg'].shape[0]) if z not in tumor_slices]\n",
    "                    if non_tumor_slices:\n",
    "                        more_slices = random.sample(non_tumor_slices, min(samples_per_patient - len(selected_slices), len(non_tumor_slices)))\n",
    "                        selected_slices.extend(more_slices)\n",
    "            else:\n",
    "                # If no tumor slices, just pick random slices\n",
    "                all_slices = list(range(patient_data['seg'].shape[0]))\n",
    "                selected_slices = random.sample(all_slices, min(samples_per_patient, len(all_slices)))\n",
    "            \n",
    "            # Extract and save patches for selected slices\n",
    "            for slice_idx in selected_slices:\n",
    "                for modality in required_modalities:\n",
    "                    slice_data = patient_data[modality][slice_idx, :, :]\n",
    "                    \n",
    "                    # Normalize for visualization (except segmentation)\n",
    "                    if modality != 'seg':\n",
    "                        non_zero = slice_data[slice_data > 0]\n",
    "                        if len(non_zero) > 0:\n",
    "                            p2, p98 = np.percentile(non_zero, (2, 98))\n",
    "                            slice_data = exposure.rescale_intensity(slice_data, in_range=(p2, p98), out_range=(0, 255))\n",
    "                    else:\n",
    "                        # For segmentation, just scale to 0-255\n",
    "                        if np.max(slice_data) > 0:\n",
    "                            slice_data = (slice_data / np.max(slice_data) * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Save as PNG\n",
    "                    output_file = os.path.join(output_dir, modality, f\"{patient_id}_slice{slice_idx}.png\")\n",
    "                    cv2.imwrite(output_file, slice_data.astype(np.uint8))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the preprocessing pipeline.\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='MRI Brain Tumor Preprocessing Pipeline')\n",
    "    parser.add_argument('--input', required=True, help='Input directory containing BraTS dataset')\n",
    "    parser.add_argument('--output', required=True, help='Output directory for processed files')\n",
    "    parser.add_argument('--visualize', action='store_true', help='Generate visualization samples')\n",
    "    parser.add_argument('--metadata', action='store_true', help='Generate metadata CSV')\n",
    "    parser.add_argument('--patches', action='store_true', help='Extract 2D patches from processed data')\n",
    "    parser.add_argument('--augment', action='store_true', help='Apply data augmentation')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Set environment variable for augmentation\n",
    "    if args.augment:\n",
    "        os.environ['AUGMENT_DATA'] = 'True'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "    \n",
    "    # Process the BraTS dataset\n",
    "    process_brats_dataset(args.input, args.output, num_visualization_samples=5 if args.visualize else 0)\n",
    "    \n",
    "    # Generate metadata if requested\n",
    "    if args.metadata:\n",
    "        metadata_csv = os.path.join(args.output, 'dataset_metadata.csv')\n",
    "        generate_metadata_csv(args.output, metadata_csv)\n",
    "        \n",
    "        # Generate visualizations of dataset statistics\n",
    "        stats_dir = os.path.join(args.output, 'statistics')\n",
    "        visualize_dataset_statistics(metadata_csv, stats_dir)\n",
    "    \n",
    "    # Extract patches if requested\n",
    "    if args.patches:\n",
    "        patches_dir = os.path.join(args.output, 'patches')\n",
    "        create_patch_dataset(args.output, patches_dir, samples_per_patient=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Handle import error for seaborn (used in visualization)\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "    except ImportError:\n",
    "        print(\"Warning: seaborn not installed. Some visualizations may not work.\")\n",
    "        # Define a simple replacement for sns.boxplot\n",
    "        def boxplot(*args, **kwargs):\n",
    "            data = kwargs.get('data')\n",
    "            x = kwargs.get('x')\n",
    "            y = kwargs.get('y')\n",
    "            if data is not None and x is not None and y is not None:\n",
    "                groups = data.groupby(x)[y].apply(list)\n",
    "                plt.boxplot(groups.values)\n",
    "                plt.xticks(range(1, len(groups) + 1), groups.index)\n",
    "            return plt.gca()\n",
    "        \n",
    "        # Create mock sns module\n",
    "        class MockSNS:\n",
    "            def boxplot(self, *args, **kwargs):\n",
    "                return boxplot(*args, **kwargs)\n",
    "        \n",
    "        sns = MockSNS()\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:11.050156Z",
     "iopub.status.busy": "2025-03-14T14:31:11.049830Z",
     "iopub.status.idle": "2025-03-14T14:31:11.060442Z",
     "shell.execute_reply": "2025-03-14T14:31:11.059543Z",
     "shell.execute_reply.started": "2025-03-14T14:31:11.050136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Calculate average Dice coefficient across all classes\n",
    "    Args:\n",
    "        y_true: ground truth masks\n",
    "        y_pred: predicted masks\n",
    "        smooth: smoothing factor\n",
    "    Returns:\n",
    "        mean Dice coefficient across all classes\n",
    "    \"\"\"\n",
    "    class_num = 4  # Number of classes\n",
    "    dice_list = []\n",
    "    \n",
    "    for i in range(class_num):\n",
    "        # Reshape the tensors into 1D arrays for each class\n",
    "        y_true_f = y_true[:, i].reshape(-1)\n",
    "        y_pred_f = y_pred[:, i].reshape(-1)\n",
    "        \n",
    "        # Compute intersection and Dice coefficient\n",
    "        intersection = torch.sum(y_true_f * y_pred_f)\n",
    "        union = torch.sum(y_true_f) + torch.sum(y_pred_f)\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_list.append(dice)\n",
    "\n",
    "    return torch.mean(torch.stack(dice_list))\n",
    "\n",
    "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient for necrotic tumor core (label 1)\"\"\"\n",
    "    intersection = torch.sum(torch.abs(y_true[:, 1] * y_pred[:, 1]))\n",
    "    union = torch.sum(torch.square(y_true[:, 1])) + torch.sum(torch.square(y_pred[:, 1]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient for peritumoral edema (label 2)\"\"\"\n",
    "    intersection = torch.sum(torch.abs(y_true[:, 2] * y_pred[:, 2]))\n",
    "    union = torch.sum(torch.square(y_true[:, 2])) + torch.sum(torch.square(y_pred[:, 2]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient for enhancing tumor (label 3)\"\"\"\n",
    "    intersection = torch.sum(torch.abs(y_true[:, 3] * y_pred[:, 3]))\n",
    "    union = torch.sum(torch.square(y_true[:, 3])) + torch.sum(torch.square(y_pred[:, 3]))\n",
    "    return (2. * intersection) / (union + epsilon)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate precision\n",
    "    \"\"\"\n",
    "    true_positives = torch.sum(torch.round(torch.clamp(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = torch.sum(torch.round(torch.clamp(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + torch.finfo(torch.float32).eps)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate sensitivity (recall)\n",
    "    \"\"\"\n",
    "    true_positives = torch.sum(torch.round(torch.clamp(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = torch.sum(torch.round(torch.clamp(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + torch.finfo(torch.float32).eps)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate specificity\n",
    "    \"\"\"\n",
    "    true_negatives = torch.sum(torch.round(torch.clamp((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = torch.sum(torch.round(torch.clamp(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + torch.finfo(torch.float32).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Safe Dice coefficient to prevent NaN values\n",
    "# def safe_dice_coef(y_true, y_pred, epsilon=1e-7):\n",
    "#     # Clip predictions to prevent numerical instability\n",
    "#     y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "    \n",
    "#     # Flatten the tensors\n",
    "#     y_true_f = y_true.view(-1)\n",
    "#     y_pred_f = y_pred.view(-1)\n",
    "    \n",
    "#     # Compute dice coefficient\n",
    "#     intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "#     # Use smooth factor for numerical stability\n",
    "#     return (2. * intersection + epsilon) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + epsilon)\n",
    "\n",
    "# # Safe versions of each metric function (assuming these are your existing metric functions)\n",
    "# def dice_coef(y_true, y_pred):\n",
    "#     return safe_dice_coef(y_true, y_pred)\n",
    "\n",
    "# def dice_coef_necrotic(y_true, y_pred):\n",
    "#     # Assuming y_true and y_pred are multi-channel tensors and necrotic is first channel\n",
    "#     # Adjust channel index as needed based on your data format\n",
    "#     return safe_dice_coef(y_true[:, 0:1], y_pred[:, 0:1])\n",
    "\n",
    "# def dice_coef_edema(y_true, y_pred):\n",
    "#     # Assuming edema is second channel \n",
    "#     return safe_dice_coef(y_true[:, 1:2], y_pred[:, 1:2])\n",
    "\n",
    "# def dice_coef_enhancing(y_true, y_pred):\n",
    "#     # Assuming enhancing is third channel\n",
    "#     return safe_dice_coef(y_true[:, 2:3], y_pred[:, 2:3])\n",
    "\n",
    "# def precision(y_true, y_pred, epsilon=1e-7):\n",
    "#     y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "#     y_true_f = y_true.view(-1)\n",
    "#     y_pred_f = y_pred.view(-1)\n",
    "    \n",
    "#     # True positives\n",
    "#     true_positives = torch.sum(y_true_f * y_pred_f)\n",
    "#     # All positives (true and false)\n",
    "#     all_positives = torch.sum(y_pred_f)\n",
    "    \n",
    "#     return (true_positives + epsilon) / (all_positives + epsilon)\n",
    "\n",
    "# def sensitivity(y_true, y_pred, epsilon=1e-7):\n",
    "#     y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "#     y_true_f = y_true.view(-1)\n",
    "#     y_pred_f = y_pred.view(-1)\n",
    "    \n",
    "#     # True positives\n",
    "#     true_positives = torch.sum(y_true_f * y_pred_f)\n",
    "#     # All actual positives\n",
    "#     actual_positives = torch.sum(y_true_f)\n",
    "    \n",
    "#     return (true_positives + epsilon) / (actual_positives + epsilon)\n",
    "\n",
    "# def specificity(y_true, y_pred, epsilon=1e-7):\n",
    "#     y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "#     y_true_f = y_true.view(-1)\n",
    "#     y_pred_f = y_pred.view(-1)\n",
    "    \n",
    "#     # True negatives\n",
    "#     true_negatives = torch.sum((1 - y_true_f) * (1 - y_pred_f))\n",
    "#     # All actual negatives\n",
    "#     actual_negatives = torch.sum(1 - y_true_f)\n",
    "    \n",
    "#     return (true_negatives + epsilon) / (actual_negatives + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:18.316751Z",
     "iopub.status.busy": "2025-03-14T14:31:18.316399Z",
     "iopub.status.idle": "2025-03-14T14:31:18.325243Z",
     "shell.execute_reply": "2025-03-14T14:31:18.324494Z",
     "shell.execute_reply.started": "2025-03-14T14:31:18.316724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "#train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories); \n",
    "\n",
    "    \n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:21.621493Z",
     "iopub.status.busy": "2025-03-14T14:31:21.621089Z",
     "iopub.status.idle": "2025-03-14T14:31:21.636916Z",
     "shell.execute_reply": "2025-03-14T14:31:21.635937Z",
     "shell.execute_reply.started": "2025-03-14T14:31:21.621461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BrainMRIDataset(Dataset):\n",
    "    def __init__(self, list_IDs, img_size=240, n_channels=2, shuffle=True, \n",
    "                 volume_slices=155, volume_start_at=0, train_dataset_path=''):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (img_size, img_size)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.volume_slices = volume_slices\n",
    "        self.volume_start_at = volume_start_at\n",
    "        self.train_dataset_path = train_dataset_path\n",
    "        self.indexes = np.arange(len(self.list_IDs) * self.volume_slices)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Total number of slices across all volumes'\n",
    "        return len(self.list_IDs) * self.volume_slices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one slice of data'\n",
    "        # Calculate which volume and slice to use\n",
    "        volume_idx = index // self.volume_slices\n",
    "        slice_idx = index % self.volume_slices + self.volume_start_at\n",
    "        \n",
    "        # Get the case ID\n",
    "        case_id = self.list_IDs[volume_idx]\n",
    "        case_path = os.path.join(self.train_dataset_path, case_id)\n",
    "\n",
    "        # Load the specific slice from MRI images\n",
    "        flair = nib.load(os.path.join(case_path, f'{case_id}_flair.nii.gz')).get_fdata()\n",
    "        ce = nib.load(os.path.join(case_path, f'{case_id}_t1ce.nii.gz')).get_fdata()\n",
    "        seg = nib.load(os.path.join(case_path, f'{case_id}_seg.nii.gz')).get_fdata()\n",
    "\n",
    "        # Get the specific slice and resize\n",
    "        X = np.zeros((*self.dim, self.n_channels), dtype=np.float32)\n",
    "        X[:, :, 0] = cv2.resize(flair[:, :, slice_idx], self.dim)\n",
    "        X[:, :, 1] = cv2.resize(ce[:, :, slice_idx], self.dim)\n",
    "        \n",
    "        # Process segmentation mask\n",
    "        y = seg[:, :, slice_idx]\n",
    "        y[y == 4] = 3  # Remap class 4 to 3\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        X = torch.FloatTensor(X).permute(2, 0, 1)  # Change to (C, H, W)\n",
    "        X = X / (torch.max(X) + 1e-8)\n",
    "        \n",
    "        # Convert mask to one-hot encoding\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        Y = F.one_hot(y, num_classes=4).permute(2, 0, 1).float()  # (num_classes, H, W)\n",
    "        Y = F.interpolate(Y.unsqueeze(0), size=self.dim, mode='nearest').squeeze(0)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "def get_data_loaders(train_ids, val_ids, test_ids, batch_size=16, **dataset_kwargs):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for train, validation, and test sets\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = BrainMRIDataset(train_ids, **dataset_kwargs)\n",
    "    val_dataset = BrainMRIDataset(val_ids, **dataset_kwargs)\n",
    "    test_dataset = BrainMRIDataset(test_ids, **dataset_kwargs)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:25.184512Z",
     "iopub.status.busy": "2025-03-14T14:31:25.184228Z",
     "iopub.status.idle": "2025-03-14T14:31:25.191491Z",
     "shell.execute_reply": "2025-03-14T14:31:25.190510Z",
     "shell.execute_reply.started": "2025-03-14T14:31:25.184492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    train_ids=train_ids,\n",
    "    val_ids=val_ids,\n",
    "    test_ids=test_ids,\n",
    "    batch_size=32,\n",
    "    img_size=240,\n",
    "    n_channels=2,\n",
    "    volume_slices=100,\n",
    "    volume_start_at=22,\n",
    "    train_dataset_path=TRAIN_DATASET_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_data_loaders(train_ids, val_ids, test_ids, batch_size=16, world_size=None, rank=None, **dataset_kwargs):\n",
    "#     \"\"\"\n",
    "#     Create PyTorch DataLoaders optimized for GPU processing with distributed training support\n",
    "    \n",
    "#     Args:\n",
    "#         train_ids: IDs for training set\n",
    "#         val_ids: IDs for validation set\n",
    "#         test_ids: IDs for test set\n",
    "#         batch_size: Batch size per GPU\n",
    "#         world_size: Number of GPUs (for distributed training)\n",
    "#         rank: Current GPU rank (for distributed training)\n",
    "#         **dataset_kwargs: Additional arguments for dataset creation\n",
    "#     \"\"\"\n",
    "#     # Create datasets\n",
    "#     train_dataset = BrainMRIDataset(train_ids, **dataset_kwargs)\n",
    "#     val_dataset = BrainMRIDataset(val_ids, **dataset_kwargs)\n",
    "#     test_dataset = BrainMRIDataset(test_ids, **dataset_kwargs)\n",
    "    \n",
    "#     # Set up for distributed training if applicable\n",
    "#     train_sampler = None\n",
    "#     val_sampler = None\n",
    "#     test_sampler = None\n",
    "    \n",
    "#     if world_size is not None and rank is not None:\n",
    "#         # Create distributed samplers for multi-GPU training\n",
    "#         train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "#             train_dataset,\n",
    "#             num_replicas=world_size,\n",
    "#             rank=rank,\n",
    "#             shuffle=True\n",
    "#         )\n",
    "        \n",
    "#         val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "#             val_dataset,\n",
    "#             num_replicas=world_size,\n",
    "#             rank=rank,\n",
    "#             shuffle=False\n",
    "#         )\n",
    "        \n",
    "#         test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "#             test_dataset,\n",
    "#             num_replicas=world_size,\n",
    "#             rank=rank,\n",
    "#             shuffle=False\n",
    "#         )\n",
    "        \n",
    "#         # For distributed training, don't shuffle in dataloader as sampler handles it\n",
    "#         shuffle = False\n",
    "#     else:\n",
    "#         # For single GPU, use regular shuffling\n",
    "#         shuffle = True\n",
    "    \n",
    "#     # Performance optimizations for data loading\n",
    "#     # Create data loaders with optimized parameters\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=shuffle if train_sampler is None else False,\n",
    "#         sampler=train_sampler,\n",
    "#         num_workers=4,  # 4 workers per GPU is typically optimal\n",
    "#         pin_memory=True,  # Faster data transfer to GPU\n",
    "#         drop_last=True,  # Avoid smaller batches, helpful for BatchNorm\n",
    "#         persistent_workers=True,  # Keep workers alive between epochs\n",
    "#         prefetch_factor=2  # Prefetch 2 batches per worker for smoother processing\n",
    "#     )\n",
    "    \n",
    "#     val_loader = DataLoader(\n",
    "#         val_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=False,\n",
    "#         sampler=val_sampler,\n",
    "#         num_workers=4, \n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=True,\n",
    "#         prefetch_factor=2\n",
    "#     )\n",
    "    \n",
    "#     test_loader = DataLoader(\n",
    "#         test_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=False,\n",
    "#         sampler=test_sampler,\n",
    "#         num_workers=4, \n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=True,\n",
    "#         prefetch_factor=2\n",
    "#     )\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# # Example usage for multi-GPU training:\n",
    "# def create_data_loaders_for_distributed(train_ids, val_ids, test_ids, batch_size, world_size, rank, **kwargs):\n",
    "#     \"\"\"Helper function to create data loaders for distributed training\"\"\"\n",
    "#     return get_data_loaders(\n",
    "#         train_ids=train_ids,\n",
    "#         val_ids=val_ids,\n",
    "#         test_ids=test_ids,\n",
    "#         batch_size=batch_size,  # This will be the per-GPU batch size\n",
    "#         world_size=world_size,\n",
    "#         rank=rank,\n",
    "#         **kwargs\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Example usage for single-GPU training:\n",
    "# def create_data_loaders_for_single_gpu(train_ids, val_ids, test_ids, batch_size, **kwargs):\n",
    "#     \"\"\"Helper function to create data loaders for single GPU training\"\"\"\n",
    "#     return get_data_loaders(\n",
    "#         train_ids=train_ids,\n",
    "#         val_ids=val_ids,\n",
    "#         test_ids=test_ids,\n",
    "#         batch_size=batch_size,\n",
    "#         **kwargs\n",
    "#     )\n",
    "\n",
    "\n",
    "# # # Use in your training script:\n",
    "# # if __name__ == '__main__':\n",
    "    \n",
    "# #     # Set environment variables before initializing process group\n",
    "# #     os.environ['RANK'] = '0'  # This process's rank\n",
    "# #     os.environ['WORLD_SIZE'] = '1'  # Total number of processes\n",
    "# #     os.environ['MASTER_ADDR'] = 'localhost'  # Address of the master process\n",
    "# #     os.environ['MASTER_PORT'] = '29500'  # Port for communication\n",
    "    \n",
    "# #     world_size = torch.cuda.device_count()\n",
    "# #     dist.init_process_group(backend='nccl')\n",
    "# #     rank = dist.get_rank()\n",
    "    \n",
    "# #     # For single GPU usage\n",
    "# #     # train_loader, val_loader, test_loader = create_data_loaders_for_single_gpu(\n",
    "# #     #     train_ids=train_ids,\n",
    "# #     #     val_ids=val_ids,\n",
    "# #     #     test_ids=test_ids,\n",
    "# #     #     batch_size=32,\n",
    "# #     #     img_size=240,\n",
    "# #     #     n_channels=2,\n",
    "# #     #     volume_slices=100,\n",
    "# #     #     volume_start_at=22,\n",
    "# #     #     train_dataset_path=TRAIN_DATASET_PATH\n",
    "# #     # )\n",
    "    \n",
    "# #     # For multi-GPU usage inside the train_ddp function\n",
    "# #     train_loader, val_loader, test_loader = create_data_loaders_for_distributed(\n",
    "# #         train_ids=train_ids,\n",
    "# #         val_ids=val_ids,\n",
    "# #         test_ids=test_ids,\n",
    "# #         batch_size=16,  # 16 per GPU = 32 total across 2 GPUs\n",
    "# #         world_size=world_size,\n",
    "# #         rank=rank,\n",
    "# #         img_size=240,\n",
    "# #         n_channels=2,\n",
    "# #         volume_slices=100,\n",
    "# #         volume_start_at=22,\n",
    "# #         train_dataset_path=TRAIN_DATASET_PATH\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:30.835242Z",
     "iopub.status.busy": "2025-03-14T14:31:30.834922Z",
     "iopub.status.idle": "2025-03-14T14:31:30.854574Z",
     "shell.execute_reply": "2025-03-14T14:31:30.853836Z",
     "shell.execute_reply.started": "2025-03-14T14:31:30.835217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, transformer_name=\"huawei-noah/TinyBERT_General_4L_312D\", embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.transformer_name = transformer_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Load transformer\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        self.transformer.eval()\n",
    "        \n",
    "        bert_hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Initial projection to match transformer dimensions\n",
    "        self.input_projection = nn.Conv2d(embedding_dim, bert_hidden_size, 1)\n",
    "        \n",
    "        # Layer norms for each stage\n",
    "        self.layer_norm1 = nn.LayerNorm(bert_hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(bert_hidden_size)\n",
    "        self.layer_norm3 = nn.LayerNorm(bert_hidden_size)\n",
    "        \n",
    "        # Dense layers\n",
    "        self.intermediate_dense = nn.Linear(bert_hidden_size, bert_hidden_size)\n",
    "        self.output_projection = nn.Conv2d(bert_hidden_size, embedding_dim, 1)\n",
    "        \n",
    "        self.pos_embedding = None\n",
    "        self.register_buffer('initialized_pos_embedding', torch.tensor(0))\n",
    "\n",
    "    def build(self, h, w):\n",
    "        device = self.initialized_pos_embedding.device\n",
    "        if self.pos_embedding is None:\n",
    "            bert_hidden_size = self.transformer.config.hidden_size\n",
    "            self.pos_embedding = nn.Parameter(\n",
    "                torch.randn(1, h * w, bert_hidden_size, device=device) * 0.02\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Ensure transformer is on the same device\n",
    "        if next(self.transformer.parameters()).device != device:\n",
    "            self.transformer = self.transformer.to(device)\n",
    "        \n",
    "        # Initial projection to match transformer dimensions\n",
    "        x = self.input_projection(x)  # B, transformer_dim, H, W\n",
    "        \n",
    "        # Reshape for transformer\n",
    "        x = x.permute(0, 2, 3, 1)  # B, H, W, transformer_dim\n",
    "        x = x.reshape(batch_size, h * w, -1)  # B, H*W, transformer_dim\n",
    "        \n",
    "        # Initialize position embeddings if needed\n",
    "        self.build(h, w)\n",
    "        \n",
    "        # Add position embeddings and normalize\n",
    "        x = x + self.pos_embedding.to(device)\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones((batch_size, h * w), dtype=torch.long, device=device)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer(inputs_embeds=x, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        # Process features\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.intermediate_dense(x)\n",
    "        x = F.gelu(x)\n",
    "        \n",
    "        x = self.layer_norm3(x)\n",
    "        \n",
    "        # Reshape back to spatial dimensions\n",
    "        x = x.reshape(batch_size, h, w, -1)\n",
    "        x = x.permute(0, 3, 1, 2)  # B, C, H, W\n",
    "        \n",
    "        # Project back to original dimension\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CustomUNetPlusPlusDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom UNet++ decoder that works with the spatial dimensions in your model.\n",
    "    This is a simplified implementation that should handle the specific feature map sizes.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels, decoder_channels=(256, 128, 64, 32, 16)):\n",
    "        super().__init__()\n",
    "        self.encoder_channels = encoder_channels\n",
    "        self.decoder_channels = decoder_channels\n",
    "        \n",
    "        # Create decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Build decoder blocks for each stage\n",
    "        for i in range(len(decoder_channels)):\n",
    "            # Calculate input channels for each block\n",
    "            if i == 0:\n",
    "                in_ch = encoder_channels[-1]\n",
    "            else:\n",
    "                in_ch = decoder_channels[i-1]\n",
    "                \n",
    "            # Add skip connection channels\n",
    "            if i < len(encoder_channels) - 1:\n",
    "                in_ch += encoder_channels[-i-2]\n",
    "                \n",
    "            # Create decoder block\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, decoder_channels[i], 3, padding=1),\n",
    "                nn.BatchNorm2d(decoder_channels[i]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(decoder_channels[i], decoder_channels[i], 3, padding=1),\n",
    "                nn.BatchNorm2d(decoder_channels[i]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            self.decoder_blocks.append(block)\n",
    "            \n",
    "        # Final segmentation head\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[-1], 4, 1)  # 4 output classes\n",
    "\n",
    "    def forward(self, *features):\n",
    "        features = features[::-1]  # Reverse to start from bottleneck\n",
    "        x = features[0]\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            # Resize to match the next feature map size if available\n",
    "            if i < len(features) - 1:\n",
    "                target_size = features[i+1].shape[2:]\n",
    "                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Concatenate with skip connection\n",
    "                x = torch.cat([x, features[i+1]], dim=1)\n",
    "            else:\n",
    "                # For the last block, upsample to twice the current size\n",
    "                x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                \n",
    "            x = decoder_block(x)\n",
    "            \n",
    "        # Final upsampling to input size if needed\n",
    "        if x.shape[2] < 240:  # Assuming your input is 240x240\n",
    "            x = F.interpolate(x, size=(240, 240), mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class ExtendedUNetPlusPlus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize encoder only\n",
    "        self.encoder = smp.encoders.get_encoder(\n",
    "            name=\"tu-resnet18\",\n",
    "            in_channels=2,\n",
    "            depth=5,\n",
    "            weights=\"imagenet\"\n",
    "        )\n",
    "        \n",
    "        # Get encoder channels info\n",
    "        # print(\"Encoder output channels:\", self.encoder.out_channels)\n",
    "        bottleneck_channels = self.encoder.out_channels[-1]  # Should be 512 for ResNet18\n",
    "        # print(f\"Bottleneck channels: {bottleneck_channels}\")\n",
    "        \n",
    "        # Initialize transformer with matching embedding dimension\n",
    "        self.transformer = TransformerBlock(embedding_dim=bottleneck_channels)\n",
    "        \n",
    "        # Modified fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(bottleneck_channels * 2, bottleneck_channels, 1),\n",
    "            nn.BatchNorm2d(bottleneck_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Initialize our custom decoder that can handle the specific spatial dimensions\n",
    "        self.decoder = CustomUNetPlusPlusDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Get encoder features\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Print original feature map sizes\n",
    "        # print(\"Original feature map sizes:\")\n",
    "        # for i, feat in enumerate(features):\n",
    "            # print(f\"Level {i}: {feat.shape}\")\n",
    "        \n",
    "        # Get bottleneck features\n",
    "        bottleneck = features[-1]\n",
    "        \n",
    "        # Process bottleneck through transformer\n",
    "        transformer_features = self.transformer(bottleneck)\n",
    "        # print(f\"Transformer output shape: {transformer_features.shape}\")\n",
    "        \n",
    "        # Ensure transformer output matches bottleneck spatial dimensions\n",
    "        if transformer_features.shape[2:] != bottleneck.shape[2:]:\n",
    "            transformer_features = F.interpolate(\n",
    "                transformer_features, \n",
    "                size=bottleneck.shape[2:], \n",
    "                mode=\"bilinear\", \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        enhanced_features = torch.cat([bottleneck, transformer_features], dim=1)\n",
    "        \n",
    "        # Fuse features back to original bottleneck size\n",
    "        enhanced_features = self.fusion(enhanced_features)\n",
    "        # print(f\"Enhanced features shape: {enhanced_features.shape}\")\n",
    "        \n",
    "        # Replace bottleneck with enhanced features\n",
    "        enhanced_features_list = list(features)\n",
    "        enhanced_features_list[-1] = enhanced_features\n",
    "        \n",
    "        # Print final feature map sizes\n",
    "        # print(\"Final feature map sizes before decoder:\")\n",
    "        # for i, feat in enumerate(enhanced_features_list):\n",
    "            # print(f\"Level {i}: {feat.shape}\")\n",
    "        \n",
    "        # Process through our custom decoder\n",
    "        decoder_output = self.decoder(*enhanced_features_list)\n",
    "        # print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "        \n",
    "        # Generate final segmentation masks\n",
    "        masks = self.decoder.segmentation_head(decoder_output)\n",
    "        # print(f\"Final output shape: {masks.shape}\")\n",
    "        \n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:37.852550Z",
     "iopub.status.busy": "2025-03-14T14:31:37.852215Z",
     "iopub.status.idle": "2025-03-14T14:31:52.044623Z",
     "shell.execute_reply": "2025-03-14T14:31:52.043368Z",
     "shell.execute_reply.started": "2025-03-14T14:31:37.852516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = ExtendedUNetPlusPlus().to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    # Create a simple summary similar to torchsummary but without hooks\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.zeros(1, *input_size, device=device, dtype=dtype)\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Print parameter count\n",
    "    print(f\"\\nTrainable parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Try to run a forward pass\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = model(x)\n",
    "            print(\"\\nForward pass successful with input shape:\", x.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nForward pass failed: {str(e)}\")\n",
    "    \n",
    "    # Print info about specific layers if needed\n",
    "    print(\"\\nLayer summary:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only print leaf modules\n",
    "            params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            print(f\"{name}: {module.__class__.__name__}, Parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:31:55.122132Z",
     "iopub.status.busy": "2025-03-14T14:31:55.121389Z",
     "iopub.status.idle": "2025-03-14T14:31:56.086018Z",
     "shell.execute_reply": "2025-03-14T14:31:56.085044Z",
     "shell.execute_reply.started": "2025-03-14T14:31:55.122098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instead of using torchsummary\n",
    "print_model_summary(model, input_size=(2, 240, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # For single GPU usage\n",
    "# train_loader, val_loader, test_loader = create_data_loaders_for_single_gpu(\n",
    "#         train_ids=train_ids,\n",
    "#         val_ids=val_ids,\n",
    "#         test_ids=test_ids,\n",
    "#         batch_size=32,\n",
    "#         img_size=240,\n",
    "#         n_channels=2,\n",
    "#         volume_slices=100,\n",
    "#         volume_start_at=22,\n",
    "#         train_dataset_path=TRAIN_DATASET_PATH\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Create data loaders AFTER initializing the process group\n",
    "# train_loader, val_loader, test_loader = create_data_loaders_for_distributed(\n",
    "#             train_ids=train_ids,\n",
    "#             val_ids=val_ids,\n",
    "#             test_ids=test_ids,\n",
    "#             batch_size=16,  # 16 per GPU = 32 total across 2 GPUs\n",
    "#             world_size=world_size,\n",
    "#             rank=rank,\n",
    "#             img_size=240,\n",
    "#             n_channels=2,\n",
    "#             volume_slices=100,\n",
    "#             volume_start_at=22,\n",
    "#             train_dataset_path=TRAIN_DATASET_PATH\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Device setup with memory management options\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU properties and setup optimal configurations\n",
    "if torch.cuda.is_available():\n",
    "    # Display GPU info\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Enable TF32 precision on Ampere and newer GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Set memory allocation strategies\n",
    "    torch.cuda.empty_cache()\n",
    "    # Set to largest prime below 512MB for better memory fragmentation handling\n",
    "    # torch.cuda.memory_reserved(509)\n",
    "    \n",
    "    # Enable cuDNN benchmark mode for optimized convolutions\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Deterministic mode can be slower, disable unless reproducibility is critical\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = ExtendedUNetPlusPlus().to(device)\n",
    "\n",
    "# Apply weight initialization for faster convergence\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d, nn.ConvTranspose3d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Define optimizer with gradient clipping\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "grad_accumulation_steps = 1  # Increase if you need larger effective batch size\n",
    "\n",
    "# Setup for proper OneCycleLR\n",
    "train_size = 2656  # Set this to the actual number of batches from your error (or dynamically get from train_loader)\n",
    "total_steps = num_epochs * train_size\n",
    "print(f\"Configuring scheduler with {total_steps} total steps over {num_epochs} epochs\")\n",
    "\n",
    "# Learning rate scheduler - OneCycleLR for faster convergence\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-3,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Loss function - combo loss for better segmentation performance\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5):\n",
    "        super(ComboLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        pred_sigmoid = torch.sigmoid(pred)\n",
    "        dice_loss = 1 - dice_coef(target, pred_sigmoid)\n",
    "        return self.weight * bce_loss + (1 - self.weight) * dice_loss\n",
    "\n",
    "criterion = ComboLoss(weight=0.4)\n",
    "\n",
    "# Create directories for logging\n",
    "log_dir = 'logs'\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = os.path.join(log_dir, f'training_UNet++_{timestamp}.log')\n",
    "with open(log_filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'epoch', 'lr', 'loss', 'val_loss', 'dice_coef', 'val_dice_coef', \n",
    "        'dice_necrotic', 'dice_edema', 'dice_enhancing',\n",
    "        'precision', 'sensitivity', 'specificity', \n",
    "        'gpu_allocated', 'gpu_reserved', 'epoch_time'\n",
    "    ])\n",
    "\n",
    "# GPU monitoring utility\n",
    "def get_gpu_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "# Memory optimization function\n",
    "def optimize_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# AMP GradScaler with updated API\n",
    "scaler = torch.GradScaler('cuda')\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    # Best model tracking\n",
    "    best_val_dice = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 30\n",
    "    \n",
    "    # Track steps to avoid scheduler errors\n",
    "    global_step = 0\n",
    "    \n",
    "    # Initial memory stats\n",
    "    allocated, reserved = get_gpu_stats()\n",
    "    print(f\"Initial GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Progress tracking\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Train {epoch+1}\", leave=False)\n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient than just zero_grad()\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            # Check if we've exceeded total steps\n",
    "            if global_step >= total_steps:\n",
    "                print(f\"Reached maximum steps ({global_step}/{total_steps}). Ending training.\")\n",
    "                break\n",
    "                \n",
    "            # Move data to device asynchronously\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            # Mixed precision forward pass with updated API\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation for larger effective batch size\n",
    "            if (batch_idx + 1) % grad_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Update LR scheduler only if still within total_steps\n",
    "                if global_step < total_steps:\n",
    "                    scheduler.step()\n",
    "                    global_step += 1\n",
    "            \n",
    "            # Apply sigmoid for predictions\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.sigmoid(outputs)\n",
    "                dice = dice_coef(masks, predictions).item()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item() * grad_accumulation_steps\n",
    "            train_dice += dice\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item() * grad_accumulation_steps:.5f}\", \n",
    "                'dice': f\"{dice:.5f}\",\n",
    "                'lr': f\"{current_lr:.6f}\",\n",
    "                'step': f\"{global_step}/{total_steps}\"\n",
    "            })\n",
    "            \n",
    "            # Monitor GPU usage every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                allocated, reserved = get_gpu_stats()\n",
    "                if reserved > 0.9 * torch.cuda.get_device_properties(0).total_memory / 1e9:\n",
    "                    optimize_memory()\n",
    "                    print(f\"Memory optimized. New allocation: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / max(batch_count, 1)  # Avoid division by zero\n",
    "        avg_train_dice = train_dice / max(batch_count, 1)\n",
    "        \n",
    "        # Validation phase with memory optimization\n",
    "        optimize_memory()\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_dice_necrotic = 0.0\n",
    "        val_dice_edema = 0.0\n",
    "        val_dice_enhancing = 0.0\n",
    "        val_precision = 0.0\n",
    "        val_sensitivity = 0.0\n",
    "        val_specificity = 0.0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Valid {epoch+1}\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_pbar:\n",
    "                # Move data to device\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    predictions = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                dice = dice_coef(masks, predictions).item()\n",
    "                dice_necrotic = dice_coef_necrotic(masks, predictions).item()\n",
    "                dice_edema = dice_coef_edema(masks, predictions).item()\n",
    "                dice_enhancing = dice_coef_enhancing(masks, predictions).item()\n",
    "                prec = precision(masks, predictions).item()\n",
    "                sens = sensitivity(masks, predictions).item()\n",
    "                spec = specificity(masks, predictions).item()\n",
    "                \n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                val_dice += dice\n",
    "                val_dice_necrotic += dice_necrotic\n",
    "                val_dice_edema += dice_edema\n",
    "                val_dice_enhancing += dice_enhancing\n",
    "                val_precision += prec\n",
    "                val_sensitivity += sens\n",
    "                val_specificity += spec\n",
    "                val_batch_count += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'val_loss': f\"{loss.item():.5f}\", \n",
    "                    'val_dice': f\"{dice:.5f}\"\n",
    "                })\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        avg_val_loss = val_loss / max(val_batch_count, 1)  # Avoid division by zero\n",
    "        avg_val_dice = val_dice / max(val_batch_count, 1)\n",
    "        avg_val_dice_necrotic = val_dice_necrotic / max(val_batch_count, 1)\n",
    "        avg_val_dice_edema = val_dice_edema / max(val_batch_count, 1)\n",
    "        avg_val_dice_enhancing = val_dice_enhancing / max(val_batch_count, 1)\n",
    "        avg_val_precision = val_precision / max(val_batch_count, 1)\n",
    "        avg_val_sensitivity = val_sensitivity / max(val_batch_count, 1)\n",
    "        avg_val_specificity = val_specificity / max(val_batch_count, 1)\n",
    "        \n",
    "        # GPU stats after epoch\n",
    "        allocated, reserved = get_gpu_stats()\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.5f} | Train Dice: {avg_train_dice:.5f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.5f} | Val Dice: {avg_val_dice:.5f}\")\n",
    "        print(f\"Dice Metrics - Necrotic: {avg_val_dice_necrotic:.5f} | Edema: {avg_val_dice_edema:.5f} | Enhancing: {avg_val_dice_enhancing:.5f}\")\n",
    "        print(f\"GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Log metrics to CSV\n",
    "        with open(log_filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch+1,\n",
    "                f\"{current_lr:.6f}\",\n",
    "                f\"{avg_train_loss:.5f}\", \n",
    "                f\"{avg_val_loss:.5f}\", \n",
    "                f\"{avg_train_dice:.5f}\",\n",
    "                f\"{avg_val_dice:.5f}\",\n",
    "                f\"{avg_val_dice_necrotic:.5f}\",\n",
    "                f\"{avg_val_dice_edema:.5f}\",\n",
    "                f\"{avg_val_dice_enhancing:.5f}\",\n",
    "                f\"{avg_val_precision:.5f}\",\n",
    "                f\"{avg_val_sensitivity:.5f}\",\n",
    "                f\"{avg_val_specificity:.5f}\",\n",
    "                f\"{allocated:.2f}\",\n",
    "                f\"{reserved:.2f}\",\n",
    "                f\"{epoch_time:.2f}\"\n",
    "            ])\n",
    "        \n",
    "        # Save checkpoint - better to save based on dice coefficient for segmentation\n",
    "        is_best_dice = avg_val_dice > best_val_dice\n",
    "        is_best_loss = avg_val_loss < best_val_loss\n",
    "        \n",
    "        if is_best_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            # Save best dice model\n",
    "            checkpoint_filename = os.path.join(\n",
    "                checkpoint_dir, \n",
    "                f\"UNet++_best_dice_{epoch+1:02d}_{avg_val_dice:.5f}.pt\"\n",
    "            )\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_dice': avg_val_dice,\n",
    "                'metrics': {\n",
    "                    'dice_necrotic': avg_val_dice_necrotic,\n",
    "                    'dice_edema': avg_val_dice_edema,\n",
    "                    'dice_enhancing': avg_val_dice_enhancing,\n",
    "                    'precision': avg_val_precision,\n",
    "                    'sensitivity': avg_val_sensitivity,\n",
    "                    'specificity': avg_val_specificity,\n",
    "                }\n",
    "            }, checkpoint_filename)\n",
    "            print(f\"New best dice! Checkpoint saved to {checkpoint_filename}\")\n",
    "        elif is_best_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            # Save best loss model\n",
    "            checkpoint_filename = os.path.join(\n",
    "                checkpoint_dir, \n",
    "                f\"UNet++_best_loss_{epoch+1:02d}_{avg_val_loss:.5f}.pt\"\n",
    "            )\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(), \n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_dice': avg_val_dice,\n",
    "                'metrics': {\n",
    "                    'dice_necrotic': avg_val_dice_necrotic,\n",
    "                    'dice_edema': avg_val_dice_edema,\n",
    "                    'dice_enhancing': avg_val_dice_enhancing,\n",
    "                    'precision': avg_val_precision,\n",
    "                    'sensitivity': avg_val_sensitivity,\n",
    "                    'specificity': avg_val_specificity,\n",
    "                }\n",
    "            }, checkpoint_filename)\n",
    "            print(f\"New best loss! Checkpoint saved to {checkpoint_filename}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "            # Save latest model periodically (every 5 epochs)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                checkpoint_filename = os.path.join(\n",
    "                    checkpoint_dir, \n",
    "                    f\"UNet++_latest_{epoch+1:02d}.pt\"\n",
    "                )\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict(),\n",
    "                }, checkpoint_filename)\n",
    "                print(f\"Latest model saved at epoch {epoch+1}\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Check if we've reached total steps\n",
    "        if global_step >= total_steps:\n",
    "            print(f\"Reached maximum steps ({global_step}/{total_steps}). Ending training.\")\n",
    "            break\n",
    "            \n",
    "        # Clean up memory at the end of each epoch\n",
    "        optimize_memory()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return best_val_dice, best_val_loss\n",
    "\n",
    "# Execute training\n",
    "if __name__ == '__main__':\n",
    "    # Print starting info\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    try:\n",
    "        best_val_dice, best_val_loss = train()\n",
    "        print(f\"Best validation dice: {best_val_dice:.5f}\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.5f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory at failure: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "            print(f\"Stack trace:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T08:18:40.604588Z",
     "iopub.status.busy": "2025-03-12T08:18:40.604259Z",
     "iopub.status.idle": "2025-03-12T11:18:54.991226Z",
     "shell.execute_reply": "2025-03-12T11:18:54.990238Z",
     "shell.execute_reply.started": "2025-03-12T08:18:40.604563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "# Device setup with memory management options\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU properties and setup optimal configurations\n",
    "if torch.cuda.is_available():\n",
    "    # Display GPU info\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Enable TF32 precision on Ampere and newer GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Set memory allocation strategies\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Enable cuDNN benchmark mode for optimized convolutions\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Deterministic mode can be slower, disable unless reproducibility is critical\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = ExtendedUNetPlusPlus().to(device)\n",
    "\n",
    "# Apply weight initialization for faster convergence\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d, nn.ConvTranspose3d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Define optimizer with gradient clipping\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # Reduced learning rate for stability\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "grad_accumulation_steps = 1  # Increase if you need larger effective batch size\n",
    "\n",
    "# Setup for proper OneCycleLR\n",
    "train_size = 2656  # Set this to the actual number of batches from your error (or dynamically get from train_loader)\n",
    "total_steps = num_epochs * train_size\n",
    "print(f\"Configuring scheduler with {total_steps} total steps over {num_epochs} epochs\")\n",
    "\n",
    "# Learning rate scheduler - OneCycleLR for faster convergence\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=2e-3,  # Reduced max learning rate to prevent NaN values\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "# Loss function - combo loss with gradient clipping and NaN prevention\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5, epsilon=1e-7):\n",
    "        super(ComboLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.epsilon = epsilon\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # Check for NaN values\n",
    "        if torch.isnan(pred).any() or torch.isnan(target).any():\n",
    "            print(\"WARNING: NaN values detected in inputs to loss function!\")\n",
    "            pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred)\n",
    "            target = torch.where(torch.isnan(target), torch.zeros_like(target), target)\n",
    "        \n",
    "        # Clip predictions to prevent extreme values\n",
    "        pred_clipped = torch.clamp(pred, -50, 50)  # Prevent exp overflow in sigmoid\n",
    "        \n",
    "        bce_loss = self.bce(pred_clipped, target)\n",
    "        \n",
    "        # Manually compute sigmoid to have more control\n",
    "        pred_sigmoid = torch.clamp(torch.sigmoid(pred_clipped), self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        dice_loss = 1 - dice_coef(target, pred_sigmoid)\n",
    "        \n",
    "        # Final combined loss\n",
    "        combined_loss = self.weight * bce_loss + (1 - self.weight) * dice_loss\n",
    "        \n",
    "        # Check for NaN in output\n",
    "        if torch.isnan(combined_loss).any():\n",
    "            print(f\"WARNING: NaN in loss output. BCE: {bce_loss.item()}, Dice: {dice_loss.item()}\")\n",
    "            return torch.tensor(0.1, device=pred.device, requires_grad=True)\n",
    "        \n",
    "        return combined_loss\n",
    "\n",
    "criterion = ComboLoss(weight=0.4)\n",
    "\n",
    "# Create directories for logging and ensure they exist\n",
    "log_dir = 'logs'\n",
    "checkpoint_dir = 'checkpoints'\n",
    "for directory in [log_dir, checkpoint_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # Test write permission\n",
    "    test_file = os.path.join(directory, 'test_write.tmp')\n",
    "    try:\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('test')\n",
    "        os.remove(test_file)\n",
    "        print(f\"Directory {directory} is writable\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Directory {directory} may not be writable: {str(e)}\")\n",
    "\n",
    "# Setup logging\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = os.path.join(log_dir, f'training_UNet++_{timestamp}.log')\n",
    "with open(log_filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'epoch', 'lr', 'loss', 'val_loss', 'dice_coef', 'val_dice_coef', \n",
    "        'dice_necrotic', 'dice_edema', 'dice_enhancing',\n",
    "        'precision', 'sensitivity', 'specificity', \n",
    "        'gpu_allocated', 'gpu_reserved', 'epoch_time'\n",
    "    ])\n",
    "\n",
    "# GPU monitoring utility\n",
    "def get_gpu_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "# Memory optimization function\n",
    "def optimize_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# AMP GradScaler with updated API\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Safely save checkpoint with retry mechanism\n",
    "def save_checkpoint(state, filename):\n",
    "    try:\n",
    "        # Save to temporary file first\n",
    "        temp_filename = filename + \".tmp\"\n",
    "        torch.save(state, temp_filename)\n",
    "        \n",
    "        # If successful, rename to final filename\n",
    "        if os.path.exists(temp_filename):\n",
    "            os.replace(temp_filename, filename)\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Gradient norm monitoring to detect potential instability\n",
    "def check_gradients(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    # Best model tracking\n",
    "    best_val_dice = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 30\n",
    "    \n",
    "    # Track steps to avoid scheduler errors\n",
    "    global_step = 0\n",
    "    \n",
    "    # Gradient monitoring for instability detection\n",
    "    grad_history = []\n",
    "    \n",
    "    # Initial memory stats\n",
    "    allocated, reserved = get_gpu_stats()\n",
    "    print(f\"Initial GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Progress tracking\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Train {epoch+1}\", leave=False)\n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient than just zero_grad()\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            # Check if we've exceeded total steps\n",
    "            if global_step >= total_steps:\n",
    "                print(f\"Reached maximum steps ({global_step}/{total_steps}). Ending training.\")\n",
    "                break\n",
    "                \n",
    "            # Move data to device asynchronously\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            # Check for NaN inputs\n",
    "            if torch.isnan(images).any() or torch.isnan(masks).any():\n",
    "                print(f\"WARNING: NaN values detected in input data at batch {batch_idx}. Skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks) / grad_accumulation_steps\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) / grad_accumulation_steps\n",
    "            \n",
    "            # Check for NaN loss - happens when gradient explosion occurs\n",
    "            if torch.isnan(loss).any():\n",
    "                print(f\"WARNING: NaN loss detected at batch {batch_idx}. Skipping backward pass.\")\n",
    "                # Skip this batch and continue with the next one\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "            \n",
    "            # Backward pass with gradient scaling if using mixed precision\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            # Gradient accumulation for larger effective batch size\n",
    "            if (batch_idx + 1) % grad_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                # Monitor gradient norms to detect instability\n",
    "                grad_norm = check_gradients(model)\n",
    "                grad_history.append(grad_norm)\n",
    "                \n",
    "                # Check for exploding gradients\n",
    "                if len(grad_history) > 5 and grad_norm > 10 * np.mean(grad_history[-10:-1]):\n",
    "                    print(f\"WARNING: Potential gradient explosion detected. Norm: {grad_norm:.2f}\")\n",
    "                \n",
    "                if scaler is not None:\n",
    "                    # Gradient clipping to prevent exploding gradients\n",
    "                    scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Clip gradients to prevent explosion\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step\n",
    "                if scaler is not None:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Update LR scheduler only if still within total_steps\n",
    "                if global_step < total_steps:\n",
    "                    scheduler.step()\n",
    "                    global_step += 1\n",
    "            \n",
    "            # Apply sigmoid for predictions\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.sigmoid(outputs)\n",
    "                # Clamp predictions to prevent NaN in metrics\n",
    "                predictions = torch.clamp(predictions, 1e-7, 1.0 - 1e-7)\n",
    "                dice = dice_coef(masks, predictions).item()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item() * grad_accumulation_steps\n",
    "            train_dice += dice\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item() * grad_accumulation_steps:.5f}\", \n",
    "                'dice': f\"{dice:.5f}\",\n",
    "                'lr': f\"{current_lr:.6f}\",\n",
    "                'step': f\"{global_step}/{total_steps}\"\n",
    "            })\n",
    "            \n",
    "            # Monitor GPU usage every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                allocated, reserved = get_gpu_stats()\n",
    "                if reserved > 0.9 * torch.cuda.get_device_properties(0).total_memory / 1e9:\n",
    "                    optimize_memory()\n",
    "                    print(f\"Memory optimized. New allocation: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / max(batch_count, 1)  # Avoid division by zero\n",
    "        avg_train_dice = train_dice / max(batch_count, 1)\n",
    "        \n",
    "        # Validation phase with memory optimization\n",
    "        optimize_memory()\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_dice_necrotic = 0.0\n",
    "        val_dice_edema = 0.0\n",
    "        val_dice_enhancing = 0.0\n",
    "        val_precision = 0.0\n",
    "        val_sensitivity = 0.0\n",
    "        val_specificity = 0.0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Valid {epoch+1}\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_pbar:\n",
    "                # Move data to device\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "                \n",
    "                # Check for NaN inputs\n",
    "                if torch.isnan(images).any() or torch.isnan(masks).any():\n",
    "                    print(f\"WARNING: NaN values detected in validation data. Skipping batch.\")\n",
    "                    continue\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                if scaler is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(images)\n",
    "                        predictions = torch.sigmoid(outputs)\n",
    "                        # Clamp predictions to prevent NaN\n",
    "                        predictions = torch.clamp(predictions, 1e-7, 1.0 - 1e-7)\n",
    "                        loss = criterion(outputs, masks)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    predictions = torch.sigmoid(outputs)\n",
    "                    # Clamp predictions to prevent NaN\n",
    "                    predictions = torch.clamp(predictions, 1e-7, 1.0 - 1e-7)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Skip batch if NaN loss\n",
    "                if torch.isnan(loss).any():\n",
    "                    print(\"WARNING: NaN loss in validation. Skipping batch.\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics\n",
    "                dice = dice_coef(masks, predictions).item()\n",
    "                dice_necrotic = dice_coef_necrotic(masks, predictions).item()\n",
    "                dice_edema = dice_coef_edema(masks, predictions).item()\n",
    "                dice_enhancing = dice_coef_enhancing(masks, predictions).item()\n",
    "                prec = precision(masks, predictions).item()\n",
    "                sens = sensitivity(masks, predictions).item()\n",
    "                spec = specificity(masks, predictions).item()\n",
    "                \n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                val_dice += dice\n",
    "                val_dice_necrotic += dice_necrotic\n",
    "                val_dice_edema += dice_edema\n",
    "                val_dice_enhancing += dice_enhancing\n",
    "                val_precision += prec\n",
    "                val_sensitivity += sens\n",
    "                val_specificity += spec\n",
    "                val_batch_count += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'val_loss': f\"{loss.item():.5f}\", \n",
    "                    'val_dice': f\"{dice:.5f}\"\n",
    "                })\n",
    "        \n",
    "        # Avoid division by zero if all batches were skipped due to NaN\n",
    "        if val_batch_count == 0:\n",
    "            print(\"WARNING: All validation batches were skipped. Setting validation metrics to zero.\")\n",
    "            avg_val_loss = 0.0\n",
    "            avg_val_dice = 0.0\n",
    "            avg_val_dice_necrotic = 0.0\n",
    "            avg_val_dice_edema = 0.0\n",
    "            avg_val_dice_enhancing = 0.0\n",
    "            avg_val_precision = 0.0\n",
    "            avg_val_sensitivity = 0.0\n",
    "            avg_val_specificity = 0.0\n",
    "        else:\n",
    "            # Calculate average validation metrics\n",
    "            avg_val_loss = val_loss / val_batch_count\n",
    "            avg_val_dice = val_dice / val_batch_count\n",
    "            avg_val_dice_necrotic = val_dice_necrotic / val_batch_count\n",
    "            avg_val_dice_edema = val_dice_edema / val_batch_count\n",
    "            avg_val_dice_enhancing = val_dice_enhancing / val_batch_count\n",
    "            avg_val_precision = val_precision / val_batch_count\n",
    "            avg_val_sensitivity = val_sensitivity / val_batch_count\n",
    "            avg_val_specificity = val_specificity / val_batch_count\n",
    "        \n",
    "        # GPU stats after epoch\n",
    "        allocated, reserved = get_gpu_stats()\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.5f} | Train Dice: {avg_train_dice:.5f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.5f} | Val Dice: {avg_val_dice:.5f}\")\n",
    "        print(f\"Dice Metrics - Necrotic: {avg_val_dice_necrotic:.5f} | Edema: {avg_val_dice_edema:.5f} | Enhancing: {avg_val_dice_enhancing:.5f}\")\n",
    "        print(f\"GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Log metrics to CSV\n",
    "        try:\n",
    "            with open(log_filename, 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    epoch+1,\n",
    "                    f\"{current_lr:.6f}\",\n",
    "                    f\"{avg_train_loss:.5f}\", \n",
    "                    f\"{avg_val_loss:.5f}\", \n",
    "                    f\"{avg_train_dice:.5f}\",\n",
    "                    f\"{avg_val_dice:.5f}\",\n",
    "                    f\"{avg_val_dice_necrotic:.5f}\",\n",
    "                    f\"{avg_val_dice_edema:.5f}\",\n",
    "                    f\"{avg_val_dice_enhancing:.5f}\",\n",
    "                    f\"{avg_val_precision:.5f}\",\n",
    "                    f\"{avg_val_sensitivity:.5f}\",\n",
    "                    f\"{avg_val_specificity:.5f}\",\n",
    "                    f\"{allocated:.2f}\",\n",
    "                    f\"{reserved:.2f}\",\n",
    "                    f\"{epoch_time:.2f}\"\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to log file: {str(e)}\")\n",
    "        \n",
    "        # Save checkpoint - better to save based on dice coefficient for segmentation\n",
    "        is_best_dice = avg_val_dice > best_val_dice\n",
    "        is_best_loss = avg_val_loss < best_val_loss\n",
    "        \n",
    "        if is_best_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            # Save best dice model\n",
    "            checkpoint_filename = os.path.join(\n",
    "                checkpoint_dir, \n",
    "                f\"UNet++_best_dice_{epoch+1:02d}_{avg_val_dice:.5f}.pt\"\n",
    "            )\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_dice': avg_val_dice,\n",
    "                'metrics': {\n",
    "                    'dice_necrotic': avg_val_dice_necrotic,\n",
    "                    'dice_edema': avg_val_dice_edema,\n",
    "                    'dice_enhancing': avg_val_dice_enhancing,\n",
    "                    'precision': avg_val_precision,\n",
    "                    'sensitivity': avg_val_sensitivity,\n",
    "                    'specificity': avg_val_specificity,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add scaler state if using mixed precision\n",
    "            if scaler is not None:\n",
    "                checkpoint_data['scaler_state_dict'] = scaler.state_dict()\n",
    "            \n",
    "            if save_checkpoint(checkpoint_data, checkpoint_filename):\n",
    "                print(f\"New best dice! Checkpoint saved to {checkpoint_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to save best dice checkpoint\")\n",
    "                \n",
    "        elif is_best_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            # Save best loss model\n",
    "            checkpoint_filename = os.path.join(\n",
    "                checkpoint_dir, \n",
    "                f\"UNet++_best_loss_{epoch+1:02d}_{avg_val_loss:.5f}.pt\"\n",
    "            )\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_dice': avg_val_dice,\n",
    "                'metrics': {\n",
    "                    'dice_necrotic': avg_val_dice_necrotic,\n",
    "                    'dice_edema': avg_val_dice_edema,\n",
    "                    'dice_enhancing': avg_val_dice_enhancing,\n",
    "                    'precision': avg_val_precision,\n",
    "                    'sensitivity': avg_val_sensitivity,\n",
    "                    'specificity': avg_val_specificity,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add scaler state if using mixed precision\n",
    "            if scaler is not None:\n",
    "                checkpoint_data['scaler_state_dict'] = scaler.state_dict()\n",
    "                \n",
    "            if save_checkpoint(checkpoint_data, checkpoint_filename):\n",
    "                print(f\"New best loss! Checkpoint saved to {checkpoint_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to save best loss checkpoint\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "            # Save latest model periodically (every 5 epochs)\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:  # Also save after first epoch\n",
    "                checkpoint_filename = os.path.join(\n",
    "                    checkpoint_dir, \n",
    "                    f\"UNet++_latest_{epoch+1:02d}.pt\"\n",
    "                )\n",
    "                checkpoint_data = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                }\n",
    "                \n",
    "                # Add scaler state if using mixed precision\n",
    "                if scaler is not None:\n",
    "                    checkpoint_data['scaler_state_dict'] = scaler.state_dict()\n",
    "                    \n",
    "                if save_checkpoint(checkpoint_data, checkpoint_filename):\n",
    "                    print(f\"Latest model saved at epoch {epoch+1}\")\n",
    "                else:\n",
    "                    print(f\"Failed to save latest checkpoint\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Check if we've reached total steps\n",
    "        if global_step >= total_steps:\n",
    "            print(f\"Reached maximum steps ({global_step}/{total_steps}). Ending training.\")\n",
    "            break\n",
    "            \n",
    "        # Clean up memory at the end of each epoch\n",
    "        optimize_memory()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return best_val_dice, best_val_loss\n",
    "\n",
    "# Execute training\n",
    "if __name__ == '__main__':\n",
    "    # Print starting info\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    try:\n",
    "        best_val_dice, best_val_loss = train()\n",
    "        print(f\"Best validation dice: {best_val_dice:.5f}\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.5f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory at failure: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "            print(f\"Stack trace:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T08:15:49.941777Z",
     "iopub.status.busy": "2025-03-12T08:15:49.941439Z",
     "iopub.status.idle": "2025-03-12T08:15:50.066483Z",
     "shell.execute_reply": "2025-03-12T08:15:50.065418Z",
     "shell.execute_reply.started": "2025-03-12T08:15:49.941751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "# import time\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Check GPU properties\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# # Initialize model and move to device\n",
    "# model = ExtendedUNetPlusPlus().to(device)\n",
    "\n",
    "# # Define optimizer with higher learning rate\n",
    "# optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=20, verbose=True\n",
    "# )\n",
    "\n",
    "# # Loss function\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# # Training parameters - increase batch size if memory allows\n",
    "# num_epochs = 1\n",
    "# batch_size = 32  # You might want to increase this further if your GPU has enough memory\n",
    "\n",
    "\n",
    "\n",
    "# # Create directories for logs and checkpoints\n",
    "# os.makedirs('logs', exist_ok=True)\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# # CSV logging setup\n",
    "# log_filename = os.path.join('logs', f'training_UNet++_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "# with open(log_filename, 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['epoch', 'loss', 'val_loss', 'dice_coef', 'val_dice_coef', \n",
    "#                     'dice_necrotic', 'dice_edema', 'dice_enhancing',\n",
    "#                     'precision', 'sensitivity', 'specificity', 'gpu_utilization'])\n",
    "\n",
    "# # Set higher priority for GPU operations\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner\n",
    "    \n",
    "# # Function to check GPU utilization\n",
    "# def get_gpu_utilization():\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.synchronize()  # Wait for all operations to finish\n",
    "#         allocated = torch.cuda.memory_allocated() / 1e9\n",
    "#         reserved = torch.cuda.memory_reserved() / 1e9\n",
    "#         return allocated, reserved\n",
    "#     return 0, 0\n",
    "\n",
    "# # Training function\n",
    "# def train():\n",
    "#     # Initialize GradScaler for mixed precision training\n",
    "#     scaler = GradScaler()\n",
    "    \n",
    "#     # Best model tracking\n",
    "#     best_val_loss = float('inf')\n",
    "#     early_stop_counter = 0\n",
    "#     early_stop_patience = 30\n",
    "    \n",
    "#     # Print initial memory usage\n",
    "#     allocated, reserved = get_gpu_utilization()\n",
    "#     print(f\"Initial GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         train_dice = 0.0\n",
    "#         batch_count = 0\n",
    "        \n",
    "#         # Pre-fetch data to GPU before epoch starts for better performance\n",
    "#         prefetch_factor = 2  # Adjust as needed\n",
    "        \n",
    "#         # Using tqdm for progress bar\n",
    "#         train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "#         for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "#             # Print batch size every 1000 batches\n",
    "#             if (batch_count + 1) % 1000 == 0:\n",
    "#                 print(f\"Processed {batch_count + 1} batches. Current batch size: {images.shape[0]}\")\n",
    "#                 allocated, reserved = get_gpu_utilization()\n",
    "#                 print(f\"GPU Memory: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "            \n",
    "#             # Move data to device\n",
    "#             images, masks = images.to(device, non_blocking=True), masks.to(device, non_blocking=True)\n",
    "            \n",
    "#             # Zero gradients - use more efficient method\n",
    "#             for param in model.parameters():\n",
    "#                 param.grad = None\n",
    "            \n",
    "#             # Forward pass with mixed precision\n",
    "#             with autocast():\n",
    "#                 outputs = model(images)\n",
    "#                 # Apply sigmoid for predictions\n",
    "#                 predictions = torch.sigmoid(outputs)\n",
    "                \n",
    "#                 # Calculate loss\n",
    "#                 loss = 1 - dice_coef(masks, predictions)  # Using 1 - dice as loss\n",
    "            \n",
    "#             # Backward pass with gradient scaling\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "            \n",
    "#             # Calculate metrics with 5 decimal precision\n",
    "#             dice = dice_coef(masks, predictions).item()\n",
    "            \n",
    "#             # Update counters\n",
    "#             train_loss += loss.item()\n",
    "#             train_dice += dice\n",
    "#             batch_count += 1\n",
    "            \n",
    "#             # Update progress bar with current metrics\n",
    "#             train_pbar.set_postfix({\n",
    "#                 'loss': f\"{loss.item():.5f}\", \n",
    "#                 'dice': f\"{dice:.5f}\"\n",
    "#             })\n",
    "            \n",
    "#             # Optional: use torch.cuda.empty_cache() occasionally if memory issues occur\n",
    "#             if batch_count % 100 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "        \n",
    "#         # Calculate average training metrics (5 decimal places)\n",
    "#         avg_train_loss = train_loss / batch_count\n",
    "#         avg_train_dice = train_dice / batch_count\n",
    "        \n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_dice = 0.0\n",
    "#         val_dice_necrotic = 0.0\n",
    "#         val_dice_edema = 0.0\n",
    "#         val_dice_enhancing = 0.0\n",
    "#         val_precision = 0.0\n",
    "#         val_sensitivity = 0.0\n",
    "#         val_specificity = 0.0\n",
    "#         val_batch_count = 0\n",
    "        \n",
    "#         # Using tqdm for validation progress bar\n",
    "#         val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\", leave=False)\n",
    "#         with torch.no_grad():\n",
    "#             for images, masks in val_pbar:\n",
    "#                 # Print batch size every 1000 batches\n",
    "#                 if (val_batch_count + 1) % 1000 == 0:\n",
    "#                     print(f\"Processed {val_batch_count + 1} validation batches. Current batch size: {images.shape[0]}\")\n",
    "                \n",
    "#                 # Move data to device\n",
    "#                 images, masks = images.to(device, non_blocking=True), masks.to(device, non_blocking=True)\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 predictions = torch.sigmoid(outputs)\n",
    "                \n",
    "#                 # Calculate loss and metrics\n",
    "#                 loss = 1 - dice_coef(masks, predictions)\n",
    "#                 dice = dice_coef(masks, predictions).item()\n",
    "#                 dice_necrotic = dice_coef_necrotic(masks, predictions).item()\n",
    "#                 dice_edema = dice_coef_edema(masks, predictions).item()\n",
    "#                 dice_enhancing = dice_coef_enhancing(masks, predictions).item()\n",
    "#                 prec = precision(masks, predictions).item()\n",
    "#                 sens = sensitivity(masks, predictions).item()\n",
    "#                 spec = specificity(masks, predictions).item()\n",
    "                \n",
    "#                 # Update counters\n",
    "#                 val_loss += loss.item()\n",
    "#                 val_dice += dice\n",
    "#                 val_dice_necrotic += dice_necrotic\n",
    "#                 val_dice_edema += dice_edema\n",
    "#                 val_dice_enhancing += dice_enhancing\n",
    "#                 val_precision += prec\n",
    "#                 val_sensitivity += sens\n",
    "#                 val_specificity += spec\n",
    "#                 val_batch_count += 1\n",
    "                \n",
    "#                 # Update progress bar\n",
    "#                 val_pbar.set_postfix({\n",
    "#                     'val_loss': f\"{loss.item():.5f}\", \n",
    "#                     'val_dice': f\"{dice:.5f}\"\n",
    "#                 })\n",
    "        \n",
    "#         # Calculate average validation metrics (5 decimal places)\n",
    "#         avg_val_loss = val_loss / val_batch_count\n",
    "#         avg_val_dice = val_dice / val_batch_count\n",
    "#         avg_val_dice_necrotic = val_dice_necrotic / val_batch_count\n",
    "#         avg_val_dice_edema = val_dice_edema / val_batch_count\n",
    "#         avg_val_dice_enhancing = val_dice_enhancing / val_batch_count\n",
    "#         avg_val_precision = val_precision / val_batch_count\n",
    "#         avg_val_sensitivity = val_sensitivity / val_batch_count\n",
    "#         avg_val_specificity = val_specificity / val_batch_count\n",
    "        \n",
    "#         # Get GPU utilization\n",
    "#         allocated, reserved = get_gpu_utilization()\n",
    "#         print(f\"GPU Memory after epoch: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "#         # Update scheduler\n",
    "#         scheduler.step(avg_val_loss)\n",
    "        \n",
    "#         # Calculate epoch time\n",
    "#         epoch_time = time.time() - start_time\n",
    "        \n",
    "#         # Print epoch summary with 5 decimal places\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "#         print(f\"Train Loss: {avg_train_loss:.5f} | Train Dice: {avg_train_dice:.5f}\")\n",
    "#         print(f\"Val Loss: {avg_val_loss:.5f} | Val Dice: {avg_val_dice:.5f}\")\n",
    "#         print(f\"Dice Necrotic: {avg_val_dice_necrotic:.5f} | Dice Edema: {avg_val_dice_edema:.5f} | Dice Enhancing: {avg_val_dice_enhancing:.5f}\")\n",
    "#         print(f\"Precision: {avg_val_precision:.5f} | Sensitivity: {avg_val_sensitivity:.5f} | Specificity: {avg_val_specificity:.5f}\")\n",
    "        \n",
    "#         # Log metrics with 5 decimal places\n",
    "#         with open(log_filename, 'a', newline='') as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch+1, \n",
    "#                 f\"{avg_train_loss:.5f}\", \n",
    "#                 f\"{avg_val_loss:.5f}\", \n",
    "#                 f\"{avg_train_dice:.5f}\",\n",
    "#                 f\"{avg_val_dice:.5f}\",\n",
    "#                 f\"{avg_val_dice_necrotic:.5f}\",\n",
    "#                 f\"{avg_val_dice_edema:.5f}\",\n",
    "#                 f\"{avg_val_dice_enhancing:.5f}\",\n",
    "#                 f\"{avg_val_precision:.5f}\",\n",
    "#                 f\"{avg_val_sensitivity:.5f}\",\n",
    "#                 f\"{avg_val_specificity:.5f}\",\n",
    "#                 f\"{allocated:.2f}\"\n",
    "#             ])\n",
    "        \n",
    "#         # Check for model improvement\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             # Reset early stopping counter\n",
    "#             early_stop_counter = 0\n",
    "            \n",
    "#             # Update best validation loss\n",
    "#             best_val_loss = avg_val_loss\n",
    "            \n",
    "#             # Save model checkpoint with 5 decimal precision in filename\n",
    "#             checkpoint_filename = f\"3D-MedVision++-weights-improvement-{epoch+1:02d}-{avg_val_loss:.5f}.pt\"\n",
    "#             checkpoint_path = os.path.join('checkpoints', checkpoint_filename)\n",
    "            \n",
    "#             # Save model state\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'train_loss': avg_train_loss,\n",
    "#                 'val_loss': avg_val_loss,\n",
    "#                 'val_dice': avg_val_dice,\n",
    "#                 'val_dice_necrotic': avg_val_dice_necrotic,\n",
    "#                 'val_dice_edema': avg_val_dice_edema,\n",
    "#                 'val_dice_enhancing': avg_val_dice_enhancing,\n",
    "#                 'val_precision': avg_val_precision,\n",
    "#                 'val_sensitivity': avg_val_sensitivity,\n",
    "#                 'val_specificity': avg_val_specificity,\n",
    "#             }, checkpoint_path)\n",
    "            \n",
    "#             print(f\"Model improved! Checkpoint saved to {checkpoint_path}\")\n",
    "#         else:\n",
    "#             # Increment early stopping counter\n",
    "#             early_stop_counter += 1\n",
    "#             print(f\"Model did not improve. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "#             # Check if early stopping should be triggered\n",
    "#             if early_stop_counter >= early_stop_patience:\n",
    "#                 print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "#                 break\n",
    "    \n",
    "#     print(\"Training complete!\")\n",
    "#     return best_val_loss\n",
    "\n",
    "# # Execute training\n",
    "# if __name__ == '__main__':\n",
    "#     best_val_loss = train()\n",
    "#     print(f\"Best validation loss: {best_val_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.distributed.parallel_loader as pl\n",
    "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "# # TPU setup function\n",
    "# def train_tpu(index):\n",
    "#     # Get TPU device\n",
    "#     device = xm.xla_device()\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # Initialize model and move to TPU\n",
    "#     model = ExtendedUNetPlusPlus().to(device)\n",
    "    \n",
    "#     # Define optimizer with higher learning rate\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "    \n",
    "#     # Learning rate scheduler\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='min', factor=0.5, patience=20, verbose=True\n",
    "#     )\n",
    "    \n",
    "#     # Loss function\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     # Training parameters\n",
    "#     num_epochs = 5\n",
    "#     batch_size = 32  # Adjust based on TPU memory\n",
    "    \n",
    "#     # Create directories for logs and checkpoints\n",
    "#     os.makedirs('logs', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "#     # CSV logging setup - only for the main process\n",
    "#     if xm.is_master_ordinal():\n",
    "#         log_filename = os.path.join('logs', f'training_UNet++_TPU_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "#         with open(log_filename, 'w', newline='') as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow(['epoch', 'loss', 'val_loss', 'dice_coef', 'val_dice_coef', \n",
    "#                             'dice_necrotic', 'dice_edema', 'dice_enhancing',\n",
    "#                             'precision', 'sensitivity', 'specificity'])\n",
    "    \n",
    "#     # Function to get memory stats for TPU\n",
    "#     def get_tpu_memory_stats():\n",
    "#         # TPU doesn't provide direct memory stats like CUDA, but we can log that we're using TPU\n",
    "#         return \"Using TPU\"\n",
    "    \n",
    "#     # ParallelLoader for efficient data loading on TPU\n",
    "#     train_para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "#     val_para_loader = pl.ParallelLoader(val_loader, [device])\n",
    "    \n",
    "#     # Best model tracking\n",
    "#     best_val_loss = float('inf')\n",
    "#     early_stop_counter = 0\n",
    "#     early_stop_patience = 30\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         if xm.is_master_ordinal():\n",
    "#             print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         train_dice = 0.0\n",
    "#         batch_count = 0\n",
    "        \n",
    "#         # Using tqdm for progress bar - only on master process\n",
    "#         if xm.is_master_ordinal():\n",
    "#             train_pbar = tqdm(train_para_loader.per_device_loader(device), \n",
    "#                              desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "#             loader_iter = train_pbar\n",
    "#         else:\n",
    "#             loader_iter = train_para_loader.per_device_loader(device)\n",
    "            \n",
    "#         for batch_idx, (images, masks) in enumerate(loader_iter):\n",
    "#             # Print batch size occasionally - only on master process\n",
    "#             if xm.is_master_ordinal() and (batch_count + 1) % 1000 == 0:\n",
    "#                 print(f\"Processed {batch_count + 1} batches. Current batch size: {images.shape[0]}\")\n",
    "            \n",
    "#             # Zero gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(images)\n",
    "#             # Apply sigmoid for predictions\n",
    "#             predictions = torch.sigmoid(outputs)\n",
    "            \n",
    "#             # Calculate loss\n",
    "#             loss = 1 - dice_coef(masks, predictions)  # Using 1 - dice as loss\n",
    "            \n",
    "#             # Backward pass\n",
    "#             loss.backward()\n",
    "            \n",
    "#             # Step with XLA optimizer\n",
    "#             xm.optimizer_step(optimizer)\n",
    "            \n",
    "#             # Calculate metrics\n",
    "#             dice = dice_coef(masks, predictions).detach()\n",
    "            \n",
    "#             # Mark step for TPU\n",
    "#             xm.mark_step()\n",
    "            \n",
    "#             # Update counters - use allreduce to sync across TPU cores\n",
    "#             train_loss += loss.detach()\n",
    "#             train_dice += dice\n",
    "#             batch_count += 1\n",
    "            \n",
    "#             # Update progress bar with current metrics - only on master process\n",
    "#             if xm.is_master_ordinal() and isinstance(loader_iter, tqdm):\n",
    "#                 train_pbar.set_postfix({\n",
    "#                     'loss': f\"{loss.item():.5f}\", \n",
    "#                     'dice': f\"{dice.item():.5f}\"\n",
    "#                 })\n",
    "        \n",
    "#         # Calculate average training metrics - use allreduce to sync across TPU cores\n",
    "#         train_loss = xm.mesh_reduce('train_loss', train_loss, lambda x: sum(x) / len(x))\n",
    "#         train_dice = xm.mesh_reduce('train_dice', train_dice, lambda x: sum(x) / len(x))\n",
    "#         batch_count = xm.mesh_reduce('batch_count', batch_count, lambda x: sum(x))\n",
    "        \n",
    "#         avg_train_loss = train_loss / batch_count\n",
    "#         avg_train_dice = train_dice / batch_count\n",
    "        \n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_dice = 0.0\n",
    "#         val_dice_necrotic = 0.0\n",
    "#         val_dice_edema = 0.0\n",
    "#         val_dice_enhancing = 0.0\n",
    "#         val_precision = 0.0\n",
    "#         val_sensitivity = 0.0\n",
    "#         val_specificity = 0.0\n",
    "#         val_batch_count = 0\n",
    "        \n",
    "#         # Using tqdm for validation progress bar - only on master process\n",
    "#         if xm.is_master_ordinal():\n",
    "#             val_pbar = tqdm(val_para_loader.per_device_loader(device), \n",
    "#                            desc=f\"Validation Epoch {epoch+1}\", leave=False)\n",
    "#             val_loader_iter = val_pbar\n",
    "#         else:\n",
    "#             val_loader_iter = val_para_loader.per_device_loader(device)\n",
    "            \n",
    "#         with torch.no_grad():\n",
    "#             for images, masks in val_loader_iter:\n",
    "#                 # Print batch size occasionally - only on master process\n",
    "#                 if xm.is_master_ordinal() and (val_batch_count + 1) % 1000 == 0:\n",
    "#                     print(f\"Processed {val_batch_count + 1} validation batches. Current batch size: {images.shape[0]}\")\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 predictions = torch.sigmoid(outputs)\n",
    "                \n",
    "#                 # Calculate loss and metrics\n",
    "#                 loss = 1 - dice_coef(masks, predictions)\n",
    "#                 dice = dice_coef(masks, predictions).detach()\n",
    "#                 dice_necrotic = dice_coef_necrotic(masks, predictions).detach()\n",
    "#                 dice_edema = dice_coef_edema(masks, predictions).detach()\n",
    "#                 dice_enhancing = dice_coef_enhancing(masks, predictions).detach()\n",
    "#                 prec = precision(masks, predictions).detach()\n",
    "#                 sens = sensitivity(masks, predictions).detach()\n",
    "#                 spec = specificity(masks, predictions).detach()\n",
    "                \n",
    "#                 # Mark step for TPU\n",
    "#                 xm.mark_step()\n",
    "                \n",
    "#                 # Update counters\n",
    "#                 val_loss += loss.detach()\n",
    "#                 val_dice += dice\n",
    "#                 val_dice_necrotic += dice_necrotic\n",
    "#                 val_dice_edema += dice_edema\n",
    "#                 val_dice_enhancing += dice_enhancing\n",
    "#                 val_precision += prec\n",
    "#                 val_sensitivity += sens\n",
    "#                 val_specificity += spec\n",
    "#                 val_batch_count += 1\n",
    "                \n",
    "#                 # Update progress bar - only on master process\n",
    "#                 if xm.is_master_ordinal() and isinstance(val_loader_iter, tqdm):\n",
    "#                     val_pbar.set_postfix({\n",
    "#                         'val_loss': f\"{loss.item():.5f}\", \n",
    "#                         'val_dice': f\"{dice.item():.5f}\"\n",
    "#                     })\n",
    "        \n",
    "#         # Calculate average validation metrics - use allreduce to sync across TPU cores\n",
    "#         val_loss = xm.mesh_reduce('val_loss', val_loss, lambda x: sum(x) / len(x))\n",
    "#         val_dice = xm.mesh_reduce('val_dice', val_dice, lambda x: sum(x) / len(x))\n",
    "#         val_dice_necrotic = xm.mesh_reduce('val_dice_necrotic', val_dice_necrotic, lambda x: sum(x) / len(x))\n",
    "#         val_dice_edema = xm.mesh_reduce('val_dice_edema', val_dice_edema, lambda x: sum(x) / len(x))\n",
    "#         val_dice_enhancing = xm.mesh_reduce('val_dice_enhancing', val_dice_enhancing, lambda x: sum(x) / len(x))\n",
    "#         val_precision = xm.mesh_reduce('val_precision', val_precision, lambda x: sum(x) / len(x))\n",
    "#         val_sensitivity = xm.mesh_reduce('val_sensitivity', val_sensitivity, lambda x: sum(x) / len(x))\n",
    "#         val_specificity = xm.mesh_reduce('val_specificity', val_specificity, lambda x: sum(x) / len(x))\n",
    "#         val_batch_count = xm.mesh_reduce('val_batch_count', val_batch_count, lambda x: sum(x))\n",
    "        \n",
    "#         avg_val_loss = val_loss / val_batch_count\n",
    "#         avg_val_dice = val_dice / val_batch_count\n",
    "#         avg_val_dice_necrotic = val_dice_necrotic / val_batch_count\n",
    "#         avg_val_dice_edema = val_dice_edema / val_batch_count\n",
    "#         avg_val_dice_enhancing = val_dice_enhancing / val_batch_count\n",
    "#         avg_val_precision = val_precision / val_batch_count\n",
    "#         avg_val_sensitivity = val_sensitivity / val_batch_count\n",
    "#         avg_val_specificity = val_specificity / val_batch_count\n",
    "        \n",
    "#         # Update scheduler - only needed on one process\n",
    "#         if xm.is_master_ordinal():\n",
    "#             scheduler.step(avg_val_loss)\n",
    "        \n",
    "#         # Calculate epoch time\n",
    "#         epoch_time = time.time() - start_time\n",
    "        \n",
    "#         # Print epoch summary - only on master process\n",
    "#         if xm.is_master_ordinal():\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "#             print(f\"Train Loss: {avg_train_loss:.5f} | Train Dice: {avg_train_dice:.5f}\")\n",
    "#             print(f\"Val Loss: {avg_val_loss:.5f} | Val Dice: {avg_val_dice:.5f}\")\n",
    "#             print(f\"Dice Necrotic: {avg_val_dice_necrotic:.5f} | Dice Edema: {avg_val_dice_edema:.5f} | Dice Enhancing: {avg_val_dice_enhancing:.5f}\")\n",
    "#             print(f\"Precision: {avg_val_precision:.5f} | Sensitivity: {avg_val_sensitivity:.5f} | Specificity: {avg_val_specificity:.5f}\")\n",
    "            \n",
    "#             # Log metrics\n",
    "#             with open(log_filename, 'a', newline='') as f:\n",
    "#                 writer = csv.writer(f)\n",
    "#                 writer.writerow([\n",
    "#                     epoch+1, \n",
    "#                     f\"{avg_train_loss:.5f}\", \n",
    "#                     f\"{avg_val_loss:.5f}\", \n",
    "#                     f\"{avg_train_dice:.5f}\",\n",
    "#                     f\"{avg_val_dice:.5f}\",\n",
    "#                     f\"{avg_val_dice_necrotic:.5f}\",\n",
    "#                     f\"{avg_val_dice_edema:.5f}\",\n",
    "#                     f\"{avg_val_dice_enhancing:.5f}\",\n",
    "#                     f\"{avg_val_precision:.5f}\",\n",
    "#                     f\"{avg_val_sensitivity:.5f}\",\n",
    "#                     f\"{avg_val_specificity:.5f}\"\n",
    "#                 ])\n",
    "        \n",
    "#         # Check for model improvement - only need to save on master process\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             early_stop_counter = 0\n",
    "            \n",
    "#             if xm.is_master_ordinal():\n",
    "#                 # Save model checkpoint with 5 decimal precision in filename\n",
    "#                 checkpoint_filename = f\"3D-MedVision++-TPU-weights-improvement-{epoch+1:02d}-{avg_val_loss:.5f}.pt\"\n",
    "#                 checkpoint_path = os.path.join('checkpoints', checkpoint_filename)\n",
    "                \n",
    "#                 # Save model state - need to extract CPU state_dict for saving\n",
    "#                 xm.save({\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                     'train_loss': avg_train_loss,\n",
    "#                     'val_loss': avg_val_loss,\n",
    "#                     'val_dice': avg_val_dice,\n",
    "#                     'val_dice_necrotic': avg_val_dice_necrotic,\n",
    "#                     'val_dice_edema': avg_val_dice_edema,\n",
    "#                     'val_dice_enhancing': avg_val_dice_enhancing,\n",
    "#                     'val_precision': avg_val_precision,\n",
    "#                     'val_sensitivity': avg_val_sensitivity,\n",
    "#                     'val_specificity': avg_val_specificity,\n",
    "#                 }, checkpoint_path)\n",
    "                \n",
    "#                 print(f\"Model improved! Checkpoint saved to {checkpoint_path}\")\n",
    "#         else:\n",
    "#             # Increment early stopping counter\n",
    "#             early_stop_counter += 1\n",
    "#             if xm.is_master_ordinal():\n",
    "#                 print(f\"Model did not improve. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "#             # Check if early stopping should be triggered\n",
    "#             if early_stop_counter >= early_stop_patience:\n",
    "#                 if xm.is_master_ordinal():\n",
    "#                     print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "#                 break\n",
    "    \n",
    "#     if xm.is_master_ordinal():\n",
    "#         print(\"Training complete!\")\n",
    "#         print(f\"Best validation loss: {best_val_loss:.5f}\")\n",
    "    \n",
    "#     return best_val_loss\n",
    "\n",
    "# # Execute training with TPUs\n",
    "# if __name__ == '__main__':\n",
    "#     # Use all available TPU cores (should be 8 for TPU v3)\n",
    "#     xmp.spawn(train_tpu, nprocs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:23:44.110530Z",
     "iopub.status.busy": "2025-03-12T11:23:44.110173Z",
     "iopub.status.idle": "2025-03-12T11:24:28.233041Z",
     "shell.execute_reply": "2025-03-12T11:24:28.232152Z",
     "shell.execute_reply.started": "2025-03-12T11:23:44.110502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model export process...\n",
      "Latest checkpoint found: checkpoints/UNet++_best_dice_01_0.37361.pt\n",
      "Latest log file found: logs/training_UNet++_20250312_081842.log\n",
      "Export directory created: /kaggle/working/model_export\n",
      "Checkpoint copied to /kaggle/working/model_export/UNet++_best_dice_01_0.37361.pt\n",
      "Log file copied to /kaggle/working/model_export/training_UNet++_20250312_081842.log\n",
      "Plotting training metrics...\n",
      "Log data loaded with 1 entries\n",
      "Training metrics plot saved to /kaggle/working/model_export/plots/training_metrics.png\n",
      "Error plotting metrics: 'gpu_utilization'\n",
      "Exporting model in multiple formats...\n",
      "Error exporting model: local variable 'torch' referenced before assignment\n",
      "Creating zip file of all exported files...\n",
      "Export zip file created: /kaggle/working/3d_medvision_model_export_20250312_112346.zip\n",
      "Creating zip file of all training files...\n",
      "Found 4 files to zip\n",
      "Added checkpoints/UNet++_best_dice_01_0.37361.pt to zip\n",
      "Added logs/training_UNet++_20250312_081842.log to zip\n",
      "Added /kaggle/working/model_export/plots/training_metrics.png to zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/zipfile.py:1528: UserWarning: Duplicate name: 'UNet++_best_dice_01_0.37361.pt'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /kaggle/working/model_export/UNet++_best_dice_01_0.37361.pt to zip\n",
      "All training files zip created: /kaggle/working/all_training_files_20250312_112359.zip\n",
      "Setting up automatic downloads...\n",
      "Setting up download links for all files...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Download all_training_files_20250312_112359.zip (489.84 MB): <a href='/kaggle/working/all_training_files_20250312_112359.zip' target='_blank'>/kaggle/working/all_training_files_20250312_112359.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/all_training_files_20250312_112359.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Download UNet++_best_dice_01_0.37361.pt (265.99 MB): <a href='checkpoints/UNet++_best_dice_01_0.37361.pt' target='_blank'>checkpoints/UNet++_best_dice_01_0.37361.pt</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/checkpoints/UNet++_best_dice_01_0.37361.pt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Download training_UNet++_20250312_081842.log (0.00 MB): <a href='logs/training_UNet++_20250312_081842.log' target='_blank'>logs/training_UNet++_20250312_081842.log</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/logs/training_UNet++_20250312_081842.log"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Download training_metrics.png (0.27 MB): <a href='/kaggle/working/model_export/plots/training_metrics.png' target='_blank'>/kaggle/working/model_export/plots/training_metrics.png</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/model_export/plots/training_metrics.png"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All download links have been set up. Click the links to download the files.\n",
      "Note: Large files may take a moment to prepare for download.\n",
      "Model export process completed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:50:16.712991Z",
     "iopub.status.busy": "2025-03-12T11:50:16.712650Z",
     "iopub.status.idle": "2025-03-12T11:50:43.364219Z",
     "shell.execute_reply": "2025-03-12T11:50:43.363440Z",
     "shell.execute_reply.started": "2025-03-12T11:50:16.712968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Step 1: Creating directories...\n",
      "✓ Created directories: /kaggle/working/model_export\n",
      "Step 2: Finding latest checkpoint and log files...\n",
      "✓ Found checkpoint: UNet++_best_dice_01_0.37361.pt\n",
      "✓ Found log file: training_UNet++_20250312_081842.log\n",
      "Step 3: Plotting training metrics...\n",
      "✓ Loaded log data with 1 entries\n",
      "✓ Saved metrics plot to /kaggle/working/model_export/plots/training_metrics.png\n",
      "Step 4: Loading model from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-168ff9d2ef67>:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model state loaded successfully\n",
      "✓ Saved PyTorch model to /kaggle/working/model_export/model_full.pt\n",
      "Step 6: Running predictions on test data...\n",
      "Step 5: Creating synthetic test data...\n",
      "✓ Created 3 synthetic test samples\n",
      "  Running predictions and generating visualizations...\n",
      "❌ Error during prediction: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "Step 7: Exporting model to ONNX format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-f25dfd537cd3>:204: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if transformer_features.shape[2:] != bottleneck.shape[2:]:\n",
      "<ipython-input-17-f25dfd537cd3>:147: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[2] < 240:  # Assuming your input is 240x240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved ONNX model to /kaggle/working/model_export/model.onnx\n",
      "Step 8: Creating zip file of all outputs...\n",
      "✓ Created zip file: /kaggle/working/model_export_20250312_115019.zip\n",
      "\n",
      "✅ All done! Summary:\n",
      "- Export directory: /kaggle/working/model_export\n",
      "- Training metrics plot: /kaggle/working/model_export/plots/training_metrics.png\n",
      "- Exported PyTorch model: /kaggle/working/model_export/model_full.pt\n",
      "- Exported ONNX model: /kaggle/working/model_export/model.onnx\n",
      "- Zip archive: /kaggle/working/model_export_20250312_115019.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:53:37.919988Z",
     "iopub.status.busy": "2025-03-12T12:53:37.919650Z",
     "iopub.status.idle": "2025-03-12T12:54:26.302486Z",
     "shell.execute_reply": "2025-03-12T12:54:26.301761Z",
     "shell.execute_reply.started": "2025-03-12T12:53:37.919965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Step 1: Creating directories...\n",
      "✓ Created directories: /kaggle/working/model_export\n",
      "Step 2: Finding latest checkpoint and log files...\n",
      "✓ Found checkpoint: UNet++_best_dice_01_0.37361.pt\n",
      "✓ Found log file: training_UNet++_20250312_081842.log\n",
      "Step 3: Plotting training metrics...\n",
      "✓ Loaded log data with 1 entries\n",
      "✓ Saved metrics plot to /kaggle/working/model_export/plots/training_metrics.png\n",
      "Step 4: Loading model from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-da49e1edbafa>:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model state loaded successfully\n",
      "✓ Saved PyTorch model to /kaggle/working/model_export/model_full.pt\n",
      "Step 6: Running predictions on real BraTS data...\n",
      "  Processing test cases and generating visualizations...\n",
      "  Visualizing case: BraTS2021_00124, slice: 80\n",
      "  Processing case: BraTS2021_00124\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_00124/BraTS2021_00124_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_00124\n",
      "  Visualizing case: BraTS2021_00054, slice: 80\n",
      "  Processing case: BraTS2021_00054\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_00054/BraTS2021_00054_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_00054\n",
      "  Visualizing case: BraTS2021_01172, slice: 80\n",
      "  Processing case: BraTS2021_01172\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_01172/BraTS2021_01172_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_01172\n",
      "  Visualizing case: BraTS2021_00191, slice: 80\n",
      "  Processing case: BraTS2021_00191\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_00191/BraTS2021_00191_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_00191\n",
      "  Visualizing case: BraTS2021_01342, slice: 80\n",
      "  Processing case: BraTS2021_01342\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_01342/BraTS2021_01342_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_01342\n",
      "  Visualizing case: BraTS2021_00097, slice: 80\n",
      "  Processing case: BraTS2021_00097\n",
      "  Flair image path: ./BraTS2021_Training_Data/BraTS2021_00097/BraTS2021_00097_flair.nii.gz\n",
      "  ✓ Successfully processed case BraTS2021_00097\n",
      "✓ Saved prediction montage to /kaggle/working/model_export/predictions/predictions_montage.png\n",
      "Step 7: Exporting model to ONNX format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-f25dfd537cd3>:204: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if transformer_features.shape[2:] != bottleneck.shape[2:]:\n",
      "<ipython-input-17-f25dfd537cd3>:147: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[2] < 240:  # Assuming your input is 240x240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved ONNX model to /kaggle/working/model_export/model.onnx\n",
      "Step 8: Creating zip file of all outputs...\n",
      "✓ Created zip file: /kaggle/working/model_export_20250312_125402.zip\n",
      "\n",
      "✅ All done! Summary:\n",
      "- Export directory: /kaggle/working/model_export\n",
      "- Training metrics plot: /kaggle/working/model_export/plots/training_metrics.png\n",
      "- Exported PyTorch model: /kaggle/working/model_export/model_full.pt\n",
      "- Exported ONNX model: /kaggle/working/model_export/model.onnx\n",
      "- Prediction montage: /kaggle/working/model_export/predictions/predictions_montage.png\n",
      "- Zip archive: /kaggle/working/model_export_20250312_125402.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap\n",
    "from pathlib import Path\n",
    "\n",
    "# Set correct input/output shapes based on your model\n",
    "INPUT_CHANNELS = 2  # 2 input channels\n",
    "INPUT_HEIGHT = 240  # Height 240\n",
    "INPUT_WIDTH = 240   # Width 240\n",
    "OUTPUT_CLASSES = 4  # 4 output classes (background + 3 tumor regions)\n",
    "\n",
    "# Constants for the BraTS dataset\n",
    "IMG_SIZE = 240\n",
    "VOLUME_SLICES = 155\n",
    "VOLUME_START_AT = 0\n",
    "SEGMENT_CLASSES = {\n",
    "    0: 'Background',\n",
    "    1: 'Necrotic',\n",
    "    2: 'Edema',\n",
    "    3: 'Enhancing'\n",
    "}\n",
    "\n",
    "# Define the dataset path (adjust as needed)\n",
    "# TRAIN_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n",
    "\n",
    "# Step 1: Create directories for saving outputs\n",
    "def create_dirs():\n",
    "    print(\"Step 1: Creating directories...\")\n",
    "    export_dir = os.path.join(\"/kaggle/working\", \"model_export\")\n",
    "    plot_dir = os.path.join(export_dir, \"plots\")\n",
    "    pred_dir = os.path.join(export_dir, \"predictions\")\n",
    "    \n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ Created directories: {export_dir}\")\n",
    "    return export_dir, plot_dir, pred_dir\n",
    "\n",
    "# Step 2: Find the latest checkpoint and log file\n",
    "def find_latest_files():\n",
    "    print(\"Step 2: Finding latest checkpoint and log files...\")\n",
    "    \n",
    "    # Find checkpoint\n",
    "    checkpoint_files = glob.glob(os.path.join('checkpoints', '*.pt'))\n",
    "    if not checkpoint_files:\n",
    "        print(\"❌ No checkpoint files found!\")\n",
    "        return None, None\n",
    "    \n",
    "    checkpoint_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    latest_checkpoint = checkpoint_files[0]\n",
    "    \n",
    "    # Find log file\n",
    "    log_files = glob.glob(os.path.join('logs', '*.log'))\n",
    "    if not log_files:\n",
    "        print(\"❌ No log files found!\")\n",
    "        return latest_checkpoint, None\n",
    "    \n",
    "    log_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    latest_log = log_files[0]\n",
    "    \n",
    "    print(f\"✓ Found checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"✓ Found log file: {os.path.basename(latest_log)}\")\n",
    "    \n",
    "    return latest_checkpoint, latest_log\n",
    "\n",
    "# Step 3: Plot training metrics from log file\n",
    "def plot_metrics(log_file, plot_dir):\n",
    "    print(\"Step 3: Plotting training metrics...\")\n",
    "    if not log_file or not os.path.exists(log_file):\n",
    "        print(\"❌ Log file not available for plotting\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read log file\n",
    "        df = pd.read_csv(log_file)\n",
    "        print(f\"✓ Loaded log data with {len(df)} entries\")\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['loss'], label='Training', marker='o', markersize=3)\n",
    "        plt.plot(df['epoch'], df['val_loss'], label='Validation', marker='s', markersize=3)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Dice Coefficient\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['dice_coef'], label='Training', marker='o', markersize=3)\n",
    "        plt.plot(df['epoch'], df['val_dice_coef'], label='Validation', marker='s', markersize=3)\n",
    "        plt.title('Dice Coefficient')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Class-wise Dice\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if 'dice_necrotic' in df.columns:\n",
    "            plt.plot(df['epoch'], df['dice_necrotic'], label='Necrotic', marker='o', markersize=3)\n",
    "            plt.plot(df['epoch'], df['dice_edema'], label='Edema', marker='s', markersize=3)\n",
    "            plt.plot(df['epoch'], df['dice_enhancing'], label='Enhancing', marker='^', markersize=3)\n",
    "            plt.title('Class-wise Dice')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "        \n",
    "        # Plot Other Metrics\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'precision' in df.columns:\n",
    "            plt.plot(df['epoch'], df['precision'], label='Precision', marker='o', markersize=3)\n",
    "            plt.plot(df['epoch'], df['sensitivity'], label='Sensitivity', marker='s', markersize=3)\n",
    "            plt.plot(df['epoch'], df['specificity'], label='Specificity', marker='^', markersize=3)\n",
    "            plt.title('Other Metrics')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        metrics_plot_path = os.path.join(plot_dir, \"training_metrics.png\")\n",
    "        plt.savefig(metrics_plot_path, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ Saved metrics plot to {metrics_plot_path}\")\n",
    "        return metrics_plot_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error plotting metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 4: Load model from checkpoint\n",
    "def load_model(checkpoint_path, export_dir, device='cpu'):\n",
    "    print(\"Step 4: Loading model from checkpoint...\")\n",
    "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
    "        print(\"❌ Checkpoint not available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model_state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Assuming model is already defined in global scope \n",
    "        # but providing a fallback message\n",
    "        if 'model' not in globals():\n",
    "            print(\"❌ 'model' variable not found in global scope. Make sure your model is defined.\")\n",
    "            print(\"   For testing purposes, the code will continue but may fail.\")\n",
    "        \n",
    "        # Load state dict into model\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        model.eval()\n",
    "        print(\"✓ Model state loaded successfully\")\n",
    "        \n",
    "        # Save the PyTorch model for later use\n",
    "        torch_model_path = os.path.join(export_dir, \"model_full.pt\")\n",
    "        torch.save(model, torch_model_path)\n",
    "        print(f\"✓ Saved PyTorch model to {torch_model_path}\")\n",
    "        \n",
    "        return model, torch_model_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 6: Run predictions on real data (replaces the synthetic data prediction)\n",
    "def predict_on_real_data(loaded_model, pred_dir, device='cpu'):\n",
    "    print(\"Step 6: Running predictions on real BraTS data...\")\n",
    "    \n",
    "    if loaded_model is None:\n",
    "        print(\"❌ Model not available for prediction\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Test cases\n",
    "        test_cases = [\n",
    "            'BraTS2021_00124', 'BraTS2021_00054', 'BraTS2021_01172', \n",
    "            'BraTS2021_00191', 'BraTS2021_01342', 'BraTS2021_00097'\n",
    "        ]\n",
    "        \n",
    "        # Check if dataset exists\n",
    "        if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "            print(f\"❌ Dataset path not found: {TRAIN_DATASET_PATH}\")\n",
    "            print(\"  Falling back to synthetic data prediction...\")\n",
    "            return predict_on_synthetic_data(loaded_model, pred_dir, device)\n",
    "        \n",
    "        # In the predict_on_real_data function, modify the predict_by_path function:\n",
    "        def predict_by_path(case_path, case):\n",
    "            print(f\"  Processing case: {case}\")\n",
    "            \n",
    "            vol_path_flair = os.path.join(case_path, f'{case}_flair.nii.gz')\n",
    "            print(f'  Flair image path: {vol_path_flair}')\n",
    "            \n",
    "            try:\n",
    "                flair = nib.load(vol_path_flair).get_fdata()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  ❌ Error: Flair image file not found for case {case}\")\n",
    "                return None\n",
    "            \n",
    "            # Prepare input tensor\n",
    "            X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, INPUT_CHANNELS))\n",
    "            for j in range(VOLUME_SLICES):\n",
    "                X[j, :, :, 0] = cv2.resize(flair[:, :, j + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
    "                # Add zeros for second channel if needed (or add t1ce data)\n",
    "                X[j, :, :, 1] = np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Convert to PyTorch tensor and normalize\n",
    "            X_tensor = torch.tensor(X / np.max(X), dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "            \n",
    "            # Predict using loaded model\n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 8  # Process in batches to save memory\n",
    "                for i in range(0, VOLUME_SLICES, batch_size):\n",
    "                    batch = X_tensor[i:min(i+batch_size, VOLUME_SLICES)].to(device)\n",
    "                    # Move model to the same device as input data before inference\n",
    "                    outputs = loaded_model.to(device)(batch)\n",
    "                    pred_batch = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    predictions.append(pred_batch)\n",
    "                    \n",
    "            return np.vstack(predictions)\n",
    "        # Function to create visualization for a specific case\n",
    "        def visualize_case(case, start_slice=60, pred_dir=pred_dir):\n",
    "            print(f\"  Visualizing case: {case}, slice: {start_slice}\")\n",
    "            path = os.path.join(TRAIN_DATASET_PATH, case)\n",
    "            \n",
    "            try:\n",
    "                # Load ground truth and original image\n",
    "                gt_path = os.path.join(path, f'{case}_seg.nii.gz')\n",
    "                if os.path.exists(gt_path):\n",
    "                    gt = nib.load(gt_path).get_fdata()\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Ground truth not found for case {case}\")\n",
    "                    gt = np.zeros((240, 240, 155))  # Create empty ground truth\n",
    "                \n",
    "                orig_path = os.path.join(path, f'{case}_flair.nii.gz')\n",
    "                if not os.path.exists(orig_path):\n",
    "                    print(f\"  ❌ Original image not found for case {case}\")\n",
    "                    return None\n",
    "                    \n",
    "                orig_image = nib.load(orig_path).get_fdata()\n",
    "                \n",
    "                # Get predictions\n",
    "                p = predict_by_path(path, case)\n",
    "                if p is None:\n",
    "                    return None\n",
    "                \n",
    "                # Extract different tumor regions\n",
    "                core = p[:, 1, :, :]  # Necrotic core\n",
    "                edema = p[:, 2, :, :]  # Edema\n",
    "                enhancing = p[:, 3, :, :]  # Enhancing tumor\n",
    "                \n",
    "                # Create figure\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                # Original image\n",
    "                plt.subplot(1, 5, 1)\n",
    "                plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "                plt.title('Original FLAIR')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Ground truth\n",
    "                plt.subplot(1, 5, 2)\n",
    "                plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "                curr_gt = cv2.resize(gt[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                plt.imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # All classes prediction\n",
    "                plt.subplot(1, 5, 3)\n",
    "                plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "                all_classes = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "                all_classes[:, :, 0] = core[start_slice]\n",
    "                all_classes[:, :, 1] = edema[start_slice]\n",
    "                all_classes[:, :, 2] = enhancing[start_slice]\n",
    "                plt.imshow(all_classes, interpolation='none', alpha=0.3)\n",
    "                plt.title('All Classes')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Core prediction\n",
    "                plt.subplot(1, 5, 4)\n",
    "                plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "                plt.imshow(core[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "                plt.title(f'{SEGMENT_CLASSES[1]} Core')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Edema prediction\n",
    "                plt.subplot(1, 5, 5)\n",
    "                plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "                plt.imshow(edema[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "                plt.title(f'{SEGMENT_CLASSES[2]} Edema')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Save the figure\n",
    "                plt.tight_layout()\n",
    "                pred_path = os.path.join(pred_dir, f\"prediction_{case}_slice{start_slice}.png\")\n",
    "                plt.savefig(pred_path, dpi=200, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                return pred_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error visualizing case {case}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Process all test cases\n",
    "        print(\"  Processing test cases and generating visualizations...\")\n",
    "        success_paths = []\n",
    "        \n",
    "        for case in test_cases:\n",
    "            try:\n",
    "                result_path = visualize_case(case, start_slice=80)\n",
    "                if result_path:\n",
    "                    success_paths.append(result_path)\n",
    "                    print(f\"  ✓ Successfully processed case {case}\")\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Failed to process case {case}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error processing case {case}: {e}\")\n",
    "        \n",
    "        # Create a montage of all successful predictions\n",
    "        if success_paths:\n",
    "            plt.figure(figsize=(15, len(success_paths) * 5))\n",
    "            for i, path in enumerate(success_paths):\n",
    "                plt.subplot(len(success_paths), 1, i+1)\n",
    "                img = plt.imread(path)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Case {os.path.basename(path).split('_')[1]}\")\n",
    "            \n",
    "            montage_path = os.path.join(pred_dir, \"predictions_montage.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(montage_path, dpi=200)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✓ Saved prediction montage to {montage_path}\")\n",
    "            return montage_path\n",
    "        else:\n",
    "            print(\"❌ No successful predictions to create montage\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during prediction on real data: {e}\")\n",
    "        print(\"  Falling back to synthetic data prediction...\")\n",
    "        return predict_on_synthetic_data(loaded_model, pred_dir, device)\n",
    "\n",
    "# Fallback to synthetic data prediction if real data not available\n",
    "def predict_on_synthetic_data(loaded_model, pred_dir, device='cpu'):\n",
    "    print(\"Using synthetic test data instead...\")\n",
    "    \n",
    "    try:\n",
    "        # Create tensors for images and masks (3 samples)\n",
    "        images = torch.randn(3, INPUT_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH)\n",
    "        masks = torch.zeros(3, OUTPUT_CLASSES, INPUT_HEIGHT, INPUT_WIDTH)\n",
    "        \n",
    "        # Create simple patterns in the masks\n",
    "        for i in range(3):\n",
    "            # Create center coordinates and radii\n",
    "            center_x, center_y = np.random.randint(80, 160, 2)\n",
    "            radius_edema = np.random.randint(30, 60)\n",
    "            radius_necrotic = np.random.randint(10, 25)\n",
    "            radius_enhancing = np.random.randint(5, 15)\n",
    "            \n",
    "            # Create coordinate grids\n",
    "            y, x = np.ogrid[:INPUT_HEIGHT, :INPUT_WIDTH]\n",
    "            dist_from_center = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)\n",
    "            \n",
    "            # Create masks for each tumor component\n",
    "            masks[i, 1, dist_from_center < radius_necrotic] = 1  # Necrotic core\n",
    "            masks[i, 2, dist_from_center < radius_edema] = 1     # Edema\n",
    "            masks[i, 3, (dist_from_center > radius_necrotic) & \n",
    "                       (dist_from_center < radius_necrotic + radius_enhancing)] = 1  # Enhancing\n",
    "        \n",
    "        # Setup visualization\n",
    "        colors = [(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)]  # Black, Red, Green, Blue\n",
    "        custom_cmap = ListedColormap(colors)\n",
    "        \n",
    "        # Run predictions\n",
    "        print(\"  Running predictions on synthetic data...\")\n",
    "        montage_fig = plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(images)):\n",
    "                # Get prediction\n",
    "                image = images[i:i+1].to(device)\n",
    "                output = loaded_model(image)\n",
    "                prediction = torch.sigmoid(output)\n",
    "                \n",
    "                # Convert to numpy for visualization\n",
    "                image_np = image[0].cpu().numpy()\n",
    "                mask_np = masks[i].cpu().numpy()\n",
    "                pred_np = prediction[0].cpu().numpy()\n",
    "                \n",
    "                # Get class predictions\n",
    "                pred_mask = np.argmax(pred_np, axis=0)\n",
    "                true_mask = np.argmax(mask_np, axis=0)\n",
    "                \n",
    "                # Create visualization\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                # Original image\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image_np[0], cmap='gray')\n",
    "                plt.title('Synthetic MRI Image')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Ground truth\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(true_mask, cmap=custom_cmap, vmin=0, vmax=3)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Prediction\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(pred_mask, cmap=custom_cmap, vmin=0, vmax=3)\n",
    "                plt.title('Prediction')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Add a legend\n",
    "                import matplotlib.patches as mpatches\n",
    "                patches = [\n",
    "                    mpatches.Patch(color='black', label='Background'),\n",
    "                    mpatches.Patch(color='red', label='Necrotic'),\n",
    "                    mpatches.Patch(color='green', label='Edema'),\n",
    "                    mpatches.Patch(color='blue', label='Enhancing')\n",
    "                ]\n",
    "                plt.figlegend(handles=patches, loc='lower center', ncol=4)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save individual prediction\n",
    "                pred_path = os.path.join(pred_dir, f\"synthetic_prediction_{i+1}.png\")\n",
    "                plt.savefig(pred_path, dpi=200, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Add to montage\n",
    "                ax = montage_fig.add_subplot(3, 1, i+1)\n",
    "                img = plt.imread(pred_path)\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"Synthetic Sample {i+1}\")\n",
    "            \n",
    "            # Save montage\n",
    "            montage_path = os.path.join(pred_dir, \"synthetic_predictions_montage.png\")\n",
    "            montage_fig.tight_layout()\n",
    "            montage_fig.savefig(montage_path, dpi=200)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✓ Saved synthetic prediction montage to {montage_path}\")\n",
    "            return montage_path\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during synthetic prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 7: Export model to ONNX format\n",
    "def export_to_onnx(loaded_model, export_dir):\n",
    "    print(\"Step 7: Exporting model to ONNX format...\")\n",
    "    \n",
    "    if loaded_model is None:\n",
    "        print(\"❌ Model not available for ONNX export\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Export to ONNX format with higher opset version\n",
    "        onnx_path = os.path.join(export_dir, \"model.onnx\")\n",
    "        dummy_input = torch.randn(1, INPUT_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH)\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            loaded_model.to('cpu'),\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'},\n",
    "                         'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Saved ONNX model to {onnx_path}\")\n",
    "        return onnx_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting to ONNX: {e}\")\n",
    "        print(\"  Continuing without ONNX export.\")\n",
    "        return None\n",
    "\n",
    "# Step 8: Create a zip file of all exported files\n",
    "def create_zip_file(export_dir):\n",
    "    print(\"Step 8: Creating zip file of all outputs...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_path = os.path.join(\"/kaggle/working\", f\"model_export_{timestamp}.zip\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(export_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, start=os.path.dirname(export_dir))\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        print(f\"✓ Created zip file: {zip_path}\")\n",
    "        return zip_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating zip file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to run everything\n",
    "def main():\n",
    "    # For debugging purposes\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Step 1: Create directories\n",
    "    export_dir, plot_dir, pred_dir = create_dirs()\n",
    "    \n",
    "    # Step 2: Find latest files\n",
    "    checkpoint_path, log_path = find_latest_files()\n",
    "    if not checkpoint_path:\n",
    "        print(\"❌ Cannot continue without a checkpoint file.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Plot metrics\n",
    "    if log_path:\n",
    "        metrics_plot = plot_metrics(log_path, plot_dir)\n",
    "    \n",
    "    # Step 4: Load model from checkpoint\n",
    "    loaded_model, model_path = load_model(checkpoint_path, export_dir, device)\n",
    "    if loaded_model is None:\n",
    "        print(\"❌ Cannot continue without loading the model.\")\n",
    "        return\n",
    "    \n",
    "    # Step 6: Run predictions on real data (replaces synthetic data)\n",
    "    montage_path = predict_on_real_data(loaded_model, pred_dir, device)\n",
    "    \n",
    "    # Step 7: Export to ONNX\n",
    "    onnx_path = export_to_onnx(loaded_model, export_dir)\n",
    "    \n",
    "    # Step 8: Create zip file\n",
    "    zip_path = create_zip_file(export_dir)\n",
    "    \n",
    "    print(\"\\n✅ All done! Summary:\")\n",
    "    print(f\"- Export directory: {export_dir}\")\n",
    "    if 'metrics_plot' in locals() and metrics_plot:\n",
    "        print(f\"- Training metrics plot: {metrics_plot}\")\n",
    "    if 'model_path' in locals() and model_path:\n",
    "        print(f\"- Exported PyTorch model: {model_path}\")\n",
    "    if 'onnx_path' in locals() and onnx_path:\n",
    "        print(f\"- Exported ONNX model: {onnx_path}\")\n",
    "    if 'montage_path' in locals() and montage_path:\n",
    "        print(f\"- Prediction montage: {montage_path}\")\n",
    "    if 'zip_path' in locals() and zip_path:\n",
    "        print(f\"- Zip archive: {zip_path}\")\n",
    "    else:\n",
    "        print(\"  Note: Some exports may have failed. Check the logs above for details.\")\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:40:05.144829Z",
     "iopub.status.busy": "2025-03-14T15:40:05.144474Z",
     "iopub.status.idle": "2025-03-14T15:40:24.605306Z",
     "shell.execute_reply": "2025-03-14T15:40:24.604270Z",
     "shell.execute_reply.started": "2025-03-14T15:40:05.144804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "# Performance monitoring\n",
    "def get_memory_usage():\n",
    "    \"\"\"Return memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Constants for the BraTS dataset\n",
    "IMG_SIZE = 240\n",
    "VOLUME_SLICES = 155\n",
    "VOLUME_START_AT = 0\n",
    "SEGMENT_CLASSES = {\n",
    "    0: 'Background',\n",
    "    1: 'Necrotic',\n",
    "    2: 'Edema',\n",
    "    3: 'Enhancing'\n",
    "}\n",
    "\n",
    "# Function to crop brain region\n",
    "def crop_brain_region(image, threshold=0.05):\n",
    "    \"\"\"Crop the brain region from the image\"\"\"\n",
    "    # Normalize the image\n",
    "    normalized = image / np.max(image)\n",
    "    \n",
    "    # Create binary mask where the brain is\n",
    "    brain_mask = normalized > threshold\n",
    "    \n",
    "    # Find bounding box\n",
    "    coords = np.argwhere(brain_mask)\n",
    "    if len(coords) == 0:  # No brain found\n",
    "        return image\n",
    "        \n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    \n",
    "    # Add padding\n",
    "    padding = 10\n",
    "    y_min = max(0, y_min - padding)\n",
    "    y_max = min(image.shape[0], y_max + padding)\n",
    "    x_min = max(0, x_min - padding)\n",
    "    x_max = min(image.shape[1], x_max + padding)\n",
    "    \n",
    "    # Crop brain region\n",
    "    cropped = image[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Resize back to original size for model compatibility\n",
    "    return cv2.resize(cropped, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# Create directories\n",
    "def setup_output_dirs():\n",
    "    pred_dir = os.path.join(os.getcwd(), \"predictions\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    print(f\"✓ Created prediction directory: {pred_dir}\")\n",
    "    return pred_dir\n",
    "\n",
    "# Load model from checkpoint\n",
    "def load_model(device='cpu'):\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    \n",
    "    # Find checkpoint\n",
    "    checkpoint_files = glob.glob(os.path.join('checkpoints', '*.pt'))\n",
    "    if not checkpoint_files:\n",
    "        raise FileNotFoundError(\"No checkpoint files found!\")\n",
    "    \n",
    "    checkpoint_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    latest_checkpoint = checkpoint_files[0]\n",
    "    print(f\"✓ Using checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    \n",
    "    # Attempt to load full model first\n",
    "    if 'model' in checkpoint:\n",
    "        model = checkpoint['model']\n",
    "        print(\"✓ Full model loaded from checkpoint\")\n",
    "    else:\n",
    "        # If only state_dict is present, need model definition\n",
    "        if 'model' not in globals():\n",
    "            raise ValueError(\"Model definition required but not found in globals\")\n",
    "        \n",
    "        model_state_dict = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        print(\"✓ Model state loaded from checkpoint\")\n",
    "    \n",
    "    # Save as full model\n",
    "    torch_model_path = os.path.join(os.getcwd(), \"model_full.pt\")\n",
    "    torch.save(model, torch_model_path)\n",
    "    print(f\"✓ Saved PyTorch model to {torch_model_path}\")\n",
    "    \n",
    "    return model, torch_model_path\n",
    "\n",
    "# Run predictions on test cases\n",
    "def predict_on_test_cases(loaded_model, pred_dir, device='cpu'):\n",
    "    print(\"Running predictions on test cases with performance metrics...\")\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        'BraTS2021_00124', 'BraTS2021_00054', 'BraTS2021_01172', \n",
    "        'BraTS2021_00191', 'BraTS2021_01342', 'BraTS2021_00097'\n",
    "    ]\n",
    "    \n",
    "    # Performance metrics\n",
    "    performance_metrics = {\n",
    "        'load_time': [],\n",
    "        'preprocessing_time': [],\n",
    "        'inference_time': [],\n",
    "        'memory_before': [],\n",
    "        'memory_after': [],\n",
    "        'total_time': []\n",
    "    }\n",
    "    \n",
    "    # Define dataset path - adjust as needed\n",
    "    TRAIN_DATASET_PATH = '/kaggle/working/BraTS2021_Training_Data'\n",
    "    \n",
    "    # Prediction function with brain cropping\n",
    "    def predict_by_path(case_path, case):\n",
    "        print(f\"  Processing case: {case}\")\n",
    "        \n",
    "        # Track memory before loading\n",
    "        memory_before = get_memory_usage()\n",
    "        performance_metrics['memory_before'].append(memory_before)\n",
    "        \n",
    "        # Record start time for loading\n",
    "        load_start = time.time()\n",
    "        \n",
    "        vol_path_flair = os.path.join(case_path, f'{case}_flair.nii.gz')\n",
    "        print(f'  Flair image path: {vol_path_flair}')\n",
    "        \n",
    "        try:\n",
    "            flair = nib.load(vol_path_flair).get_fdata()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ❌ Error: Flair image file not found for case {case}\")\n",
    "            return None\n",
    "            \n",
    "        load_time = time.time() - load_start\n",
    "        performance_metrics['load_time'].append(load_time)\n",
    "        \n",
    "        # Record preprocessing time (including brain cropping)\n",
    "        preprocess_start = time.time()\n",
    "        \n",
    "        X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n",
    "        for j in range(VOLUME_SLICES):\n",
    "            # Apply brain cropping\n",
    "            cropped_brain = crop_brain_region(flair[:, :, j + VOLUME_START_AT])\n",
    "            X[j, :, :, 0] = cropped_brain\n",
    "            # Add zeros for second channel \n",
    "            X[j, :, :, 1] = np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        # Convert to PyTorch tensor and normalize\n",
    "        X_tensor = torch.tensor(X / np.max(X), dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        \n",
    "        preprocess_time = time.time() - preprocess_start\n",
    "        performance_metrics['preprocessing_time'].append(preprocess_time)\n",
    "        \n",
    "        # Predict using loaded model\n",
    "        inference_start = time.time()\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            batch_size = 8  # Process in batches to save memory\n",
    "            for i in range(0, VOLUME_SLICES, batch_size):\n",
    "                batch = X_tensor[i:min(i+batch_size, VOLUME_SLICES)].to(device)\n",
    "                outputs = loaded_model.to(device)(batch)\n",
    "                pred_batch = torch.sigmoid(outputs).cpu().numpy()\n",
    "                predictions.append(pred_batch)\n",
    "        \n",
    "        inference_time = time.time() - inference_start\n",
    "        performance_metrics['inference_time'].append(inference_time)\n",
    "        \n",
    "        # Track memory after inference\n",
    "        memory_after = get_memory_usage()\n",
    "        performance_metrics['memory_after'].append(memory_after)\n",
    "        \n",
    "        total_time = load_time + preprocess_time + inference_time\n",
    "        performance_metrics['total_time'].append(total_time)\n",
    "        \n",
    "        print(f\"  ✓ Case {case} processed in {total_time:.2f}s (Load: {load_time:.2f}s, Preprocess: {preprocess_time:.2f}s, Inference: {inference_time:.2f}s)\")\n",
    "        print(f\"  ✓ Memory usage: {memory_before:.2f}MB → {memory_after:.2f}MB (Δ: {memory_after-memory_before:.2f}MB)\")\n",
    "        \n",
    "        return np.vstack(predictions), flair\n",
    "\n",
    "    # Function to create visualization for a specific case\n",
    "    def visualize_case(case, start_slice=80):\n",
    "        print(f\"  Visualizing case: {case}, slice: {start_slice}\")\n",
    "        path = os.path.join(TRAIN_DATASET_PATH, case)\n",
    "        \n",
    "        try:\n",
    "            # Get predictions and original image\n",
    "            p, orig_image = predict_by_path(path, case)\n",
    "            if p is None:\n",
    "                return None\n",
    "            \n",
    "            # Load ground truth if available\n",
    "            gt_path = os.path.join(path, f'{case}_seg.nii.gz')\n",
    "            if os.path.exists(gt_path):\n",
    "                gt = nib.load(gt_path).get_fdata()\n",
    "                has_gt = True\n",
    "            else:\n",
    "                print(f\"  ⚠️ Ground truth not found for case {case}\")\n",
    "                gt = np.zeros((240, 240, 155))  # Create empty ground truth\n",
    "                has_gt = False\n",
    "            \n",
    "            # Extract different tumor regions\n",
    "            core = p[:, 1, :, :]      # Necrotic core\n",
    "            edema = p[:, 2, :, :]     # Edema\n",
    "            enhancing = p[:, 3, :, :] # Enhancing tumor\n",
    "            \n",
    "            # Get the original slice and crop the brain\n",
    "            orig_slice = orig_image[:, :, start_slice + VOLUME_START_AT]\n",
    "            cropped_brain = crop_brain_region(orig_slice)\n",
    "            \n",
    "            # If we have ground truth, also crop it\n",
    "            if has_gt:\n",
    "                gt_slice = gt[:, :, start_slice + VOLUME_START_AT]\n",
    "                # Ensure gt_slice is cropped exactly like the brain image\n",
    "                # First, get the bounds from the original image\n",
    "                normalized = orig_slice / np.max(orig_slice)\n",
    "                brain_mask = normalized > 0.05\n",
    "                coords = np.argwhere(brain_mask)\n",
    "                \n",
    "                if len(coords) > 0:\n",
    "                    y_min, x_min = coords.min(axis=0)\n",
    "                    y_max, x_max = coords.max(axis=0)\n",
    "                    \n",
    "                    # Add padding\n",
    "                    padding = 10\n",
    "                    y_min = max(0, y_min - padding)\n",
    "                    y_max = min(orig_slice.shape[0], y_max + padding)\n",
    "                    x_min = max(0, x_min - padding)\n",
    "                    x_max = min(orig_slice.shape[1], x_max + padding)\n",
    "                    \n",
    "                    # Crop gt with the same bounds\n",
    "                    gt_slice_cropped = gt_slice[y_min:y_max, x_min:x_max]\n",
    "                    gt_slice_resized = cv2.resize(gt_slice_cropped, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                else:\n",
    "                    # Fallback if no brain detected\n",
    "                    gt_slice_resized = cv2.resize(gt_slice, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Create figure\n",
    "            fig_cols = 5 if has_gt else 4\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Original image (cropped brain)\n",
    "            plt.subplot(1, fig_cols, 1)\n",
    "            plt.imshow(cropped_brain, cmap=\"gray\")\n",
    "            plt.title('Cropped Brain FLAIR')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Ground truth if available\n",
    "            if has_gt:\n",
    "                plt.subplot(1, fig_cols, 2)\n",
    "                plt.imshow(cropped_brain, cmap=\"gray\")\n",
    "                plt.imshow(gt_slice_resized, cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                start_idx = 3\n",
    "            else:\n",
    "                start_idx = 2\n",
    "            \n",
    "            # All classes prediction\n",
    "            plt.subplot(1, fig_cols, start_idx)\n",
    "            plt.imshow(cropped_brain, cmap=\"gray\")\n",
    "            all_classes = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "            all_classes[:, :, 0] = core[start_slice]\n",
    "            all_classes[:, :, 1] = edema[start_slice]\n",
    "            all_classes[:, :, 2] = enhancing[start_slice]\n",
    "            plt.imshow(all_classes, interpolation='none', alpha=0.3)\n",
    "            plt.title('All Classes')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Core prediction\n",
    "            plt.subplot(1, fig_cols, start_idx+1)\n",
    "            plt.imshow(cropped_brain, cmap=\"gray\")\n",
    "            plt.imshow(core[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "            plt.title(f'{SEGMENT_CLASSES[1]} Core')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Edema prediction\n",
    "            plt.subplot(1, fig_cols, start_idx+2)\n",
    "            plt.imshow(cropped_brain, cmap=\"gray\")\n",
    "            plt.imshow(edema[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "            plt.title(f'{SEGMENT_CLASSES[2]} Edema')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            pred_path = os.path.join(pred_dir, f\"prediction_{case}_slice{start_slice}.png\")\n",
    "            plt.savefig(pred_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            return pred_path\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error visualizing case {case}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    # # Function to create visualization for a specific case\n",
    "    # def visualize_case(case, start_slice=80):\n",
    "    #     print(f\"  Visualizing case: {case}, slice: {start_slice}\")\n",
    "    #     path = os.path.join(TRAIN_DATASET_PATH, case)\n",
    "        \n",
    "    #     try:\n",
    "    #         # Get predictions and original image\n",
    "    #         p, orig_image = predict_by_path(path, case)\n",
    "    #         if p is None:\n",
    "    #             return None\n",
    "            \n",
    "    #         # Load ground truth if available\n",
    "    #         gt_path = os.path.join(path, f'{case}_seg.nii.gz')\n",
    "    #         if os.path.exists(gt_path):\n",
    "    #             gt = nib.load(gt_path).get_fdata()\n",
    "    #             has_gt = True\n",
    "    #         else:\n",
    "    #             print(f\"  ⚠️ Ground truth not found for case {case}\")\n",
    "    #             gt = np.zeros((240, 240, 155))  # Create empty ground truth\n",
    "    #             has_gt = False\n",
    "            \n",
    "    #         # Extract different tumor regions\n",
    "    #         core = p[:, 1, :, :]      # Necrotic core\n",
    "    #         edema = p[:, 2, :, :]     # Edema\n",
    "    #         enhancing = p[:, 3, :, :] # Enhancing tumor\n",
    "            \n",
    "    #         # Create figure\n",
    "    #         fig_cols = 5 if has_gt else 4\n",
    "    #         plt.figure(figsize=(15, 5))\n",
    "            \n",
    "    #         # Original image\n",
    "    #         plt.subplot(1, fig_cols, 1)\n",
    "    #         plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "    #         plt.title('Original FLAIR')\n",
    "    #         plt.axis('off')\n",
    "            \n",
    "    #         # Ground truth if available\n",
    "    #         if has_gt:\n",
    "    #             plt.subplot(1, fig_cols, 2)\n",
    "    #             plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "    #             curr_gt = cv2.resize(gt[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "    #             plt.imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
    "    #             plt.title('Ground Truth')\n",
    "    #             plt.axis('off')\n",
    "                \n",
    "    #             start_idx = 3\n",
    "    #         else:\n",
    "    #             start_idx = 2\n",
    "            \n",
    "    #         # All classes prediction\n",
    "    #         plt.subplot(1, fig_cols, start_idx)\n",
    "    #         plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "    #         all_classes = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "    #         all_classes[:, :, 0] = core[start_slice]\n",
    "    #         all_classes[:, :, 1] = edema[start_slice]\n",
    "    #         all_classes[:, :, 2] = enhancing[start_slice]\n",
    "    #         plt.imshow(all_classes, interpolation='none', alpha=0.3)\n",
    "    #         plt.title('All Classes')\n",
    "    #         plt.axis('off')\n",
    "            \n",
    "    #         # Core prediction\n",
    "    #         plt.subplot(1, fig_cols, start_idx+1)\n",
    "    #         plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "    #         plt.imshow(core[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "    #         plt.title(f'{SEGMENT_CLASSES[1]} Core')\n",
    "    #         plt.axis('off')\n",
    "            \n",
    "    #         # Edema prediction\n",
    "    #         plt.subplot(1, fig_cols, start_idx+2)\n",
    "    #         plt.imshow(cv2.resize(orig_image[:, :, start_slice + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
    "    #         plt.imshow(edema[start_slice], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
    "    #         plt.title(f'{SEGMENT_CLASSES[2]} Edema')\n",
    "    #         plt.axis('off')\n",
    "            \n",
    "    #         # Save the figure\n",
    "    #         plt.tight_layout()\n",
    "    #         pred_path = os.path.join(pred_dir, f\"prediction_{case}_slice{start_slice}.png\")\n",
    "    #         plt.savefig(pred_path, dpi=200, bbox_inches='tight')\n",
    "    #         plt.close()\n",
    "            \n",
    "    #         return pred_path\n",
    "                \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"  ❌ Error visualizing case {case}: {e}\")\n",
    "    #         return None\n",
    "    \n",
    "    # Process all test cases\n",
    "    print(\"  Processing test cases and generating visualizations...\")\n",
    "    success_paths = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        try:\n",
    "            result_path = visualize_case(case, start_slice=80)\n",
    "            if result_path:\n",
    "                success_paths.append(result_path)\n",
    "                print(f\"  ✓ Successfully processed case {case}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Failed to process case {case}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing case {case}: {e}\")\n",
    "    \n",
    "    # Create a montage of all successful predictions\n",
    "    if success_paths:\n",
    "        plt.figure(figsize=(15, len(success_paths) * 5))\n",
    "        for i, path in enumerate(success_paths):\n",
    "            plt.subplot(len(success_paths), 1, i+1)\n",
    "            img = plt.imread(path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Case {os.path.basename(path).split('_')[1]}\")\n",
    "        \n",
    "        montage_path = os.path.join(pred_dir, \"predictions_montage.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(montage_path, dpi=200)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ Saved prediction montage to {montage_path}\")\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    plot_performance_metrics(performance_metrics, pred_dir)\n",
    "    \n",
    "    return success_paths\n",
    "\n",
    "# Plot performance metrics\n",
    "def plot_performance_metrics(metrics, output_dir):\n",
    "    print(\"Plotting performance metrics...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot execution times\n",
    "    plt.subplot(2, 1, 1)\n",
    "    cases = range(len(metrics['total_time']))\n",
    "    width = 0.2\n",
    "    \n",
    "    plt.bar([x - width for x in cases], metrics['load_time'], width, label='Load Time')\n",
    "    plt.bar([x for x in cases], metrics['preprocessing_time'], width, label='Preprocessing Time')\n",
    "    plt.bar([x + width for x in cases], metrics['inference_time'], width, label='Inference Time')\n",
    "    \n",
    "    plt.xlabel('Case')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Execution Time Breakdown')\n",
    "    plt.legend()\n",
    "    plt.xticks(cases)\n",
    "    \n",
    "    # Plot memory usage\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(cases, metrics['memory_before'], 'b-o', label='Memory Before')\n",
    "    plt.plot(cases, metrics['memory_after'], 'r-o', label='Memory After')\n",
    "    \n",
    "    plt.xlabel('Case')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Usage')\n",
    "    plt.legend()\n",
    "    plt.xticks(cases)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    metrics_path = os.path.join(output_dir, \"performance_metrics.png\")\n",
    "    plt.savefig(metrics_path, dpi=200)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✓ Saved performance metrics plot to {metrics_path}\")\n",
    "    \n",
    "    # Calculate and print average metrics\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"  Average load time: {np.mean(metrics['load_time']):.4f}s ± {np.std(metrics['load_time']):.4f}s\")\n",
    "    print(f\"  Average preprocessing time: {np.mean(metrics['preprocessing_time']):.4f}s ± {np.std(metrics['preprocessing_time']):.4f}s\")\n",
    "    print(f\"  Average inference time: {np.mean(metrics['inference_time']):.4f}s ± {np.std(metrics['inference_time']):.4f}s\")\n",
    "    print(f\"  Average total time: {np.mean(metrics['total_time']):.4f}s ± {np.std(metrics['total_time']):.4f}s\")\n",
    "    print(f\"  Average memory usage increase: {np.mean(np.array(metrics['memory_after']) - np.array(metrics['memory_before'])):.2f}MB\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    import json\n",
    "    summary_metrics = {\n",
    "        'avg_load_time': float(np.mean(metrics['load_time'])),\n",
    "        'avg_preprocessing_time': float(np.mean(metrics['preprocessing_time'])),\n",
    "        'avg_inference_time': float(np.mean(metrics['inference_time'])),\n",
    "        'avg_total_time': float(np.mean(metrics['total_time'])),\n",
    "        'avg_memory_increase': float(np.mean(np.array(metrics['memory_after']) - np.array(metrics['memory_before'])))\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'performance_summary.json'), 'w') as f:\n",
    "        json.dump(summary_metrics, f, indent=4)\n",
    "    \n",
    "    return metrics_path\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting brain tumor prediction pipeline at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Setup directories\n",
    "    pred_dir = setup_output_dirs()\n",
    "    \n",
    "    try:\n",
    "        # Try to load existing model_full.pt if available\n",
    "        model_path = \"/kaggle/input/medvision/pytorch/default/1/model_full.pt\"\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Loading existing model from {model_path}\")\n",
    "            loaded_model = torch.load(model_path, map_location=device)\n",
    "        else:\n",
    "            # Load model from checkpoint\n",
    "            loaded_model, model_path = load_model(device)\n",
    "        \n",
    "        if loaded_model is None:\n",
    "            print(\"❌ Cannot continue without loading the model.\")\n",
    "            return\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        loaded_model.eval()\n",
    "        \n",
    "        # Run predictions with brain cropping and get performance metrics\n",
    "        predict_on_test_cases(loaded_model, pred_dir, device)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n✅ All done! Pipeline completed in {total_time:.2f} seconds.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-14T15:58:51.328175Z",
     "iopub.status.busy": "2025-03-14T15:58:51.327805Z",
     "iopub.status.idle": "2025-03-14T15:58:55.903902Z",
     "shell.execute_reply": "2025-03-14T15:58:55.902955Z",
     "shell.execute_reply.started": "2025-03-14T15:58:51.328149Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T13:15:53.781518Z",
     "iopub.status.busy": "2025-03-12T13:15:53.781178Z",
     "iopub.status.idle": "2025-03-12T13:16:05.612411Z",
     "shell.execute_reply": "2025-03-12T13:16:05.611334Z",
     "shell.execute_reply.started": "2025-03-12T13:15:53.781493Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:59:03.261539Z",
     "iopub.status.busy": "2025-03-14T15:59:03.261214Z",
     "iopub.status.idle": "2025-03-14T15:59:03.359292Z",
     "shell.execute_reply": "2025-03-14T15:59:03.358665Z",
     "shell.execute_reply.started": "2025-03-14T15:59:03.261507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from captum.attr import IntegratedGradients, LayerGradCam\n",
    "\n",
    "# Constants for the BraTS dataset\n",
    "IMG_SIZE = 240\n",
    "VOLUME_SLICES = 155\n",
    "VOLUME_START_AT = 22\n",
    "INPUT_CHANNELS = 2\n",
    "OUTPUT_CLASSES = 4\n",
    "SEGMENT_CLASSES = {\n",
    "    0: 'Background',\n",
    "    1: 'Necrotic',\n",
    "    2: 'Edema',\n",
    "    3: 'Enhancing'\n",
    "}\n",
    "\n",
    "# ===================== STEP 1: UTILITY FUNCTIONS =====================\n",
    "\n",
    "def crop_brain_region(image, threshold=0.05):\n",
    "    \"\"\"Crop the brain region from the image\"\"\"\n",
    "    # Normalize the image\n",
    "    normalized = image / np.max(image)\n",
    "    \n",
    "    # Create binary mask where the brain is\n",
    "    brain_mask = normalized > threshold\n",
    "    \n",
    "    # Find bounding box\n",
    "    coords = np.argwhere(brain_mask)\n",
    "    if len(coords) == 0:  # No brain found\n",
    "        return image\n",
    "        \n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    \n",
    "    # Add padding\n",
    "    padding = 10\n",
    "    y_min = max(0, y_min - padding)\n",
    "    y_max = min(image.shape[0], y_max + padding)\n",
    "    x_min = max(0, x_min - padding)\n",
    "    x_max = min(image.shape[1], x_max + padding)\n",
    "    \n",
    "    # Crop brain region\n",
    "    cropped = image[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Resize back to original size for model compatibility\n",
    "    return cv2.resize(cropped, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "def normalize_for_display(attr_map):\n",
    "    \"\"\"Normalize an attribution map for display\"\"\"\n",
    "    if np.max(np.abs(attr_map)) > 0:\n",
    "        attr_map = attr_map / np.max(np.abs(attr_map))\n",
    "        # Shift to [0, 1] range for display\n",
    "        attr_map = (attr_map + 1) / 2\n",
    "    return attr_map\n",
    "\n",
    "def overlay_heatmap(attr_map, original_img, colormap=cv2.COLORMAP_JET, alpha=0.4):\n",
    "    \"\"\"Create a heatmap overlay on the original image - standalone function\"\"\"\n",
    "    # Ensure attr_map is in range [0, 1]\n",
    "    attr_map = np.clip(attr_map, 0, 1)\n",
    "    \n",
    "    # Convert to uint8 for colormap\n",
    "    heatmap = np.uint8(attr_map * 255)\n",
    "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize original image if needed\n",
    "    if original_img.max() > 1:\n",
    "        original_img = original_img / original_img.max()\n",
    "    \n",
    "    # Convert grayscale to RGB if needed\n",
    "    if len(original_img.shape) == 2:\n",
    "        original_img = np.stack([original_img] * 3, axis=2)\n",
    "    elif len(original_img.shape) > 2 and original_img.shape[2] == 1:\n",
    "        original_img = np.concatenate([original_img] * 3, axis=2)\n",
    "    \n",
    "    # Create overlay\n",
    "    overlay = (1-alpha) * original_img + alpha * heatmap / 255.0\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "# ===================== STEP 2: SLICE EXPLAINER =====================\n",
    "\n",
    "class SLICEExplainer:\n",
    "    \"\"\"SLICE (Synthetic Labeled Input Counterfactual Explanations) implementation\"\"\"\n",
    "    def __init__(self, model=None, num_samples=10, perturbation_std=0.1):\n",
    "        self.model = model\n",
    "        self.num_samples = num_samples\n",
    "        self.perturbation_std = perturbation_std\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        \"\"\"Set model after initialization\"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def explain(self, input_tensor, target_class):\n",
    "        \"\"\"\n",
    "        Generate SLICE explanation by analyzing model's response to perturbed inputs\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not available for SLICE\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "        device = input_tensor.device\n",
    "        batch_size, channels, height, width = input_tensor.shape\n",
    "        \n",
    "        # Initialize attention map\n",
    "        attention_map = torch.zeros((height, width), device=device)\n",
    "        \n",
    "        try:\n",
    "            # Original prediction\n",
    "            with torch.no_grad():\n",
    "                original_output = self.model(input_tensor)\n",
    "                original_pred = torch.sigmoid(original_output)[0, target_class]\n",
    "                # Get mean prediction value for the target class\n",
    "                original_score = original_pred.mean().item()\n",
    "            \n",
    "            # Create perturbations and track prediction changes\n",
    "            for i in range(self.num_samples):\n",
    "                # Create noise with gradually increasing perturbation\n",
    "                scale = self.perturbation_std * (i + 1) / self.num_samples\n",
    "                noise = torch.randn_like(input_tensor) * scale\n",
    "                perturbed_input = input_tensor + noise\n",
    "                \n",
    "                # Get prediction\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(perturbed_input)\n",
    "                    pred = torch.sigmoid(output)[0, target_class]\n",
    "                    # Get mean prediction value for the target class\n",
    "                    pred_score = pred.mean().item()\n",
    "                \n",
    "                # Calculate change in prediction\n",
    "                pred_diff = abs(pred_score - original_score)\n",
    "                \n",
    "                # Calculate pixel-wise difference weighted by prediction change\n",
    "                input_diff = torch.abs(perturbed_input - input_tensor).mean(dim=1).squeeze()\n",
    "                weighted_diff = input_diff * pred_diff\n",
    "                \n",
    "                # Add to attention map\n",
    "                attention_map += weighted_diff.detach()\n",
    "            \n",
    "            # Normalize attention map\n",
    "            if torch.max(attention_map) > 0:\n",
    "                attention_map = attention_map / torch.max(attention_map)\n",
    "            \n",
    "            return attention_map.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in SLICE: {e}\")\n",
    "            return np.zeros((height, width))\n",
    "\n",
    "# ===================== STEP 3: GRAD-CAM EXPLAINER =====================\n",
    "\n",
    "class GradCAMExplainer:\n",
    "    \"\"\"Grad-CAM explainer for medical vision models\"\"\"\n",
    "    def __init__(self, model=None, target_layer=None):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        \"\"\"Set model after initialization\"\"\"\n",
    "        self.model = model\n",
    "        # Try to automatically find target layer\n",
    "        if hasattr(model, 'fusion') and len(model.fusion) > 0:\n",
    "            self.target_layer = model.fusion[0]\n",
    "        elif hasattr(model, 'encoder') and hasattr(model.encoder, 'blocks'):\n",
    "            self.target_layer = model.encoder.blocks[-1][-1]\n",
    "    \n",
    "    def explain(self, input_tensor, target_class):\n",
    "        \"\"\"Generate GradCAM explanation\"\"\"\n",
    "        if self.model is None or self.target_layer is None:\n",
    "            print(\"Model or target layer not available for GradCAM\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        try:\n",
    "            # Create a wrapper function that returns a scalar output for the target class\n",
    "            def wrapper_fn(inputs):\n",
    "                outputs = self.model(inputs)\n",
    "                # Sum over spatial dimensions to get scalar per batch item\n",
    "                return outputs[:, target_class].sum(dim=[1, 2])\n",
    "            \n",
    "            # Setup\n",
    "            layer_gc = LayerGradCam(wrapper_fn, self.target_layer)\n",
    "            \n",
    "            # Generate attributions\n",
    "            attributions = layer_gc.attribute(input_tensor)\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            attr_map = attributions.squeeze().cpu().detach().numpy()\n",
    "            \n",
    "            # Upsample to match input size if needed\n",
    "            if attr_map.shape != (IMG_SIZE, IMG_SIZE):\n",
    "                attr_map = cv2.resize(attr_map, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Normalize\n",
    "            attr_map = normalize_for_display(attr_map)\n",
    "            \n",
    "            return attr_map\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in GradCAM: {e}\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# ===================== STEP 4: ATTENTION MAP EXTRACTOR =====================\n",
    "\n",
    "class AttentionMapExtractor:\n",
    "    \"\"\"Extracts attention maps from transformer blocks in the model\"\"\"\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "        self.attention_maps = []\n",
    "        self.hooks = []\n",
    "        \n",
    "        if model is not None:\n",
    "            self.register_hooks()\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        \"\"\"Set model after initialization\"\"\"\n",
    "        self.model = model\n",
    "        self.register_hooks()\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks to capture attention maps\"\"\"\n",
    "        # Clear any previous hooks\n",
    "        self.remove_hooks()\n",
    "        \n",
    "        # Try to register hooks on the transformer if it exists\n",
    "        if self.model is None:\n",
    "            return\n",
    "            \n",
    "        if hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'transformer'):\n",
    "            self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register hooks to capture attention maps\"\"\"\n",
    "        def hook_fn(module, input, output):\n",
    "            # For transformer models, try to extract attention weights\n",
    "            if hasattr(output, 'attentions') and output.attentions is not None:\n",
    "                self.attention_maps.append(output.attentions)\n",
    "        \n",
    "        # Find transformer blocks to hook\n",
    "        try:\n",
    "            if hasattr(self.model.transformer.transformer, 'encoder') and \\\n",
    "               hasattr(self.model.transformer.transformer.encoder, 'layer'):\n",
    "                for layer in self.model.transformer.transformer.encoder.layer:\n",
    "                    if hasattr(layer, 'attention'):\n",
    "                        hook = layer.attention.register_forward_hook(hook_fn)\n",
    "                        self.hooks.append(hook)\n",
    "        except AttributeError:\n",
    "            print(\"Could not register attention hooks - transformer structure not found\")\n",
    "    \n",
    "    def get_attention(self, input_tensor):\n",
    "        \"\"\"Get attention maps for an input\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not available for attention extraction\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "        # Clear previous attention maps\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(input_tensor)\n",
    "            \n",
    "            # If we captured any attention maps\n",
    "            if self.attention_maps:\n",
    "                # Average across layers and heads\n",
    "                all_attentions = []\n",
    "                for layer_attentions in self.attention_maps:\n",
    "                    # Average across heads\n",
    "                    if isinstance(layer_attentions, tuple):\n",
    "                        layer_attentions = layer_attentions[0]  # Get first item if it's a tuple\n",
    "                    \n",
    "                    avg_attention = layer_attentions.mean(dim=1)  # Average across heads\n",
    "                    all_attentions.append(avg_attention)\n",
    "                \n",
    "                if all_attentions:\n",
    "                    # Stack and average across layers\n",
    "                    avg_attention = torch.stack(all_attentions).mean(dim=0)\n",
    "                    \n",
    "                    # Get spatial dimensions for attention map reshaping\n",
    "                    h = w = int(np.sqrt(avg_attention.shape[-1]))\n",
    "                    \n",
    "                    # Reshape to spatial dimensions and get the diagonal (self-attention)\n",
    "                    attention_map = avg_attention.reshape(avg_attention.shape[0], h, w)\n",
    "                    attention_map = attention_map[0].cpu().numpy()  # Use first batch item\n",
    "                    \n",
    "                    # Resize to input size\n",
    "                    attention_map = cv2.resize(attention_map, (IMG_SIZE, IMG_SIZE))\n",
    "                    \n",
    "                    # Normalize\n",
    "                    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min() + 1e-8)\n",
    "                    \n",
    "                    return attention_map\n",
    "            \n",
    "            # No attention maps captured, generate blank map\n",
    "            print(\"No attention maps could be captured from the model\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting attention maps: {e}\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "# ===================== STEP 5: INTEGRATED GRADIENTS EXPLAINER =====================\n",
    "\n",
    "class IntegratedGradientsExplainer:\n",
    "    \"\"\"Integrated Gradients explainer for medical vision models\"\"\"\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        \"\"\"Set model after initialization\"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def explain(self, input_tensor, target_class, n_steps=50):\n",
    "        \"\"\"Generate IntegratedGradients explanation\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not available for IntegratedGradients\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        try:\n",
    "            # Create a wrapper function that returns a scalar output for the target class\n",
    "            def wrapper_fn(inputs):\n",
    "                outputs = self.model(inputs)\n",
    "                # Sum over spatial dimensions to get scalar per batch item\n",
    "                return outputs[:, target_class].sum(dim=[1, 2])\n",
    "            \n",
    "            # Setup\n",
    "            input_tensor.requires_grad = True\n",
    "            ig = IntegratedGradients(wrapper_fn)\n",
    "            \n",
    "            # Generate attributions\n",
    "            attributions = ig.attribute(\n",
    "                input_tensor,\n",
    "                n_steps=n_steps,\n",
    "                method='gausslegendre'\n",
    "            )\n",
    "            \n",
    "            # Average over channels for visualization\n",
    "            attr_map = attributions.sum(dim=1).squeeze().cpu().detach().numpy()\n",
    "            \n",
    "            # Normalize\n",
    "            attr_map = normalize_for_display(attr_map)\n",
    "            \n",
    "            return attr_map\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in IntegratedGradients: {e}\")\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# ===================== STEP 6: UNIFIED XAI EXPLAINER =====================\n",
    "\n",
    "class MedicalVisionExplainer:\n",
    "    \"\"\"XAI wrapper for medical vision models using Captum\"\"\"\n",
    "    def __init__(self, model=None, device='cuda'):\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize individual explainers\n",
    "        self.slice_explainer = SLICEExplainer()\n",
    "        self.gradcam_explainer = GradCAMExplainer()\n",
    "        self.attention_extractor = AttentionMapExtractor()\n",
    "        self.ig_explainer = IntegratedGradientsExplainer()\n",
    "        \n",
    "        # Set model if provided\n",
    "        if model is not None:\n",
    "            self.set_model(model)\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        \"\"\"Set the model and update all explainers\"\"\"\n",
    "        if model is not None:\n",
    "            self.model = model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Update all explainers\n",
    "            self.slice_explainer.set_model(self.model)\n",
    "            self.gradcam_explainer.set_model(self.model)\n",
    "            self.attention_extractor.set_model(self.model)\n",
    "            self.ig_explainer.set_model(self.model)\n",
    "    \n",
    "    def predict(self, input_tensor):\n",
    "        \"\"\"Make a prediction with the model\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not available for prediction\")\n",
    "            return torch.zeros((1, OUTPUT_CLASSES, IMG_SIZE, IMG_SIZE), device=self.device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            if isinstance(output, list):\n",
    "                output = output[0]  # Some models return a list\n",
    "            return torch.sigmoid(output)\n",
    "    \n",
    "    def explain_integrated_gradients(self, input_tensor, target_class, n_steps=50):\n",
    "        \"\"\"Generate IntegratedGradients explanation\"\"\"\n",
    "        return self.ig_explainer.explain(input_tensor, target_class, n_steps)\n",
    "    \n",
    "    def explain_layergrad_cam(self, input_tensor, target_class):\n",
    "        \"\"\"Generate LayerGradCam explanation\"\"\"\n",
    "        return self.gradcam_explainer.explain(input_tensor, target_class)\n",
    "    \n",
    "    def explain_slice(self, input_tensor, target_class):\n",
    "        \"\"\"Generate SLICE explanation\"\"\"\n",
    "        return self.slice_explainer.explain(input_tensor, target_class)\n",
    "    \n",
    "    def extract_attention(self, input_tensor):\n",
    "        \"\"\"Extract attention map from transformer\"\"\"\n",
    "        return self.attention_extractor.get_attention(input_tensor)\n",
    "    \n",
    "    def generate_all_explanations(self, input_tensor, target_class):\n",
    "        \"\"\"Generate all explanations for a given input and target class\"\"\"\n",
    "        print(f\"Generating explanations for class {SEGMENT_CLASSES[target_class]}...\")\n",
    "        \n",
    "        # Get model prediction\n",
    "        pred = self.predict(input_tensor)\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        \n",
    "        explanations = {\n",
    "            'prediction': pred_np[target_class].mean(),  # Average prediction for this class\n",
    "        }\n",
    "        \n",
    "        # Generate explanations one by one\n",
    "        explanations['integrated_gradients'] = self.explain_integrated_gradients(input_tensor, target_class)\n",
    "        explanations['layergrad_cam'] = self.explain_layergrad_cam(input_tensor, target_class)\n",
    "        explanations['slice'] = self.explain_slice(input_tensor, target_class)\n",
    "        explanations['attention'] = self.extract_attention(input_tensor)\n",
    "        \n",
    "        return explanations\n",
    "\n",
    "# ===================== STEP 7: DATA LOADING AND PREPROCESSING =====================\n",
    "\n",
    "def load_and_preprocess_case(case_path, case_id, slice_idx):\n",
    "    \"\"\"Load and preprocess a single slice from a BraTS case with brain cropping\"\"\"\n",
    "    print(f\"Loading data for case {case_id}, slice {slice_idx}...\")\n",
    "    \n",
    "    # Load flair image\n",
    "    vol_path_flair = os.path.join(case_path, f'{case_id}_flair.nii.gz')\n",
    "    if not os.path.exists(vol_path_flair):\n",
    "        print(f\"Flair image not found at {vol_path_flair}\")\n",
    "        return None, None\n",
    "    \n",
    "    flair = nib.load(vol_path_flair).get_fdata()\n",
    "    \n",
    "    # Load T1ce as second channel if available\n",
    "    vol_path_t1ce = os.path.join(case_path, f'{case_id}_t1ce.nii.gz')\n",
    "    if os.path.exists(vol_path_t1ce):\n",
    "        t1ce = nib.load(vol_path_t1ce).get_fdata()\n",
    "    else:\n",
    "        # If T1ce not available, try T2 or just use zeros\n",
    "        vol_path_t2 = os.path.join(case_path, f'{case_id}_t2.nii.gz')\n",
    "        if os.path.exists(vol_path_t2):\n",
    "            t1ce = nib.load(vol_path_t2).get_fdata()\n",
    "        else:\n",
    "            t1ce = np.zeros_like(flair)\n",
    "    \n",
    "    # Load ground truth if available\n",
    "    gt_path = os.path.join(case_path, f'{case_id}_seg.nii.gz')\n",
    "    if os.path.exists(gt_path):\n",
    "        gt = nib.load(gt_path).get_fdata()\n",
    "    else:\n",
    "        gt = None\n",
    "    \n",
    "    # Ensure slice is in bounds\n",
    "    if slice_idx + VOLUME_START_AT >= flair.shape[2]:\n",
    "        print(f\"Slice {slice_idx} out of bounds, adjusting...\")\n",
    "        slice_idx = flair.shape[2] - VOLUME_START_AT - 1\n",
    "    \n",
    "    # Extract the specific slice\n",
    "    flair_slice = flair[:, :, slice_idx + VOLUME_START_AT]\n",
    "    t1ce_slice = t1ce[:, :, slice_idx + VOLUME_START_AT]\n",
    "    \n",
    "    # Apply brain cropping to both modalities\n",
    "    flair_slice = crop_brain_region(flair_slice)\n",
    "    t1ce_slice = crop_brain_region(t1ce_slice)\n",
    "    \n",
    "    # Stack channels\n",
    "    input_slice = np.stack([flair_slice, t1ce_slice], axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    input_slice = input_slice / np.max(input_slice)\n",
    "    \n",
    "    # Extract and crop ground truth if available\n",
    "    if gt is not None:\n",
    "        gt_slice = gt[:, :, slice_idx + VOLUME_START_AT]\n",
    "        \n",
    "        # Crop ground truth using the same approach as in your prediction pipeline\n",
    "        normalized = flair[:, :, slice_idx + VOLUME_START_AT] / np.max(flair[:, :, slice_idx + VOLUME_START_AT])\n",
    "        brain_mask = normalized > 0.05\n",
    "        coords = np.argwhere(brain_mask)\n",
    "        \n",
    "        if len(coords) > 0:\n",
    "            y_min, x_min = coords.min(axis=0)\n",
    "            y_max, x_max = coords.max(axis=0)\n",
    "            \n",
    "            # Add padding\n",
    "            padding = 10\n",
    "            y_min = max(0, y_min - padding)\n",
    "            y_max = min(flair_slice.shape[0], y_max + padding)\n",
    "            x_min = max(0, x_min - padding)\n",
    "            x_max = min(flair_slice.shape[1], x_max + padding)\n",
    "            \n",
    "            # Crop gt with the same bounds\n",
    "            gt_slice_cropped = gt_slice[y_min:y_max, x_min:x_max]\n",
    "            gt_slice = cv2.resize(gt_slice_cropped, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_slice = cv2.resize(gt_slice, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return input_slice, gt_slice\n",
    "# ===================== STEP 8: VISUALIZATION =====================\n",
    "\n",
    "def visualize_explanations(original_img, explanations, gt_slice, target_class, output_path):\n",
    "    \"\"\"Create and save a visualization of all explanations with improved visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Ensure original_img is 2D for display\n",
    "    if len(original_img.shape) > 2:\n",
    "        display_img = original_img[0]  # Use first channel (FLAIR) for display\n",
    "    else:\n",
    "        display_img = original_img\n",
    "    \n",
    "    # Normalize for display\n",
    "    display_img = (display_img - display_img.min()) / (display_img.max() - display_img.min() + 1e-8)\n",
    "    \n",
    "    # Plot original cropped brain image\n",
    "    axes[0, 0].imshow(display_img, cmap='gray')\n",
    "    axes[0, 0].set_title(f'Cropped Brain FLAIR\\nPrediction: {explanations[\"prediction\"]:.4f}')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Plot ground truth if available\n",
    "    axes[0, 1].imshow(display_img, cmap='gray')\n",
    "    if gt_slice is not None:\n",
    "        mask = gt_slice == target_class\n",
    "        axes[0, 1].imshow(mask, alpha=0.5, cmap='Reds')\n",
    "    axes[0, 1].set_title(f'Ground Truth - {SEGMENT_CLASSES[target_class]}')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Plot LayerGradCAM\n",
    "    gc_overlay = overlay_heatmap(explanations['layergrad_cam'], display_img, alpha=0.5)\n",
    "    axes[0, 2].imshow(gc_overlay)\n",
    "    axes[0, 2].set_title('Layer GradCAM')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Plot IntegratedGradients\n",
    "    ig_overlay = overlay_heatmap(explanations['integrated_gradients'], display_img, alpha=0.5)\n",
    "    axes[1, 0].imshow(ig_overlay)\n",
    "    axes[1, 0].set_title('Integrated Gradients')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Plot SLICE\n",
    "    slice_overlay = overlay_heatmap(explanations['slice'], display_img, alpha=0.5)\n",
    "    axes[1, 1].imshow(slice_overlay)\n",
    "    axes[1, 1].set_title('SLICE')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Plot Attention Map\n",
    "    attn_overlay = overlay_heatmap(explanations['attention'], display_img, alpha=0.5)\n",
    "    axes[1, 2].imshow(attn_overlay)\n",
    "    axes[1, 2].set_title('Transformer Attention')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    try:\n",
    "        # Save figure\n",
    "        plt.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Visualization saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving visualization: {e}\")\n",
    "    \n",
    "    return output_path\n",
    "# ===================== STEP 9: ANALYSIS RUNNER =====================\n",
    "\n",
    "def run_analysis_on_case(model, case_id, case_path, output_dir, slice_idx=80, device='cuda'):\n",
    "    \"\"\"Run XAI analysis on a specific case and slice\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    input_slice, gt_slice = load_and_preprocess_case(case_path, case_id, slice_idx)\n",
    "    if input_slice is None:\n",
    "        return False\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    input_tensor = torch.tensor(input_slice, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize explainer with model\n",
    "    explainer = MedicalVisionExplainer(model, device)\n",
    "    \n",
    "    # Generate explanations for each tumor class\n",
    "    for target_class in range(1, OUTPUT_CLASSES):  # Skip background class (0)\n",
    "        class_name = SEGMENT_CLASSES[target_class]\n",
    "        print(f\"Analyzing class {target_class}: {class_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate explanations\n",
    "            explanations = explainer.generate_all_explanations(input_tensor, target_class)\n",
    "            \n",
    "            # Create visualization\n",
    "            output_path = os.path.join(output_dir, f\"{case_id}_slice{slice_idx}_class{target_class}_{class_name}.png\")\n",
    "            visualize_explanations(input_slice, explanations, gt_slice, target_class, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing class {class_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Clean up hooks\n",
    "    explainer.attention_extractor.remove_hooks()\n",
    "    \n",
    "    print(f\"✓ XAI analysis completed for case {case_id}\")\n",
    "    return True\n",
    "\n",
    "# ===================== STEP 10: MAIN EXECUTION =====================\n",
    "\n",
    "def analyze_test_case(model_path, case_id, dataset_path, slice_idx=80):\n",
    "    \"\"\"Main function to run XAI analysis on a BraTS case\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set paths\n",
    "    case_path = os.path.join(dataset_path, case_id)\n",
    "    output_dir = os.path.join(\"./xai_results\", case_id)\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Check if this is a state dict or full model\n",
    "        if not isinstance(model, torch.nn.Module):\n",
    "            if isinstance(model, dict) and 'model_state_dict' in model:\n",
    "                # Try to import the model class\n",
    "                try:\n",
    "                    # If ExtendedUNetPlusPlus is available in the current scope\n",
    "                    from unet_model import ExtendedUNetPlusPlus\n",
    "                    model_instance = ExtendedUNetPlusPlus()\n",
    "                    model_instance.load_state_dict(model['model_state_dict'])\n",
    "                    model = model_instance\n",
    "                except ImportError:\n",
    "                    print(\"Error: Could not import ExtendedUNetPlusPlus. Make sure it's defined in your workspace.\")\n",
    "                    return\n",
    "            else:\n",
    "                print(\"Error: Couldn't load model properly\")\n",
    "                return\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Run analysis\n",
    "    run_analysis_on_case(model, case_id, case_path, output_dir, slice_idx, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T16:10:11.590803Z",
     "iopub.status.busy": "2025-03-14T16:10:11.590407Z",
     "iopub.status.idle": "2025-03-14T16:10:16.683129Z",
     "shell.execute_reply": "2025-03-14T16:10:16.682230Z",
     "shell.execute_reply.started": "2025-03-14T16:10:11.590766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "MODEL_PATH = '/kaggle/input/medvision/pytorch/default/1/model_full.pt'\n",
    "DATASET_PATH = '/kaggle/working/BraTS2021_Training_Data'\n",
    "\n",
    "# Run analysis on a single case\n",
    "case_id = 'BraTS2021_00124'\n",
    "analyze_test_case(MODEL_PATH, case_id, DATASET_PATH, slice_idx=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:30:59.891291Z",
     "iopub.status.busy": "2025-03-12T15:30:59.890938Z",
     "iopub.status.idle": "2025-03-12T15:31:05.201684Z",
     "shell.execute_reply": "2025-03-12T15:31:05.200537Z",
     "shell.execute_reply.started": "2025-03-12T15:30:59.891257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Define the path to your folder containing PNG images\n",
    "FOLDER_PATH = \"/kaggle/working/xai_results/BraTS2021_00124\"  # Change this to your folder path\n",
    "\n",
    "# Get all PNG files in the folder\n",
    "png_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".png\")]\n",
    "\n",
    "# Sort files (optional, useful if they have an order)\n",
    "png_files.sort()\n",
    "\n",
    "# Plot all images\n",
    "fig, axes = plt.subplots(1, len(png_files), figsize=(25, 15))\n",
    "\n",
    "for i, file in enumerate(png_files):\n",
    "    img_path = os.path.join(FOLDER_PATH, file)\n",
    "    img = cv2.imread(img_path)  # Read image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # If there's only one image, `axes` won't be an array\n",
    "    if len(png_files) == 1:\n",
    "        axes.imshow(img)\n",
    "        axes.axis(\"off\")\n",
    "    else:\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:32:01.494179Z",
     "iopub.status.busy": "2025-03-12T15:32:01.493836Z",
     "iopub.status.idle": "2025-03-12T15:32:07.537777Z",
     "shell.execute_reply": "2025-03-12T15:32:07.536738Z",
     "shell.execute_reply.started": "2025-03-12T15:32:01.494155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Define the path to your folder containing PNG images\n",
    "FOLDER_PATH = \"/kaggle/working/xai_results/BraTS2021_00124\"  # Change this to your folder path\n",
    "\n",
    "# Get all PNG files in the folder\n",
    "png_files = sorted([f for f in os.listdir(FOLDER_PATH) if f.endswith(\".png\")])\n",
    "\n",
    "# Plot each image in a separate row\n",
    "for file in png_files:\n",
    "    img_path = os.path.join(FOLDER_PATH, file)\n",
    "    img = cv2.imread(img_path)  # Read image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 10))  # Adjust figure size\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(file)  # Display file name as title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T16:11:59.692465Z",
     "iopub.status.busy": "2025-03-14T16:11:59.692124Z",
     "iopub.status.idle": "2025-03-14T16:11:59.713230Z",
     "shell.execute_reply": "2025-03-14T16:11:59.712310Z",
     "shell.execute_reply.started": "2025-03-14T16:11:59.692441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class MedicalXAIExplainer:\n",
    "    \"\"\"\n",
    "    An agent that uses GPT-4 Vision to explain XAI visualizations to medical practitioners\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"gpt-4o\", max_tokens=1000):\n",
    "        \"\"\"\n",
    "        Initialize the medical XAI explainer\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key\n",
    "            model: OpenAI model to use (default: gpt-4-vision-preview)\n",
    "            max_tokens: Maximum tokens for response (default: 1000)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        self.segment_classes = {\n",
    "            0: 'Background',\n",
    "            1: 'Necrotic',\n",
    "            2: 'Edema',\n",
    "            3: 'Enhancing'\n",
    "        }\n",
    "    \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Encode image to base64 for API request\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def get_image_dimensions(self, image_path):\n",
    "        \"\"\"Get dimensions of the image\"\"\"\n",
    "        with Image.open(image_path) as img:\n",
    "            return img.size\n",
    "    \n",
    "    def create_prompt(self, tumor_class, class_name, slice_idx=None, prediction_value=None):\n",
    "        \"\"\"\n",
    "        Create a structured prompt for GPT to explain the XAI visualization\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are MedBrainInsight, an advanced medical imaging AI assistant specializing in brain tumor MRI analysis and explainable AI. \n",
    "        \n",
    "You're examining a visualization of XAI (Explainable AI) methods applied to a brain MRI scan for potential {class_name} tumor detection (Class {tumor_class}).\n",
    "\n",
    "This image shows different XAI techniques highlighting regions of interest in the brain scan:\n",
    "\n",
    "1. Top-left: Original MRI brain scan\n",
    "2. Top-middle: Ground truth tumor segmentation (if available) \n",
    "3. Top-right: Layer GradCAM visualization - highlights regions most activating the model's prediction\n",
    "4. Bottom-left: Integrated Gradients visualization - shows attribution of each pixel to the prediction\n",
    "5. Bottom-middle: SLICE (Synthetic Labeled Input Counterfactual Explanation) visualization - shows regions whose alteration would most change the prediction\n",
    "6. Bottom-right: Transformer Attention visualization - shows where the model's transformer component is focusing\n",
    "\n",
    "Please analyze this visualization thoroughly and provide:\n",
    "\n",
    "1. **Clinical Assessment**: What potential {class_name} tumor features is the model detecting? Are the highlighted regions anatomically reasonable for this tumor type?\n",
    "\n",
    "2. **XAI Method Comparison**: How do the different XAI methods compare in this case? Which seems most clinically useful/interpretable?\n",
    "\n",
    "3. **Confidence Analysis**: Based on the visualizations, how reliable does the model's prediction appear? Are there concerning disagreements between XAI methods?\n",
    "\n",
    "4. **Clinical Recommendation**: What would you suggest the physician focus on when reviewing this scan? Any specific regions or features deserving further clinical assessment?\n",
    "\n",
    "Use your expertise in both neuroradiology and AI to provide insights that would help a physician understand and trust (or appropriately question) the model's analysis. Be specific about anatomical structures and clinical implications.\n",
    "\n",
    "Your analysis should be structured, detailed yet concise, with clear headings for each section. Write in a professional medical tone suitable for a radiologist or neurosurgeon.\n",
    "\"\"\"\n",
    "        \n",
    "        # Add additional context if available\n",
    "        if prediction_value is not None:\n",
    "            prompt += f\"\\n\\nThe model's prediction score for {class_name} is {prediction_value:.4f} (on a scale of 0-1).\"\n",
    "        \n",
    "        if slice_idx is not None:\n",
    "            prompt += f\"\\n\\nThis is slice #{slice_idx} of the MRI volume.\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def extract_prediction_value(self, image_path):\n",
    "        \"\"\"\n",
    "        Try to extract the prediction value from the image using OCR or pattern matching\n",
    "        Returns None if extraction fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Using a simple image-based text extraction approach\n",
    "            # This is a placeholder - in practice you might want to use a proper OCR library\n",
    "            img = Image.open(image_path)\n",
    "            # Convert to grayscale for text detection\n",
    "            img_gray = img.convert('L')\n",
    "            \n",
    "            # Use a simple method to look for \"Prediction: X.XXXX\" text\n",
    "            # This is just a placeholder and would need a proper OCR implementation\n",
    "            # For now, we'll extract from the filename if it follows our pattern\n",
    "            \n",
    "            filename = os.path.basename(image_path)\n",
    "            match = re.search(r'prediction_([0-9\\.]+)', filename)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract prediction value: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def explain_visualization(self, image_path, extract_info=True):\n",
    "        \"\"\"\n",
    "        Send XAI visualization to GPT-4 Vision and get explanation\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to XAI visualization image\n",
    "            extract_info: Whether to try extracting additional info from filename/image\n",
    "        \n",
    "        Returns:\n",
    "            GPT's explanation of the XAI visualization\n",
    "        \"\"\"\n",
    "        # Extract tumor class and other info from filename\n",
    "        filename = os.path.basename(image_path)\n",
    "        \n",
    "        # Extract class from filename pattern like \"BraTS2021_00124_slice80_class1_Necrotic.png\"\n",
    "        class_match = re.search(r'class(\\d+)_(\\w+)', filename)\n",
    "        slice_match = re.search(r'slice(\\d+)', filename)\n",
    "        \n",
    "        if class_match:\n",
    "            tumor_class = int(class_match.group(1))\n",
    "            class_name = class_match.group(2)\n",
    "        else:\n",
    "            # Default values if pattern not found\n",
    "            tumor_class = 1\n",
    "            class_name = \"Tumor\"\n",
    "        \n",
    "        slice_idx = int(slice_match.group(1)) if slice_match else None\n",
    "        \n",
    "        # Try to extract prediction value if requested\n",
    "        prediction_value = self.extract_prediction_value(image_path) if extract_info else None\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = self.create_prompt(tumor_class, class_name, slice_idx, prediction_value)\n",
    "        \n",
    "        # Encode the image\n",
    "        base64_image = self.encode_image(image_path)\n",
    "        \n",
    "        # Prepare the request\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": self.max_tokens\n",
    "        }\n",
    "        \n",
    "        # Make the request\n",
    "        try:\n",
    "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            explanation = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return {\n",
    "                \"image_path\": image_path,\n",
    "                \"class\": tumor_class,\n",
    "                \"class_name\": class_name,\n",
    "                \"slice_idx\": slice_idx,\n",
    "                \"prediction_value\": prediction_value,\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting explanation from OpenAI: {e}\")\n",
    "            if 'response' in locals():\n",
    "                print(f\"Response: {response.text}\")\n",
    "            return {\n",
    "                \"image_path\": image_path,\n",
    "                \"class\": tumor_class,\n",
    "                \"class_name\": class_name,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def save_explanation(self, explanation, output_dir):\n",
    "        \"\"\"\n",
    "        Save the explanation to a text file\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        image_name = os.path.basename(explanation[\"image_path\"])\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_explanation.txt\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# XAI Explanation for {image_name}\\n\\n\")\n",
    "            f.write(f\"Tumor Class: {explanation['class']} ({explanation['class_name']})\\n\")\n",
    "            if explanation.get(\"slice_idx\"):\n",
    "                f.write(f\"MRI Slice: {explanation['slice_idx']}\\n\")\n",
    "            if explanation.get(\"prediction_value\"):\n",
    "                f.write(f\"Prediction Score: {explanation['prediction_value']:.4f}\\n\")\n",
    "            f.write(\"\\n## AI Analysis\\n\\n\")\n",
    "            f.write(explanation[\"explanation\"])\n",
    "        \n",
    "        print(f\"Explanation saved to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def create_combined_visualization(self, image_path, explanation, output_dir):\n",
    "        \"\"\"\n",
    "        Create a combined visualization with the original image and the explanation\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Create a figure with the image and explanation\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Display the image\n",
    "        ax1.imshow(np.array(img))\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(\"XAI Visualization\")\n",
    "        \n",
    "        # Display the explanation\n",
    "        ax2.text(0.05, 0.95, explanation[\"explanation\"], \n",
    "                 wrap=True, va='top', ha='left', fontsize=11, \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title(\"AI Explanation\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        image_name = os.path.basename(image_path)\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_explained.png\")\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def process_directory(self, input_dir, output_dir, create_combined=True):\n",
    "        \"\"\"\n",
    "        Process all XAI visualization images in a directory\n",
    "        \n",
    "        Args:\n",
    "            input_dir: Directory containing XAI visualization images\n",
    "            output_dir: Directory to save explanations\n",
    "            create_combined: Whether to create combined visualizations\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get all PNG files in the input directory\n",
    "        image_paths = list(Path(input_dir).glob(\"*.png\"))\n",
    "        \n",
    "        if not image_paths:\n",
    "            print(f\"No PNG files found in {input_dir}\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for image_path in image_paths:\n",
    "            print(f\"Processing {image_path}...\")\n",
    "            explanation = self.explain_visualization(str(image_path))\n",
    "            \n",
    "            # Save the explanation\n",
    "            text_path = self.save_explanation(explanation, output_dir)\n",
    "            \n",
    "            # Create combined visualization if requested\n",
    "            if create_combined and \"error\" not in explanation:\n",
    "                combined_path = self.create_combined_visualization(str(image_path), explanation, output_dir)\n",
    "                explanation[\"combined_path\"] = combined_path\n",
    "            \n",
    "            explanation[\"text_path\"] = text_path\n",
    "            results.append(explanation)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T16:12:12.497387Z",
     "iopub.status.busy": "2025-03-14T16:12:12.497087Z",
     "iopub.status.idle": "2025-03-14T16:12:43.781550Z",
     "shell.execute_reply": "2025-03-14T16:12:43.780431Z",
     "shell.execute_reply.started": "2025-03-14T16:12:12.497364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install pillow requests matplotlib numpy\n",
    "\n",
    "# Import the MedicalXAIExplainer class from the previous cell\n",
    "# (No need to import again if you've already run the cell containing the class)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = \" \"  # Replace with your actual API key\n",
    "\n",
    "# Initialize the explainer\n",
    "explainer = MedicalXAIExplainer(api_key=OPENAI_API_KEY, max_tokens=1500)\n",
    "\n",
    "# Set paths for your Kaggle environment\n",
    "XAI_RESULTS_DIR = \"/kaggle/working/xai_results/BraTS2021_00124\"  # Directory with your XAI visualizations\n",
    "EXPLANATIONS_DIR = \"/kaggle/working/xai_explanations\"  # Where to save explanations\n",
    "\n",
    "# Option 1: Process a single XAI visualization\n",
    "image_path = \"/kaggle/working/xai_results/BraTS2021_00124/BraTS2021_00124_slice80_class1_Necrotic.png\"\n",
    "explanation = explainer.explain_visualization(image_path)\n",
    "explainer.save_explanation(explanation, EXPLANATIONS_DIR)\n",
    "combined_path = explainer.create_combined_visualization(image_path, explanation, EXPLANATIONS_DIR)\n",
    "\n",
    "# Display the results\n",
    "from IPython.display import display, Image, Markdown\n",
    "display(Markdown(f\"## Explanation for {os.path.basename(image_path)}\"))\n",
    "display(Image(combined_path))\n",
    "display(Markdown(explanation[\"explanation\"]))\n",
    "\n",
    "# Option 2: Process a directory of XAI visualizations\n",
    "results = explainer.process_directory(XAI_RESULTS_DIR, EXPLANATIONS_DIR, create_combined=True)\n",
    "print(f\"Processed {len(results)} images\")\n",
    "\n",
    "# Display summary\n",
    "display(Markdown(\"## Summary of Processed Images\"))\n",
    "for result in results:\n",
    "    if \"error\" in result:\n",
    "        display(Markdown(f\"❌ {os.path.basename(result['image_path'])}: {result['error']}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"✅ {os.path.basename(result['image_path'])}: Class {result['class']} ({result['class_name']})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1541666,
     "sourceId": 2542390,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "sourceId": 286202,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
